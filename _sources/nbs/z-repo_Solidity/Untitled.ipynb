{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "from scipy.optimize import fsolve # finding roots\n",
    "\n",
    "%matplotlib inline\n",
    "from pylab import mpl, plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # surface plot\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BFGS',\n",
       " 'Bounds',\n",
       " 'HessianUpdateStrategy',\n",
       " 'LbfgsInvHessProduct',\n",
       " 'LinearConstraint',\n",
       " 'NonlinearConstraint',\n",
       " 'OptimizeResult',\n",
       " 'OptimizeWarning',\n",
       " 'RootResults',\n",
       " 'SR1',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__nnls',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_basinhopping',\n",
       " '_bglu_dense',\n",
       " '_cobyla',\n",
       " '_constraints',\n",
       " '_differentiable_functions',\n",
       " '_differentialevolution',\n",
       " '_dual_annealing',\n",
       " '_group_columns',\n",
       " '_hessian_update_strategy',\n",
       " '_lbfgsb',\n",
       " '_linprog',\n",
       " '_linprog_ip',\n",
       " '_linprog_rs',\n",
       " '_linprog_simplex',\n",
       " '_linprog_util',\n",
       " '_lsap',\n",
       " '_lsap_module',\n",
       " '_lsq',\n",
       " '_minimize',\n",
       " '_minpack',\n",
       " '_nnls',\n",
       " '_numdiff',\n",
       " '_remove_redundancy',\n",
       " '_root',\n",
       " '_root_scalar',\n",
       " '_shgo',\n",
       " '_shgo_lib',\n",
       " '_slsqp',\n",
       " '_spectral',\n",
       " '_trlib',\n",
       " '_trustregion',\n",
       " '_trustregion_constr',\n",
       " '_trustregion_dogleg',\n",
       " '_trustregion_exact',\n",
       " '_trustregion_krylov',\n",
       " '_trustregion_ncg',\n",
       " '_zeros',\n",
       " 'anderson',\n",
       " 'approx_fprime',\n",
       " 'basinhopping',\n",
       " 'bisect',\n",
       " 'bracket',\n",
       " 'brent',\n",
       " 'brenth',\n",
       " 'brentq',\n",
       " 'broyden1',\n",
       " 'broyden2',\n",
       " 'brute',\n",
       " 'check_grad',\n",
       " 'cobyla',\n",
       " 'curve_fit',\n",
       " 'diagbroyden',\n",
       " 'differential_evolution',\n",
       " 'dual_annealing',\n",
       " 'excitingmixing',\n",
       " 'fixed_point',\n",
       " 'fmin',\n",
       " 'fmin_bfgs',\n",
       " 'fmin_cg',\n",
       " 'fmin_cobyla',\n",
       " 'fmin_l_bfgs_b',\n",
       " 'fmin_ncg',\n",
       " 'fmin_powell',\n",
       " 'fmin_slsqp',\n",
       " 'fmin_tnc',\n",
       " 'fminbound',\n",
       " 'fsolve',\n",
       " 'golden',\n",
       " 'lbfgsb',\n",
       " 'least_squares',\n",
       " 'leastsq',\n",
       " 'line_search',\n",
       " 'linear_sum_assignment',\n",
       " 'linearmixing',\n",
       " 'linesearch',\n",
       " 'linprog',\n",
       " 'linprog_verbose_callback',\n",
       " 'lsq_linear',\n",
       " 'minimize',\n",
       " 'minimize_scalar',\n",
       " 'minpack',\n",
       " 'minpack2',\n",
       " 'moduleTNC',\n",
       " 'newton',\n",
       " 'newton_krylov',\n",
       " 'nnls',\n",
       " 'nonlin',\n",
       " 'optimize',\n",
       " 'ridder',\n",
       " 'root',\n",
       " 'root_scalar',\n",
       " 'rosen',\n",
       " 'rosen_der',\n",
       " 'rosen_hess',\n",
       " 'rosen_hess_prod',\n",
       " 'shgo',\n",
       " 'show_options',\n",
       " 'slsqp',\n",
       " 'test',\n",
       " 'tnc',\n",
       " 'toms748',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "dir(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.optimize in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.optimize\n",
      "\n",
      "DESCRIPTION\n",
      "    =====================================================\n",
      "    Optimization and root finding (:mod:`scipy.optimize`)\n",
      "    =====================================================\n",
      "    \n",
      "    .. currentmodule:: scipy.optimize\n",
      "    \n",
      "    SciPy ``optimize`` provides functions for minimizing (or maximizing)\n",
      "    objective functions, possibly subject to constraints. It includes\n",
      "    solvers for nonlinear problems (with support for both local and global\n",
      "    optimization algorithms), linear programing, constrained\n",
      "    and nonlinear least-squares, root finding, and curve fitting.\n",
      "    \n",
      "    Common functions and objects, shared across different solvers, are:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       show_options - Show specific options optimization solvers.\n",
      "       OptimizeResult - The optimization result returned by some optimizers.\n",
      "       OptimizeWarning - The optimization encountered problems.\n",
      "    \n",
      "    \n",
      "    Optimization\n",
      "    ============\n",
      "    \n",
      "    Scalar functions optimization\n",
      "    -----------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       minimize_scalar - Interface for minimizers of univariate functions\n",
      "    \n",
      "    The `minimize_scalar` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize_scalar-brent\n",
      "       optimize.minimize_scalar-bounded\n",
      "       optimize.minimize_scalar-golden\n",
      "    \n",
      "    Local (multivariate) optimization\n",
      "    ---------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       minimize - Interface for minimizers of multivariate functions.\n",
      "    \n",
      "    The `minimize` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize-neldermead\n",
      "       optimize.minimize-powell\n",
      "       optimize.minimize-cg\n",
      "       optimize.minimize-bfgs\n",
      "       optimize.minimize-newtoncg\n",
      "       optimize.minimize-lbfgsb\n",
      "       optimize.minimize-tnc\n",
      "       optimize.minimize-cobyla\n",
      "       optimize.minimize-slsqp\n",
      "       optimize.minimize-trustconstr\n",
      "       optimize.minimize-dogleg\n",
      "       optimize.minimize-trustncg\n",
      "       optimize.minimize-trustkrylov\n",
      "       optimize.minimize-trustexact\n",
      "    \n",
      "    Constraints are passed to `minimize` function as a single object or\n",
      "    as a list of objects from the following classes:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       NonlinearConstraint - Class defining general nonlinear constraints.\n",
      "       LinearConstraint - Class defining general linear constraints.\n",
      "    \n",
      "    Simple bound constraints are handled separately and there is a special class\n",
      "    for them:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       Bounds - Bound constraints.\n",
      "    \n",
      "    Quasi-Newton strategies implementing `HessianUpdateStrategy`\n",
      "    interface can be used to approximate the Hessian in `minimize`\n",
      "    function (available only for the 'trust-constr' method). Available\n",
      "    quasi-Newton methods implementing this interface are:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       BFGS - Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "       SR1 - Symmetric-rank-1 Hessian update strategy.\n",
      "    \n",
      "    Global optimization\n",
      "    -------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       basinhopping - Basinhopping stochastic optimizer.\n",
      "       brute - Brute force searching optimizer.\n",
      "       differential_evolution - stochastic minimization using differential evolution.\n",
      "    \n",
      "       shgo - simplicial homology global optimisation\n",
      "       dual_annealing - Dual annealing stochastic optimizer.\n",
      "    \n",
      "    \n",
      "    Least-squares and curve fitting\n",
      "    ===============================\n",
      "    \n",
      "    Nonlinear least-squares\n",
      "    -----------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       least_squares - Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "    \n",
      "    Linear least-squares\n",
      "    --------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       nnls - Linear least-squares problem with non-negativity constraint.\n",
      "       lsq_linear - Linear least-squares problem with bound constraints.\n",
      "    \n",
      "    Curve fitting\n",
      "    -------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       curve_fit -- Fit curve to a set of points.\n",
      "    \n",
      "    Root finding\n",
      "    ============\n",
      "    \n",
      "    Scalar functions\n",
      "    ----------------\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       root_scalar - Unified interface for nonlinear solvers of scalar functions.\n",
      "       brentq - quadratic interpolation Brent method.\n",
      "       brenth - Brent method, modified by Harris with hyperbolic extrapolation.\n",
      "       ridder - Ridder's method.\n",
      "       bisect - Bisection method.\n",
      "       newton - Newton's method (also Secant and Halley's methods).\n",
      "       toms748 - Alefeld, Potra & Shi Algorithm 748.\n",
      "       RootResults - The root finding result returned by some root finders.\n",
      "    \n",
      "    The `root_scalar` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.root_scalar-brentq\n",
      "       optimize.root_scalar-brenth\n",
      "       optimize.root_scalar-bisect\n",
      "       optimize.root_scalar-ridder\n",
      "       optimize.root_scalar-newton\n",
      "       optimize.root_scalar-toms748\n",
      "       optimize.root_scalar-secant\n",
      "       optimize.root_scalar-halley\n",
      "    \n",
      "    \n",
      "    \n",
      "    The table below lists situations and appropriate methods, along with\n",
      "    *asymptotic* convergence rates per iteration (and per function evaluation)\n",
      "    for successful convergence to a simple root(*).\n",
      "    Bisection is the slowest of them all, adding one bit of accuracy for each\n",
      "    function evaluation, but is guaranteed to converge.\n",
      "    The other bracketing methods all (eventually) increase the number of accurate\n",
      "    bits by about 50% for every function evaluation.\n",
      "    The derivative-based methods, all built on `newton`, can converge quite quickly\n",
      "    if the initial value is close to the root.  They can also be applied to\n",
      "    functions defined on (a subset of) the complex plane.\n",
      "    \n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | Domain of f | Bracket? |    Derivatives?      | Solvers     |        Convergence           |\n",
      "    +             +          +----------+-----------+             +-------------+----------------+\n",
      "    |             |          | `fprime` | `fprime2` |             | Guaranteed? |  Rate(s)(*)    |\n",
      "    +=============+==========+==========+===========+=============+=============+================+\n",
      "    | `R`         | Yes      | N/A      | N/A       | - bisection | - Yes       | - 1 \"Linear\"   |\n",
      "    |             |          |          |           | - brentq    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - brenth    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - ridder    | - Yes       | - 2.0 (1.41)   |\n",
      "    |             |          |          |           | - toms748   | - Yes       | - 2.7 (1.65)   |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | No       | No        | secant      | No          | 1.62 (1.62)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | No        | newton      | No          | 2.00 (1.41)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | Yes       | halley      | No          | 3.00 (1.44)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "       `scipy.optimize.cython_optimize` -- Typed Cython versions of zeros functions\n",
      "    \n",
      "    Fixed point finding:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fixed_point - Single-variable fixed-point solver.\n",
      "    \n",
      "    Multidimensional\n",
      "    ----------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       root - Unified interface for nonlinear solvers of multivariate functions.\n",
      "    \n",
      "    The `root` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.root-hybr\n",
      "       optimize.root-lm\n",
      "       optimize.root-broyden1\n",
      "       optimize.root-broyden2\n",
      "       optimize.root-anderson\n",
      "       optimize.root-linearmixing\n",
      "       optimize.root-diagbroyden\n",
      "       optimize.root-excitingmixing\n",
      "       optimize.root-krylov\n",
      "       optimize.root-dfsane\n",
      "    \n",
      "    Linear programming\n",
      "    ==================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linprog -- Unified interface for minimizers of linear programming problems.\n",
      "    \n",
      "    The `linprog` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.linprog-simplex\n",
      "       optimize.linprog-interior-point\n",
      "       optimize.linprog-revised_simplex\n",
      "    \n",
      "    The simplex method supports callback functions, such as:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linprog_verbose_callback -- Sample callback function for linprog (simplex).\n",
      "    \n",
      "    Assignment problems:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linear_sum_assignment -- Solves the linear-sum assignment problem.\n",
      "    \n",
      "    Utilities\n",
      "    =========\n",
      "    \n",
      "    Finite-difference approximation\n",
      "    -------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       approx_fprime - Approximate the gradient of a scalar function.\n",
      "       check_grad - Check the supplied derivative using finite differences.\n",
      "    \n",
      "    \n",
      "    Line search\n",
      "    -----------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       bracket - Bracket a minimum, given two starting points.\n",
      "       line_search - Return a step that satisfies the strong Wolfe conditions.\n",
      "    \n",
      "    Hessian approximation\n",
      "    ---------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       LbfgsInvHessProduct - Linear operator for L-BFGS approximate inverse Hessian.\n",
      "       HessianUpdateStrategy - Interface for implementing Hessian update strategies\n",
      "    \n",
      "    Benchmark problems\n",
      "    ------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       rosen - The Rosenbrock function.\n",
      "       rosen_der - The derivative of the Rosenbrock function.\n",
      "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
      "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
      "    \n",
      "    Legacy functions\n",
      "    ================\n",
      "    \n",
      "    The functions below are not recommended for use in new scripts;\n",
      "    all of these methods are accessible via a newer, more consistent\n",
      "    interfaces, provided by the interfaces above.\n",
      "    \n",
      "    Optimization\n",
      "    ------------\n",
      "    \n",
      "    General-purpose multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin - Nelder-Mead Simplex algorithm.\n",
      "       fmin_powell - Powell's (modified) level set method.\n",
      "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm.\n",
      "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno).\n",
      "       fmin_ncg - Line-search Newton Conjugate Gradient.\n",
      "    \n",
      "    Constrained multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer.\n",
      "       fmin_tnc - Truncated Newton code.\n",
      "       fmin_cobyla - Constrained optimization by linear approximation.\n",
      "       fmin_slsqp - Minimization using sequential least-squares programming.\n",
      "    \n",
      "    Univariate (scalar) minimization methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fminbound - Bounded minimization of a scalar function.\n",
      "       brent - 1-D function minimization using Brent method.\n",
      "       golden - 1-D function minimization using Golden Section method.\n",
      "    \n",
      "    Least-squares\n",
      "    -------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       leastsq - Minimize the sum of squares of M equations in N unknowns.\n",
      "    \n",
      "    Root finding\n",
      "    ------------\n",
      "    \n",
      "    General nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fsolve - Non-linear multivariable equation solver.\n",
      "       broyden1 - Broyden's first method.\n",
      "       broyden2 - Broyden's second method.\n",
      "    \n",
      "    Large-scale nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       newton_krylov\n",
      "       anderson\n",
      "    \n",
      "    Simple iteration solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       excitingmixing\n",
      "       linearmixing\n",
      "       diagbroyden\n",
      "    \n",
      "    :mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __nnls\n",
      "    _basinhopping\n",
      "    _bglu_dense\n",
      "    _cobyla\n",
      "    _constraints\n",
      "    _differentiable_functions\n",
      "    _differentialevolution\n",
      "    _dual_annealing\n",
      "    _group_columns\n",
      "    _hessian_update_strategy\n",
      "    _lbfgsb\n",
      "    _linprog\n",
      "    _linprog_ip\n",
      "    _linprog_rs\n",
      "    _linprog_simplex\n",
      "    _linprog_util\n",
      "    _lsap\n",
      "    _lsap_module\n",
      "    _lsq (package)\n",
      "    _minimize\n",
      "    _minpack\n",
      "    _nnls\n",
      "    _numdiff\n",
      "    _remove_redundancy\n",
      "    _root\n",
      "    _root_scalar\n",
      "    _shgo\n",
      "    _shgo_lib (package)\n",
      "    _slsqp\n",
      "    _spectral\n",
      "    _trlib (package)\n",
      "    _trustregion\n",
      "    _trustregion_constr (package)\n",
      "    _trustregion_dogleg\n",
      "    _trustregion_exact\n",
      "    _trustregion_krylov\n",
      "    _trustregion_ncg\n",
      "    _tstutils\n",
      "    _zeros\n",
      "    cobyla\n",
      "    cython_optimize (package)\n",
      "    lbfgsb\n",
      "    linesearch\n",
      "    minpack\n",
      "    minpack2\n",
      "    moduleTNC\n",
      "    nonlin\n",
      "    optimize\n",
      "    setup\n",
      "    slsqp\n",
      "    tests (package)\n",
      "    tnc\n",
      "    zeros\n",
      "\n",
      "CLASSES\n",
      "    builtins.UserWarning(builtins.Warning)\n",
      "        scipy.optimize.optimize.OptimizeWarning\n",
      "    builtins.dict(builtins.object)\n",
      "        scipy.optimize.optimize.OptimizeResult\n",
      "    builtins.object\n",
      "        scipy.optimize._constraints.Bounds\n",
      "        scipy.optimize._constraints.LinearConstraint\n",
      "        scipy.optimize._constraints.NonlinearConstraint\n",
      "        scipy.optimize._hessian_update_strategy.HessianUpdateStrategy\n",
      "        scipy.optimize.zeros.RootResults\n",
      "    scipy.optimize._hessian_update_strategy.FullHessianUpdateStrategy(scipy.optimize._hessian_update_strategy.HessianUpdateStrategy)\n",
      "        scipy.optimize._hessian_update_strategy.BFGS\n",
      "        scipy.optimize._hessian_update_strategy.SR1\n",
      "    scipy.sparse.linalg.interface.LinearOperator(builtins.object)\n",
      "        scipy.optimize.lbfgsb.LbfgsInvHessProduct\n",
      "    \n",
      "    class BFGS(FullHessianUpdateStrategy)\n",
      "     |  BFGS(exception_strategy='skip_update', min_curvature=None, init_scale='auto')\n",
      "     |  \n",
      "     |  Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  exception_strategy : {'skip_update', 'damp_update'}, optional\n",
      "     |      Define how to proceed when the curvature condition is violated.\n",
      "     |      Set it to 'skip_update' to just skip the update. Or, alternatively,\n",
      "     |      set it to 'damp_update' to interpolate between the actual BFGS\n",
      "     |      result and the unmodified matrix. Both exceptions strategies\n",
      "     |      are explained  in [1]_, p.536-537.\n",
      "     |  min_curvature : float\n",
      "     |      This number, scaled by a normalization factor, defines the\n",
      "     |      minimum curvature ``dot(delta_grad, delta_x)`` allowed to go\n",
      "     |      unaffected by the exception strategy. By default is equal to\n",
      "     |      1e-8 when ``exception_strategy = 'skip_update'`` and equal\n",
      "     |      to 0.2 when ``exception_strategy = 'damp_update'``.\n",
      "     |  init_scale : {float, 'auto'}\n",
      "     |      Matrix scale at first iteration. At the first\n",
      "     |      iteration the Hessian matrix or its inverse will be initialized\n",
      "     |      with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      By default uses 'auto'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.140.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BFGS\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, exception_strategy='skip_update', min_curvature=None, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Bounds(builtins.object)\n",
      "     |  Bounds(lb, ub, keep_feasible=False)\n",
      "     |  \n",
      "     |  Bounds constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= x <= ub\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  lb, ub : array_like, optional\n",
      "     |      Lower and upper bounds on independent variables. Each array must\n",
      "     |      have the same size as x or be a scalar, in which case a bound will be\n",
      "     |      the same for all the variables. Set components of `lb` and `ub` equal\n",
      "     |      to fix a variable. Use ``np.inf`` with an appropriate sign to disable\n",
      "     |      bounds on all or some variables. Note that you can mix constraints of\n",
      "     |      different types: interval, one-sided or equality, by setting different\n",
      "     |      components of `lb` and `ub` as necessary.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lb, ub, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class HessianUpdateStrategy(builtins.object)\n",
      "     |  Interface for implementing Hessian update strategies.\n",
      "     |  \n",
      "     |  Many optimization methods make use of Hessian (or inverse Hessian)\n",
      "     |  approximations, such as the quasi-Newton methods BFGS, SR1, L-BFGS.\n",
      "     |  Some of these  approximations, however, do not actually need to store\n",
      "     |  the entire matrix or can compute the internal matrix product with a\n",
      "     |  given vector in a very efficiently manner. This class serves as an\n",
      "     |  abstract interface between the optimization algorithm and the\n",
      "     |  quasi-Newton update strategies, giving freedom of implementation\n",
      "     |  to store and update the internal matrix as efficiently as possible.\n",
      "     |  Different choices of initialization and update procedure will result\n",
      "     |  in different quasi-Newton strategies.\n",
      "     |  \n",
      "     |  Four methods should be implemented in derived classes: ``initialize``,\n",
      "     |  ``update``, ``dot`` and ``get_matrix``.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Any instance of a class that implements this interface,\n",
      "     |  can be accepted by the method ``minimize`` and used by\n",
      "     |  the compatible solvers to approximate the Hessian (or\n",
      "     |  inverse Hessian) used by the optimization algorithms.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      H : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian\n",
      "     |          or its inverse (depending on how 'approx_type'\n",
      "     |          is defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LbfgsInvHessProduct(scipy.sparse.linalg.interface.LinearOperator)\n",
      "     |  LbfgsInvHessProduct(*args, **kwargs)\n",
      "     |  \n",
      "     |  Linear operator for the L-BFGS approximate inverse Hessian.\n",
      "     |  \n",
      "     |  This operator computes the product of a vector with the approximate inverse\n",
      "     |  of the Hessian of the objective function, using the L-BFGS limited\n",
      "     |  memory approximation to the inverse Hessian, accumulated during the\n",
      "     |  optimization.\n",
      "     |  \n",
      "     |  Objects of this class implement the ``scipy.sparse.linalg.LinearOperator``\n",
      "     |  interface.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the solution vector.\n",
      "     |      (See [1]).\n",
      "     |  yk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the gradient. (See [1]).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge. \"Updating quasi-Newton matrices with limited\n",
      "     |     storage.\" Mathematics of computation 35.151 (1980): 773-782.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LbfgsInvHessProduct\n",
      "     |      scipy.sparse.linalg.interface.LinearOperator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sk, yk)\n",
      "     |      Construct the operator.\n",
      "     |  \n",
      "     |  todense(self)\n",
      "     |      Return a dense array representation of this operator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arr : ndarray, shape=(n, n)\n",
      "     |          An array with the same shape and containing\n",
      "     |          the same data represented by this `LinearOperator`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __add__(self, x)\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __matmul__(self, other)\n",
      "     |  \n",
      "     |  __mul__(self, x)\n",
      "     |  \n",
      "     |  __neg__(self)\n",
      "     |  \n",
      "     |  __pow__(self, p)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmatmul__(self, other)\n",
      "     |  \n",
      "     |  __rmul__(self, x)\n",
      "     |  \n",
      "     |  __sub__(self, x)\n",
      "     |  \n",
      "     |  adjoint(self)\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  dot(self, x)\n",
      "     |      Matrix-matrix or matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          1-d or 2-d array, representing a vector or matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Ax : array\n",
      "     |          1-d or 2-d array (depending on the shape of x) that represents\n",
      "     |          the result of applying this linear operator on x.\n",
      "     |  \n",
      "     |  matmat(self, X)\n",
      "     |      Matrix-matrix multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*X where A is an MxN linear\n",
      "     |      operator and X dense N*K matrix or ndarray.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          An array with shape (N,K).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,K) depending on\n",
      "     |          the type of the X argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matmat wraps any user-specified matmat routine or overridden\n",
      "     |      _matmat method to ensure that y has the correct type.\n",
      "     |  \n",
      "     |  matvec(self, x)\n",
      "     |      Matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (N,) or (N,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,) or (M,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matvec wraps the user-specified matvec routine or overridden\n",
      "     |      _matvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  rmatmat(self, X)\n",
      "     |      Adjoint matrix-matrix multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y = A^H * x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array, or 2-d array.\n",
      "     |      The default implementation defers to the adjoint.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          A matrix or 2D array.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or 2D array depending on the type of the input.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatmat wraps the user-specified rmatmat routine.\n",
      "     |  \n",
      "     |  rmatvec(self, x)\n",
      "     |      Adjoint matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y = A^H * x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (M,) or (M,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (N,) or (N,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatvec wraps the user-specified rmatvec routine or overridden\n",
      "     |      _rmatvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  transpose(self)\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwargs)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  H\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  T\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  ndim = 2\n",
      "    \n",
      "    class LinearConstraint(builtins.object)\n",
      "     |  LinearConstraint(A, lb, ub, keep_feasible=False)\n",
      "     |  \n",
      "     |  Linear constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= A.dot(x) <= ub\n",
      "     |  \n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and the matrix A has shape (m, n).\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  A : {array_like, sparse matrix}, shape (m, n)\n",
      "     |      Matrix defining the constraint.\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, A, lb, ub, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NonlinearConstraint(builtins.object)\n",
      "     |  NonlinearConstraint(fun, lb, ub, jac='2-point', hess=<scipy.optimize._hessian_update_strategy.BFGS object at 0x12d689130>, keep_feasible=False, finite_diff_rel_step=None, finite_diff_jac_sparsity=None)\n",
      "     |  \n",
      "     |  Nonlinear constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= fun(x) <= ub\n",
      "     |  \n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and ``fun`` returns a vector with m components.\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fun : callable\n",
      "     |      The function defining the constraint.\n",
      "     |      The signature is ``fun(x) -> array_like, shape (m,)``.\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary.\n",
      "     |  jac : {callable,  '2-point', '3-point', 'cs'}, optional\n",
      "     |      Method of computing the Jacobian matrix (an m-by-n matrix,\n",
      "     |      where element (i, j) is the partial derivative of f[i] with\n",
      "     |      respect to x[j]).  The keywords {'2-point', '3-point',\n",
      "     |      'cs'} select a finite difference scheme for the numerical estimation.\n",
      "     |      A callable must have the following signature:\n",
      "     |      ``jac(x) -> {ndarray, sparse matrix}, shape (m, n)``.\n",
      "     |      Default is '2-point'.\n",
      "     |  hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy, None}, optional\n",
      "     |      Method for computing the Hessian matrix. The keywords\n",
      "     |      {'2-point', '3-point', 'cs'} select a finite difference scheme for\n",
      "     |      numerical  estimation.  Alternatively, objects implementing\n",
      "     |      `HessianUpdateStrategy` interface can be used to approximate the\n",
      "     |      Hessian. Currently available implementations are:\n",
      "     |  \n",
      "     |          - `BFGS` (default option)\n",
      "     |          - `SR1`\n",
      "     |  \n",
      "     |      A callable must return the Hessian matrix of ``dot(fun, v)`` and\n",
      "     |      must have the following signature:\n",
      "     |      ``hess(x, v) -> {LinearOperator, sparse matrix, array_like}, shape (n, n)``.\n",
      "     |      Here ``v`` is ndarray with shape (m,) containing Lagrange multipliers.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  finite_diff_rel_step: None or array_like, optional\n",
      "     |      Relative step size for the finite difference approximation. Default is\n",
      "     |      None, which will select a reasonable value automatically depending\n",
      "     |      on a finite difference scheme.\n",
      "     |  finite_diff_jac_sparsity: {None, array_like, sparse matrix}, optional\n",
      "     |      Defines the sparsity structure of the Jacobian matrix for finite\n",
      "     |      difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "     |      only few non-zero elements in *each* row, providing the sparsity\n",
      "     |      structure will greatly speed up the computations. A zero entry means\n",
      "     |      that a corresponding element in the Jacobian is identically zero.\n",
      "     |      If provided, forces the use of 'lsmr' trust-region solver.\n",
      "     |      If None (default) then dense differencing will be used.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Finite difference schemes {'2-point', '3-point', 'cs'} may be used for\n",
      "     |  approximating either the Jacobian or the Hessian. We, however, do not allow\n",
      "     |  its use for approximating both simultaneously. Hence whenever the Jacobian\n",
      "     |  is estimated via finite-differences, we require the Hessian to be estimated\n",
      "     |  using one of the quasi-Newton strategies.\n",
      "     |  \n",
      "     |  The scheme 'cs' is potentially the most accurate, but requires the function\n",
      "     |  to correctly handles complex inputs and be analytically continuable to the\n",
      "     |  complex plane. The scheme '3-point' is more accurate than '2-point' but\n",
      "     |  requires twice as many operations.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Constrain ``x[0] < sin(x[1]) + 1.9``\n",
      "     |  \n",
      "     |  >>> from scipy.optimize import NonlinearConstraint\n",
      "     |  >>> con = lambda x: x[0] - np.sin(x[1])\n",
      "     |  >>> nlc = NonlinearConstraint(con, -np.inf, 1.9)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fun, lb, ub, jac='2-point', hess=<scipy.optimize._hessian_update_strategy.BFGS object at 0x12d689130>, keep_feasible=False, finite_diff_rel_step=None, finite_diff_jac_sparsity=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class OptimizeResult(builtins.dict)\n",
      "     |  Represents the optimization result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  x : ndarray\n",
      "     |      The solution of the optimization.\n",
      "     |  success : bool\n",
      "     |      Whether or not the optimizer exited successfully.\n",
      "     |  status : int\n",
      "     |      Termination status of the optimizer. Its value depends on the\n",
      "     |      underlying solver. Refer to `message` for details.\n",
      "     |  message : str\n",
      "     |      Description of the cause of the termination.\n",
      "     |  fun, jac, hess: ndarray\n",
      "     |      Values of objective function, its Jacobian and its Hessian (if\n",
      "     |      available). The Hessians may be approximations, see the documentation\n",
      "     |      of the function in question.\n",
      "     |  hess_inv : object\n",
      "     |      Inverse of the objective function's Hessian; may be an approximation.\n",
      "     |      Not available for all solvers. The type of this attribute may be\n",
      "     |      either np.ndarray or scipy.sparse.linalg.LinearOperator.\n",
      "     |  nfev, njev, nhev : int\n",
      "     |      Number of evaluations of the objective functions and of its\n",
      "     |      Jacobian and Hessian.\n",
      "     |  nit : int\n",
      "     |      Number of iterations performed by the optimizer.\n",
      "     |  maxcv : float\n",
      "     |      The maximum constraint violation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There may be additional attributes not listed above depending of the\n",
      "     |  specific solver. Since this class is essentially a subclass of dict\n",
      "     |  with attribute accessors, one can see which attributes are available\n",
      "     |  using the `keys()` method.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeResult\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delattr__ = __delitem__(self, key, /)\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__ = __setitem__(self, key, value, /)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __reversed__(self, /)\n",
      "     |      Return a reverse iterator over the dict keys.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      D.copy() -> a shallow copy of D\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      "     |  \n",
      "     |  popitem(self, /)\n",
      "     |      Remove and return a (key, value) pair as a 2-tuple.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      "     |      Raises KeyError if the dict is empty.\n",
      "     |  \n",
      "     |  setdefault(self, key, default=None, /)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None, /) from builtins.type\n",
      "     |      Create a new dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class OptimizeWarning(builtins.UserWarning)\n",
      "     |  Base class for warnings generated by user code.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeWarning\n",
      "     |      builtins.UserWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class RootResults(builtins.object)\n",
      "     |  RootResults(root, iterations, function_calls, flag)\n",
      "     |  \n",
      "     |  Represents the root finding result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  root : float\n",
      "     |      Estimated root location.\n",
      "     |  iterations : int\n",
      "     |      Number of iterations needed to find the root.\n",
      "     |  function_calls : int\n",
      "     |      Number of times the function was called.\n",
      "     |  converged : bool\n",
      "     |      True if the routine converged.\n",
      "     |  flag : str\n",
      "     |      Description of the cause of termination.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, iterations, function_calls, flag)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SR1(FullHessianUpdateStrategy)\n",
      "     |  SR1(min_denominator=1e-08, init_scale='auto')\n",
      "     |  \n",
      "     |  Symmetric-rank-1 Hessian update strategy.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  min_denominator : float\n",
      "     |      This number, scaled by a normalization factor,\n",
      "     |      defines the minimum denominator magnitude allowed\n",
      "     |      in the update. When the condition is violated we skip\n",
      "     |      the update. By default uses ``1e-8``.\n",
      "     |  init_scale : {float, 'auto'}, optional\n",
      "     |      Matrix scale at first iteration. At the first\n",
      "     |      iteration the Hessian matrix or its inverse will be initialized\n",
      "     |      with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      By default uses 'auto'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.144-146.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SR1\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, min_denominator=1e-08, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using (extended) Anderson mixing.\n",
      "        \n",
      "        The Jacobian is formed by for a 'best' solution in the space\n",
      "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
      "        inversions and MxN multiplications are required. [Ey]_\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        M : float, optional\n",
      "            Number of previous vectors to retain. Defaults to 5.\n",
      "        w0 : float, optional\n",
      "            Regularization parameter for numerical stability.\n",
      "            Compared to unity, good values of the order of 0.01.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='anderson'`` in particular.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
      "    \n",
      "    approx_fprime(xk, f, epsilon, *args)\n",
      "        Finite-difference approximation of the gradient of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xk : array_like\n",
      "            The coordinate vector at which to determine the gradient of `f`.\n",
      "        f : callable\n",
      "            The function of which to determine the gradient (partial derivatives).\n",
      "            Should take `xk` as first argument, other arguments to `f` can be\n",
      "            supplied in ``*args``. Should return a scalar, the value of the\n",
      "            function at `xk`.\n",
      "        epsilon : array_like\n",
      "            Increment to `xk` to use for determining the function gradient.\n",
      "            If a scalar, uses the same finite difference delta for all partial\n",
      "            derivatives. If an array, should contain one value per element of\n",
      "            `xk`.\n",
      "        \\*args : args, optional\n",
      "            Any other arguments that are to be passed to `f`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        grad : ndarray\n",
      "            The partial derivatives of `f` to `xk`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        check_grad : Check correctness of gradient function against approx_fprime.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function gradient is determined by the forward finite difference\n",
      "        formula::\n",
      "        \n",
      "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
      "            f'[i] = ---------------------------------\n",
      "                                epsilon[i]\n",
      "        \n",
      "        The main use of `approx_fprime` is in scalar function optimizers like\n",
      "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c0, c1):\n",
      "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
      "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
      "        \n",
      "        >>> x = np.ones(2)\n",
      "        >>> c0, c1 = (1, 200)\n",
      "        >>> eps = np.sqrt(np.finfo(float).eps)\n",
      "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
      "        array([   2.        ,  400.00004198])\n",
      "    \n",
      "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None, seed=None)\n",
      "        Find the global minimum of a function using the basin-hopping algorithm\n",
      "        \n",
      "        Basin-hopping is a two-phase method that combines a global stepping\n",
      "        algorithm with local minimization at each step. Designed to mimic\n",
      "        the natural process of energy minimization of clusters of atoms, it works\n",
      "        well for similar problems with \"funnel-like, but rugged\" energy landscapes\n",
      "        [5]_.\n",
      "        \n",
      "        As the step-taking, step acceptance, and minimization methods are all\n",
      "        customizable, this function can also be used to implement other two-phase\n",
      "        methods.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            Function to be optimized.  ``args`` can be passed as an optional item\n",
      "            in the dict ``minimizer_kwargs``\n",
      "        x0 : array_like\n",
      "            Initial guess.\n",
      "        niter : integer, optional\n",
      "            The number of basin-hopping iterations\n",
      "        T : float, optional\n",
      "            The \"temperature\" parameter for the accept or reject criterion. Higher\n",
      "            \"temperatures\" mean that larger jumps in function value will be\n",
      "            accepted.  For best results ``T`` should be comparable to the\n",
      "            separation (in function value) between local minima.\n",
      "        stepsize : float, optional\n",
      "            Maximum step size for use in the random displacement.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the local minimizer\n",
      "            ``scipy.optimize.minimize()`` Some important options could be:\n",
      "        \n",
      "                method : str\n",
      "                    The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
      "                args : tuple\n",
      "                    Extra arguments passed to the objective function (``func``) and\n",
      "                    its derivatives (Jacobian, Hessian).\n",
      "        \n",
      "        take_step : callable ``take_step(x)``, optional\n",
      "            Replace the default step-taking routine with this routine. The default\n",
      "            step-taking routine is a random displacement of the coordinates, but\n",
      "            other step-taking algorithms may be better for some systems.\n",
      "            ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
      "            If this attribute exists, then ``basinhopping`` will adjust\n",
      "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
      "            search.\n",
      "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
      "            Define a test which will be used to judge whether or not to accept the\n",
      "            step.  This will be used in addition to the Metropolis test based on\n",
      "            \"temperature\" ``T``.  The acceptable return values are True,\n",
      "            False, or ``\"force accept\"``. If any of the tests return False\n",
      "            then the step is rejected. If the latter, then this will override any\n",
      "            other tests in order to accept the step. This can be used, for example,\n",
      "            to forcefully escape from a local minimum that ``basinhopping`` is\n",
      "            trapped in.\n",
      "        callback : callable, ``callback(x, f, accept)``, optional\n",
      "            A callback function which will be called for all minima found. ``x``\n",
      "            and ``f`` are the coordinates and function value of the trial minimum,\n",
      "            and ``accept`` is whether or not that minimum was accepted. This can\n",
      "            be used, for example, to save the lowest N minima found. Also,\n",
      "            ``callback`` can be used to specify a user defined stop criterion by\n",
      "            optionally returning True to stop the ``basinhopping`` routine.\n",
      "        interval : integer, optional\n",
      "            interval for how often to update the ``stepsize``\n",
      "        disp : bool, optional\n",
      "            Set to True to print status messages\n",
      "        niter_success : integer, optional\n",
      "            Stop the run if the global minimum candidate remains the same for this\n",
      "            number of iterations.\n",
      "        seed : {int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "            If `seed` is not specified the `~np.random.RandomState` singleton is\n",
      "            used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
      "            with seed.\n",
      "            If `seed` is already a ``RandomState`` or ``Generator`` instance, then\n",
      "            that object is used.\n",
      "            Specify `seed` for repeatable minimizations. The random numbers\n",
      "            generated with this seed only affect the default Metropolis\n",
      "            `accept_test` and the default `take_step`. If you supply your own\n",
      "            `take_step` and `accept_test`, and these functions use random\n",
      "            number generation, then those functions are responsible for the state\n",
      "            of their random number generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination. The ``OptimizeResult`` object returned by the\n",
      "            selected minimizer at the lowest minimum is also contained within this\n",
      "            object and can be accessed through the ``lowest_optimization_result``\n",
      "            attribute.  See `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize :\n",
      "            The local minimization function called once for each basinhopping step.\n",
      "            ``minimizer_kwargs`` is passed to this routine.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
      "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
      "        [4]_. The algorithm in its current form was described by David Wales and\n",
      "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
      "        \n",
      "        The algorithm is iterative with each cycle composed of the following\n",
      "        features\n",
      "        \n",
      "        1) random perturbation of the coordinates\n",
      "        \n",
      "        2) local minimization\n",
      "        \n",
      "        3) accept or reject the new coordinates based on the minimized function\n",
      "           value\n",
      "        \n",
      "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
      "        Carlo algorithms, although there are many other possibilities [3]_.\n",
      "        \n",
      "        This global minimization method has been shown to be extremely efficient\n",
      "        for a wide variety of problems in physics and chemistry. It is\n",
      "        particularly useful when the function has many minima separated by large\n",
      "        barriers. See the Cambridge Cluster Database\n",
      "        http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
      "        that have been optimized primarily using basin-hopping. This database\n",
      "        includes minimization problems exceeding 300 degrees of freedom.\n",
      "        \n",
      "        See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
      "        a Fortran implementation of basin-hopping. This implementation has many\n",
      "        different variations of the procedure described above, including more\n",
      "        advanced step taking algorithms and alternate acceptance criterion.\n",
      "        \n",
      "        For stochastic global optimization there is no way to determine if the true\n",
      "        global minimum has actually been found. Instead, as a consistency check,\n",
      "        the algorithm can be run from a number of different random starting points\n",
      "        to ensure the lowest minimum found in each example has converged to the\n",
      "        global minimum. For this reason, ``basinhopping`` will by default simply\n",
      "        run for the number of iterations ``niter`` and return the lowest minimum\n",
      "        found. It is left to the user to ensure that this is in fact the global\n",
      "        minimum.\n",
      "        \n",
      "        Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
      "        depends on the problem being solved. The step is chosen uniformly in the\n",
      "        region from x0-stepsize to x0+stepsize, in each dimension. Ideally, it\n",
      "        should be comparable to the typical separation (in argument values) between\n",
      "        local minima of the function being optimized. ``basinhopping`` will, by\n",
      "        default, adjust ``stepsize`` to find an optimal value, but this may take\n",
      "        many iterations. You will get quicker results if you set a sensible\n",
      "        initial value for ``stepsize``.\n",
      "        \n",
      "        Choosing ``T``: The parameter ``T`` is the \"temperature\" used in the\n",
      "        Metropolis criterion. Basinhopping steps are always accepted if\n",
      "        ``func(xnew) < func(xold)``. Otherwise, they are accepted with\n",
      "        probability::\n",
      "        \n",
      "            exp( -(func(xnew) - func(xold)) / T )\n",
      "        \n",
      "        So, for best results, ``T`` should to be comparable to the typical\n",
      "        difference (in function values) between local minima. (The height of\n",
      "        \"walls\" between local minima is irrelevant.)\n",
      "        \n",
      "        If ``T`` is 0, the algorithm becomes Monotonic Basin-Hopping, in which all\n",
      "        steps that increase energy are rejected.\n",
      "        \n",
      "        .. versionadded:: 0.12.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
      "            Cambridge, UK.\n",
      "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
      "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
      "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
      "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
      "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
      "            1987, 84, 6611.\n",
      "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
      "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
      "        .. [5] Olson, B., Hashmi, I., Molloy, K., and Shehu1, A., Basin Hopping as\n",
      "            a General and Versatile Optimization Framework for the Characterization\n",
      "            of Biological Macromolecules, Advances in Artificial Intelligence,\n",
      "            Volume 2012 (2012), Article ID 674832, :doi:`10.1155/2012/674832`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 1-D minimization problem, with many\n",
      "        local minima superimposed on a parabola.\n",
      "        \n",
      "        >>> from scipy.optimize import basinhopping\n",
      "        >>> func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
      "        >>> x0=[1.]\n",
      "        \n",
      "        Basinhopping, internally, uses a local minimization algorithm. We will use\n",
      "        the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
      "        use and how to set up that minimizer. This parameter will be passed to\n",
      "        ``scipy.optimize.minimize()``.\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
      "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
      "        global minimum: x = -0.1951, f(x0) = -1.0009\n",
      "        \n",
      "        Next consider a 2-D minimization problem. Also, this time, we\n",
      "        will use gradient information to significantly speed up the search.\n",
      "        \n",
      "        >>> def func2d(x):\n",
      "        ...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
      "        ...                                                            0.2) * x[0]\n",
      "        ...     df = np.zeros(2)\n",
      "        ...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
      "        ...     df[1] = 2. * x[1] + 0.2\n",
      "        ...     return f, df\n",
      "        \n",
      "        We'll also use a different local minimization algorithm. Also, we must tell\n",
      "        the minimizer that our function returns both energy and gradient (Jacobian).\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
      "        >>> x0 = [1.0, 1.0]\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Here is an example using a custom step-taking routine. Imagine you want\n",
      "        the first coordinate to take larger steps than the rest of the coordinates.\n",
      "        This can be implemented like so:\n",
      "        \n",
      "        >>> class MyTakeStep(object):\n",
      "        ...    def __init__(self, stepsize=0.5):\n",
      "        ...        self.stepsize = stepsize\n",
      "        ...    def __call__(self, x):\n",
      "        ...        s = self.stepsize\n",
      "        ...        x[0] += np.random.uniform(-2.*s, 2.*s)\n",
      "        ...        x[1:] += np.random.uniform(-s, s, x[1:].shape)\n",
      "        ...        return x\n",
      "        \n",
      "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
      "        of ``stepsize`` to optimize the search. We'll use the same 2-D function as\n",
      "        before\n",
      "        \n",
      "        >>> mytakestep = MyTakeStep()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200, take_step=mytakestep)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Now, let's do an example using a custom callback function which prints the\n",
      "        value of every minimum found\n",
      "        \n",
      "        >>> def print_fun(x, f, accepted):\n",
      "        ...         print(\"at minimum %.4f accepted %d\" % (f, int(accepted)))\n",
      "        \n",
      "        We'll run it for only 10 basinhopping steps this time.\n",
      "        \n",
      "        >>> np.random.seed(1)\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, callback=print_fun)\n",
      "        at minimum 0.4159 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum 0.9102 accepted 1\n",
      "        at minimum 0.9102 accepted 1\n",
      "        at minimum 2.2945 accepted 0\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        \n",
      "        \n",
      "        The minimum at -1.0109 is actually the global minimum, found already on the\n",
      "        8th iteration.\n",
      "        \n",
      "        Now let's implement bounds on the problem using a custom ``accept_test``:\n",
      "        \n",
      "        >>> class MyBounds(object):\n",
      "        ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
      "        ...         self.xmax = np.array(xmax)\n",
      "        ...         self.xmin = np.array(xmin)\n",
      "        ...     def __call__(self, **kwargs):\n",
      "        ...         x = kwargs[\"x_new\"]\n",
      "        ...         tmax = bool(np.all(x <= self.xmax))\n",
      "        ...         tmin = bool(np.all(x >= self.xmin))\n",
      "        ...         return tmax and tmin\n",
      "        \n",
      "        >>> mybounds = MyBounds()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, accept_test=mybounds)\n",
      "    \n",
      "    bisect(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find root of a function within an interval using bisection.\n",
      "        \n",
      "        Basic bisection routine to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. `f(a)` and `f(b)` cannot have the same signs.\n",
      "        Slow but sure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  `f` must be continuous, and\n",
      "            f(a) and f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in a `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.bisect(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        >>> root = optimize.bisect(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        fsolve : n-dimensional root-finding\n",
      "    \n",
      "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
      "        Bracket the minimum of the function.\n",
      "        \n",
      "        Given a function and distinct initial points, search in the\n",
      "        downhill direction (as defined by the initial points) and return\n",
      "        new points xa, xb, xc that bracket the minimum of the function\n",
      "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
      "        solution will satisfy xa<=x<=xb.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to minimize.\n",
      "        xa, xb : float, optional\n",
      "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to `func`.\n",
      "        grow_limit : float, optional\n",
      "            Maximum grow limit.  Defaults to 110.0\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Defaults to 1000.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xa, xb, xc : float\n",
      "            Bracket.\n",
      "        fa, fb, fc : float\n",
      "            Objective function values in bracket.\n",
      "        funcalls : int\n",
      "            Number of function evaluations made.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        This function can find a downward convex region of a function:\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import bracket\n",
      "        >>> def f(x):\n",
      "        ...     return 10*x**2 + 3*x + 5\n",
      "        >>> x = np.linspace(-2, 2)\n",
      "        >>> y = f(x)\n",
      "        >>> init_xa, init_xb = 0, 1\n",
      "        >>> xa, xb, xc, fa, fb, fc, funcalls = bracket(f, xa=init_xa, xb=init_xb)\n",
      "        >>> plt.axvline(x=init_xa, color=\"k\", linestyle=\"--\")\n",
      "        >>> plt.axvline(x=init_xb, color=\"k\", linestyle=\"--\")\n",
      "        >>> plt.plot(x, y, \"-k\")\n",
      "        >>> plt.plot(xa, fa, \"bx\")\n",
      "        >>> plt.plot(xb, fb, \"rx\")\n",
      "        >>> plt.plot(xc, fc, \"bx\")\n",
      "        >>> plt.show()\n",
      "    \n",
      "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
      "        Given a function of one variable and a possible bracket, return\n",
      "        the local minimum of the function isolated to a fractional precision\n",
      "        of tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present).\n",
      "        brack : tuple, optional\n",
      "            Either a triple (xa,xb,xc) where xa<xb<xc and func(xb) <\n",
      "            func(xa), func(xc) or a pair (xa,xb) which are used as a\n",
      "            starting interval for a downhill bracket search (see\n",
      "            `bracket`). Providing the pair (xa,xb) does not always mean\n",
      "            the obtained solution will satisfy xa<=x<=xb.\n",
      "        tol : float, optional\n",
      "            Stop if between iteration change is less than `tol`.\n",
      "        full_output : bool, optional\n",
      "            If True, return all output args (xmin, fval, iter,\n",
      "            funcalls).\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations in solution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xmin : ndarray\n",
      "            Optimum point.\n",
      "        fval : float\n",
      "            Optimum value.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of objective function evaluations made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Brent' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses inverse parabolic interpolation when possible to speed up\n",
      "        convergence of golden section method.\n",
      "        \n",
      "        Does not ensure that the minimum lies in the range specified by\n",
      "        `brack`. See `fminbound`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3 respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range (xa,xb).\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.brent(f,brack=(1,2))\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.brent(f,brack=(-1,0.5,2))\n",
      "        >>> minimum\n",
      "        -2.7755575615628914e-17\n",
      "    \n",
      "    brenth(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's\n",
      "        method with hyperbolic extrapolation.\n",
      "        \n",
      "        A variation on the classic Brent routine to find a zero of the function f\n",
      "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
      "        inverse quadratic extrapolation. There was a paper back in the 1980's ...\n",
      "        f(a) and f(b) cannot have the same signs. Generally, on a par with the\n",
      "        brent routine, but not as heavily tested. It is a safe version of the\n",
      "        secant method that uses hyperbolic extrapolation. The version here is by\n",
      "        Chuck Harris.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. As with `brentq`, for nice\n",
      "            functions the method will often satisfy the above condition\n",
      "            with ``xtol/2`` and ``rtol/2``.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. As with `brentq`, for nice functions\n",
      "            the method will often satisfy the above condition with\n",
      "            ``xtol/2`` and ``rtol/2``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.brenth(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        >>> root = optimize.brenth(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fmin, fmin_powell, fmin_cg,\n",
      "               fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
      "        \n",
      "        leastsq : nonlinear least squares minimizer\n",
      "        \n",
      "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
      "        \n",
      "        basinhopping, differential_evolution, brute : global optimizers\n",
      "        \n",
      "        fminbound, brent, golden, bracket : local scalar minimizers\n",
      "        \n",
      "        fsolve : N-D root-finding\n",
      "        \n",
      "        brentq, brenth, ridder, bisect, newton : 1-D root-finding\n",
      "        \n",
      "        fixed_point : scalar fixed-point finder\n",
      "    \n",
      "    brentq(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's method.\n",
      "        \n",
      "        Uses the classic Brent's method to find a zero of the function `f` on\n",
      "        the sign changing interval [a , b]. Generally considered the best of the\n",
      "        rootfinding routines here. It is a safe version of the secant method that\n",
      "        uses inverse quadratic extrapolation. Brent's method combines root\n",
      "        bracketing, interval bisection, and inverse quadratic interpolation. It is\n",
      "        sometimes known as the van Wijngaarden-Dekker-Brent method. Brent (1973)\n",
      "        claims convergence is guaranteed for functions computable within [a,b].\n",
      "        \n",
      "        [Brent1973]_ provides the classic description of the algorithm. Another\n",
      "        description can be found in a recent edition of Numerical Recipes, including\n",
      "        [PressEtal1992]_. A third description is at\n",
      "        http://mathworld.wolfram.com/BrentsMethod.html. It should be easy to\n",
      "        understand the algorithm just by reading our code. Our code diverges a bit\n",
      "        from standard presentations: we choose a different formula for the\n",
      "        extrapolation step.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)` must\n",
      "            have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval :math:`[a, b]`.\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval :math:`[a, b]`.\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
      "        \n",
      "        Related functions fall into several classes:\n",
      "        \n",
      "        multivariate local optimizers\n",
      "          `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
      "        nonlinear least squares minimizer\n",
      "          `leastsq`\n",
      "        constrained multivariate optimizers\n",
      "          `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
      "        global optimizers\n",
      "          `basinhopping`, `brute`, `differential_evolution`\n",
      "        local scalar minimizers\n",
      "          `fminbound`, `brent`, `golden`, `bracket`\n",
      "        N-D root-finding\n",
      "          `fsolve`\n",
      "        1-D root-finding\n",
      "          `brenth`, `ridder`, `bisect`, `newton`\n",
      "        scalar fixed-point finder\n",
      "          `fixed_point`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Brent1973]\n",
      "           Brent, R. P.,\n",
      "           *Algorithms for Minimization Without Derivatives*.\n",
      "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
      "        \n",
      "        .. [PressEtal1992]\n",
      "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
      "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
      "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
      "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.brentq(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        >>> root = optimize.brentq(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "    \n",
      "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \\\"Broyden's good method\\\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='broyden1'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
      "        \n",
      "        which corresponds to Broyden's first Jacobian update\n",
      "        \n",
      "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \\\"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\\\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "    \n",
      "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \"Broyden's bad method\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='broyden2'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
      "        \n",
      "        corresponding to Broyden's second method.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "    \n",
      "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin at 0x12d5d9af0>, disp=False, workers=1)\n",
      "        Minimize a function over a given range by brute force.\n",
      "        \n",
      "        Uses the \"brute force\" method, i.e., computes the function's value\n",
      "        at each point of a multidimensional grid of points, to find the global\n",
      "        minimum of the function.\n",
      "        \n",
      "        The function is evaluated everywhere in the range with the datatype of the\n",
      "        first call to the function, as enforced by the ``vectorize`` NumPy\n",
      "        function. The value and type of the function evaluation returned when\n",
      "        ``full_output=True`` are affected in addition by the ``finish`` argument\n",
      "        (see Notes).\n",
      "        \n",
      "        The brute force approach is inefficient because the number of grid points\n",
      "        increases exponentially - the number of grid points to evaluate is\n",
      "        ``Ns ** len(x)``. Consequently, even with coarse grid spacing, even\n",
      "        moderately sized problems can take a long time to run, and/or run into\n",
      "        memory limitations.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the\n",
      "            form ``f(x, *args)``, where ``x`` is the argument in\n",
      "            the form of a 1-D array and ``args`` is a tuple of any\n",
      "            additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        ranges : tuple\n",
      "            Each component of the `ranges` tuple must be either a\n",
      "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
      "            The program uses these to create the grid of points on which\n",
      "            the objective function will be computed. See `Note 2` for\n",
      "            more detail.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        Ns : int, optional\n",
      "            Number of grid points along the axes, if not otherwise\n",
      "            specified. See `Note2`.\n",
      "        full_output : bool, optional\n",
      "            If True, return the evaluation grid and the objective function's\n",
      "            values on it.\n",
      "        finish : callable, optional\n",
      "            An optimization function that is called with the result of brute force\n",
      "            minimization as initial guess. `finish` should take `func` and\n",
      "            the initial guess as positional arguments, and take `args` as\n",
      "            keyword arguments. It may additionally take `full_output`\n",
      "            and/or `disp` as keyword arguments. Use None if no \"polishing\"\n",
      "            function is to be used. See Notes for more details.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages from the `finish` callable.\n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the grid is subdivided into `workers`\n",
      "            sections and evaluated in parallel (uses\n",
      "            `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply `-1` to use all cores available to the Process.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the grid in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            Requires that `func` be pickleable.\n",
      "        \n",
      "            .. versionadded:: 1.3.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : ndarray\n",
      "            A 1-D array containing the coordinates of a point at which the\n",
      "            objective function had its minimum value. (See `Note 1` for\n",
      "            which point is returned.)\n",
      "        fval : float\n",
      "            Function value at the point `x0`. (Returned when `full_output` is\n",
      "            True.)\n",
      "        grid : tuple\n",
      "            Representation of the evaluation grid. It has the same\n",
      "            length as `x0`. (Returned when `full_output` is True.)\n",
      "        Jout : ndarray\n",
      "            Function values at each point of the evaluation\n",
      "            grid, i.e., ``Jout = func(*grid)``. (Returned\n",
      "            when `full_output` is True.)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        basinhopping, differential_evolution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
      "        of the objective function occurs. If `finish` is None, that is the\n",
      "        point returned. When the global minimum occurs within (or not very far\n",
      "        outside) the grid's boundaries, and the grid is fine enough, that\n",
      "        point will be in the neighborhood of the global minimum.\n",
      "        \n",
      "        However, users often employ some other optimization program to\n",
      "        \"polish\" the gridpoint values, i.e., to seek a more precise\n",
      "        (local) minimum near `brute's` best gridpoint.\n",
      "        The `brute` function's `finish` option provides a convenient way to do\n",
      "        that. Any polishing program used must take `brute's` output as its\n",
      "        initial guess as a positional argument, and take `brute's` input values\n",
      "        for `args` as keyword arguments, otherwise an error will be raised.\n",
      "        It may additionally take `full_output` and/or `disp` as keyword arguments.\n",
      "        \n",
      "        `brute` assumes that the `finish` function returns either an\n",
      "        `OptimizeResult` object or a tuple in the form:\n",
      "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing\n",
      "        value of the argument, ``Jmin`` is the minimum value of the objective\n",
      "        function, \"...\" may be some other returned values (which are not used\n",
      "        by `brute`), and ``statuscode`` is the status code of the `finish` program.\n",
      "        \n",
      "        Note that when `finish` is not None, the values returned are those\n",
      "        of the `finish` program, *not* the gridpoint ones. Consequently,\n",
      "        while `brute` confines its search to the input grid points,\n",
      "        the `finish` program's results usually will not coincide with any\n",
      "        gridpoint, and may fall outside the grid's boundary. Thus, if a\n",
      "        minimum only needs to be found over the provided grid points, make\n",
      "        sure to pass in `finish=None`.\n",
      "        \n",
      "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
      "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
      "        Each component of the `ranges` tuple can be either a slice object or a\n",
      "        two-tuple giving a range of values, such as (0, 5). If the component is a\n",
      "        slice object, `brute` uses it directly. If the component is a two-tuple\n",
      "        range, `brute` internally converts it to a slice object that interpolates\n",
      "        `Ns` points from its low-value to its high-value, inclusive.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the use of `brute` to seek the global minimum of a function\n",
      "        of two variables that is given as the sum of a positive-definite\n",
      "        quadratic and two deep \"Gaussian-shaped\" craters. Specifically, define\n",
      "        the objective function `f` as the sum of three other functions,\n",
      "        ``f = f1 + f2 + f3``. We suppose each of these has a signature\n",
      "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
      "        are as defined below.\n",
      "        \n",
      "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
      "        >>> def f1(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
      "        \n",
      "        >>> def f2(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
      "        \n",
      "        >>> def f3(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
      "        \n",
      "        >>> def f(z, *params):\n",
      "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
      "        \n",
      "        Thus, the objective function may have local minima near the minimum\n",
      "        of each of the three functions of which it is composed. To\n",
      "        use `fmin` to polish its gridpoint result, we may then continue as\n",
      "        follows:\n",
      "        \n",
      "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
      "        >>> from scipy import optimize\n",
      "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
      "        ...                           finish=optimize.fmin)\n",
      "        >>> resbrute[0]  # global minimum\n",
      "        array([-1.05665192,  1.80834843])\n",
      "        >>> resbrute[1]  # function value at global minimum\n",
      "        -3.4085818767\n",
      "        \n",
      "        Note that if `finish` had been set to None, we would have gotten the\n",
      "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
      "    \n",
      "    check_grad(func, grad, x0, *args, **kwargs)\n",
      "        Check the correctness of a gradient function by comparing it against a\n",
      "        (forward) finite-difference approximation of the gradient.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x0, *args)``\n",
      "            Function whose derivative is to be checked.\n",
      "        grad : callable ``grad(x0, *args)``\n",
      "            Gradient of `func`.\n",
      "        x0 : ndarray\n",
      "            Points to check `grad` against forward difference approximation of grad\n",
      "            using `func`.\n",
      "        args : \\*args, optional\n",
      "            Extra arguments passed to `func` and `grad`.\n",
      "        epsilon : float, optional\n",
      "            Step size used for the finite difference approximation. It defaults to\n",
      "            ``sqrt(np.finfo(float).eps)``, which is approximately 1.49e-08.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        err : float\n",
      "            The square root of the sum of squares (i.e., the 2-norm) of the\n",
      "            difference between ``grad(x0, *args)`` and the finite difference\n",
      "            approximation of `grad` using func at the points `x0`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        approx_fprime\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def func(x):\n",
      "        ...     return x[0]**2 - 0.5 * x[1]**3\n",
      "        >>> def grad(x):\n",
      "        ...     return [2 * x[0], -1.5 * x[1]**2]\n",
      "        >>> from scipy.optimize import check_grad\n",
      "        >>> check_grad(func, grad, [1.5, -1.5])\n",
      "        2.9802322387695312e-08\n",
      "    \n",
      "    curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=True, bounds=(-inf, inf), method=None, jac=None, **kwargs)\n",
      "        Use non-linear least squares to fit a function, f, to data.\n",
      "        \n",
      "        Assumes ``ydata = f(xdata, *params) + eps``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            The model function, f(x, ...). It must take the independent\n",
      "            variable as the first argument and the parameters to fit as\n",
      "            separate remaining arguments.\n",
      "        xdata : array_like or object\n",
      "            The independent variable where the data is measured.\n",
      "            Should usually be an M-length sequence or an (k,M)-shaped array for\n",
      "            functions with k predictors, but can actually be any object.\n",
      "        ydata : array_like\n",
      "            The dependent data, a length M array - nominally ``f(xdata, ...)``.\n",
      "        p0 : array_like, optional\n",
      "            Initial guess for the parameters (length N). If None, then the\n",
      "            initial values will all be 1 (if the number of parameters for the\n",
      "            function can be determined using introspection, otherwise a\n",
      "            ValueError is raised).\n",
      "        sigma : None or M-length sequence or MxM array, optional\n",
      "            Determines the uncertainty in `ydata`. If we define residuals as\n",
      "            ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n",
      "            depends on its number of dimensions:\n",
      "        \n",
      "                - A 1-D `sigma` should contain values of standard deviations of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = sum((r / sigma) ** 2)``.\n",
      "        \n",
      "                - A 2-D `sigma` should contain the covariance matrix of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = r.T @ inv(sigma) @ r``.\n",
      "        \n",
      "                  .. versionadded:: 0.19\n",
      "        \n",
      "            None (default) is equivalent of 1-D `sigma` filled with ones.\n",
      "        absolute_sigma : bool, optional\n",
      "            If True, `sigma` is used in an absolute sense and the estimated parameter\n",
      "            covariance `pcov` reflects these absolute values.\n",
      "        \n",
      "            If False (default), only the relative magnitudes of the `sigma` values matter.\n",
      "            The returned parameter covariance matrix `pcov` is based on scaling\n",
      "            `sigma` by a constant factor. This constant is set by demanding that the\n",
      "            reduced `chisq` for the optimal parameters `popt` when using the\n",
      "            *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n",
      "            match the sample variance of the residuals after the fit. Default is False.\n",
      "            Mathematically,\n",
      "            ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\n",
      "        check_finite : bool, optional\n",
      "            If True, check that the input arrays do not contain nans of infs,\n",
      "            and raise a ValueError if they do. Setting this parameter to\n",
      "            False may silently produce nonsensical results if the input arrays\n",
      "            do contain nans. Default is True.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on parameters. Defaults to no bounds.\n",
      "            Each element of the tuple must be either an array with the length equal\n",
      "            to the number of parameters, or a scalar (in which case the bound is\n",
      "            taken to be the same for all parameters). Use ``np.inf`` with an\n",
      "            appropriate sign to disable bounds on all or some parameters.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        method : {'lm', 'trf', 'dogbox'}, optional\n",
      "            Method to use for optimization. See `least_squares` for more details.\n",
      "            Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n",
      "            provided. The method 'lm' won't work when the number of observations\n",
      "            is less than the number of variables, use 'trf' or 'dogbox' in this\n",
      "            case.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        jac : callable, string or None, optional\n",
      "            Function with signature ``jac(x, ...)`` which computes the Jacobian\n",
      "            matrix of the model function with respect to parameters as a dense\n",
      "            array_like structure. It will be scaled according to provided `sigma`.\n",
      "            If None (default), the Jacobian will be estimated numerically.\n",
      "            String keywords for 'trf' and 'dogbox' methods can be used to select\n",
      "            a finite difference scheme, see `least_squares`.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        kwargs\n",
      "            Keyword arguments passed to `leastsq` for ``method='lm'`` or\n",
      "            `least_squares` otherwise.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        popt : array\n",
      "            Optimal values for the parameters so that the sum of the squared\n",
      "            residuals of ``f(xdata, *popt) - ydata`` is minimized.\n",
      "        pcov : 2-D array\n",
      "            The estimated covariance of popt. The diagonals provide the variance\n",
      "            of the parameter estimate. To compute one standard deviation errors\n",
      "            on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\n",
      "        \n",
      "            How the `sigma` parameter affects the estimated covariance\n",
      "            depends on `absolute_sigma` argument, as described above.\n",
      "        \n",
      "            If the Jacobian matrix at the solution doesn't have a full rank, then\n",
      "            'lm' method returns a matrix filled with ``np.inf``, on the other hand\n",
      "            'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n",
      "            the covariance matrix.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            if either `ydata` or `xdata` contain NaNs, or if incompatible options\n",
      "            are used.\n",
      "        \n",
      "        RuntimeError\n",
      "            if the least-squares minimization fails.\n",
      "        \n",
      "        OptimizeWarning\n",
      "            if covariance of the parameters can not be estimated.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Minimize the sum of squares of nonlinear functions.\n",
      "        scipy.stats.linregress : Calculate a linear least squares regression for\n",
      "                                 two sets of measurements.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\n",
      "        through `leastsq`. Note that this algorithm can only deal with\n",
      "        unconstrained problems.\n",
      "        \n",
      "        Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\n",
      "        the docstring of `least_squares` for more information.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import curve_fit\n",
      "        \n",
      "        >>> def func(x, a, b, c):\n",
      "        ...     return a * np.exp(-b * x) + c\n",
      "        \n",
      "        Define the data to be fit with some noise:\n",
      "        \n",
      "        >>> xdata = np.linspace(0, 4, 50)\n",
      "        >>> y = func(xdata, 2.5, 1.3, 0.5)\n",
      "        >>> np.random.seed(1729)\n",
      "        >>> y_noise = 0.2 * np.random.normal(size=xdata.size)\n",
      "        >>> ydata = y + y_noise\n",
      "        >>> plt.plot(xdata, ydata, 'b-', label='data')\n",
      "        \n",
      "        Fit for the parameters a, b, c of the function `func`:\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata)\n",
      "        >>> popt\n",
      "        array([ 2.55423706,  1.35190947,  0.47450618])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'r-',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "        \n",
      "        Constrain the optimization to the region of ``0 <= a <= 3``,\n",
      "        ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n",
      "        >>> popt\n",
      "        array([ 2.43708906,  1.        ,  0.35015434])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'g--',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "        \n",
      "        >>> plt.xlabel('x')\n",
      "        >>> plt.ylabel('y')\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "    \n",
      "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
      "        \n",
      "        The Jacobian approximation is derived from previous iterations, by\n",
      "        retaining only the diagonal of Broyden matrices.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='diagbroyden'`` in particular.\n",
      "    \n",
      "    differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0, updating='immediate', workers=1, constraints=())\n",
      "        Finds the global minimum of a multivariate function.\n",
      "        \n",
      "        Differential Evolution is stochastic in nature (does not use gradient\n",
      "        methods) to find the minimum, and can search large areas of candidate\n",
      "        space, but often requires larger numbers of function evaluations than\n",
      "        conventional gradient-based techniques.\n",
      "        \n",
      "        The algorithm is due to Storn and Price [1]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence or `Bounds`, optional\n",
      "            Bounds for variables. There are two ways to specify the bounds:\n",
      "            1. Instance of `Bounds` class.\n",
      "            2. ``(min, max)`` pairs for each element in ``x``, defining the finite\n",
      "            lower and upper bounds for the optimizing argument of `func`. It is\n",
      "            required to have ``len(bounds) == len(x)``. ``len(bounds)`` is used\n",
      "            to determine the number of parameters in ``x``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to\n",
      "            completely specify the objective function.\n",
      "        strategy : str, optional\n",
      "            The differential evolution strategy to use. Should be one of:\n",
      "        \n",
      "                - 'best1bin'\n",
      "                - 'best1exp'\n",
      "                - 'rand1exp'\n",
      "                - 'randtobest1exp'\n",
      "                - 'currenttobest1exp'\n",
      "                - 'best2exp'\n",
      "                - 'rand2exp'\n",
      "                - 'randtobest1bin'\n",
      "                - 'currenttobest1bin'\n",
      "                - 'best2bin'\n",
      "                - 'rand2bin'\n",
      "                - 'rand1bin'\n",
      "        \n",
      "            The default is 'best1bin'.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of generations over which the entire population is\n",
      "            evolved. The maximum number of function evaluations (with no polishing)\n",
      "            is: ``(maxiter + 1) * popsize * len(x)``\n",
      "        popsize : int, optional\n",
      "            A multiplier for setting the total population size. The population has\n",
      "            ``popsize * len(x)`` individuals (unless the initial population is\n",
      "            supplied via the `init` keyword).\n",
      "        tol : float, optional\n",
      "            Relative tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        mutation : float or tuple(float, float), optional\n",
      "            The mutation constant. In the literature this is also known as\n",
      "            differential weight, being denoted by F.\n",
      "            If specified as a float it should be in the range [0, 2].\n",
      "            If specified as a tuple ``(min, max)`` dithering is employed. Dithering\n",
      "            randomly changes the mutation constant on a generation by generation\n",
      "            basis. The mutation constant for that generation is taken from\n",
      "            ``U[min, max)``. Dithering can help speed convergence significantly.\n",
      "            Increasing the mutation constant increases the search radius, but will\n",
      "            slow down convergence.\n",
      "        recombination : float, optional\n",
      "            The recombination constant, should be in the range [0, 1]. In the\n",
      "            literature this is also known as the crossover probability, being\n",
      "            denoted by CR. Increasing this value allows a larger number of mutants\n",
      "            to progress into the next generation, but at the risk of population\n",
      "            stability.\n",
      "        seed : {int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "            If `seed` is not specified the `~np.random.RandomState` singleton is\n",
      "            used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with seed.\n",
      "            If `seed` is already a ``RandomState`` or a ``Generator`` instance,\n",
      "            then that object is used.\n",
      "            Specify `seed` for repeatable minimizations.\n",
      "        disp : bool, optional\n",
      "            Prints the evaluated `func` at every iteration.\n",
      "        callback : callable, `callback(xk, convergence=val)`, optional\n",
      "            A function to follow the progress of the minimization. ``xk`` is\n",
      "            the current value of ``x0``. ``val`` represents the fractional\n",
      "            value of the population convergence.  When ``val`` is greater than one\n",
      "            the function halts. If callback returns `True`, then the minimization\n",
      "            is halted (any polishing is still carried out).\n",
      "        polish : bool, optional\n",
      "            If True (default), then `scipy.optimize.minimize` with the `L-BFGS-B`\n",
      "            method is used to polish the best population member at the end, which\n",
      "            can improve the minimization slightly. If a constrained problem is\n",
      "            being studied then the `trust-constr` method is used instead.\n",
      "        init : str or array-like, optional\n",
      "            Specify which type of population initialization is performed. Should be\n",
      "            one of:\n",
      "        \n",
      "                - 'latinhypercube'\n",
      "                - 'random'\n",
      "                - array specifying the initial population. The array should have\n",
      "                  shape ``(M, len(x))``, where M is the total population size and\n",
      "                  len(x) is the number of parameters.\n",
      "                  `init` is clipped to `bounds` before use.\n",
      "        \n",
      "            The default is 'latinhypercube'. Latin Hypercube sampling tries to\n",
      "            maximize coverage of the available parameter space. 'random'\n",
      "            initializes the population randomly - this has the drawback that\n",
      "            clustering can occur, preventing the whole of parameter space being\n",
      "            covered. Use of an array to specify a population subset could be used,\n",
      "            for example, to create a tight bunch of initial guesses in an location\n",
      "            where the solution is known to exist, thereby reducing time for\n",
      "            convergence.\n",
      "        atol : float, optional\n",
      "            Absolute tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        updating : {'immediate', 'deferred'}, optional\n",
      "            If ``'immediate'``, the best solution vector is continuously updated\n",
      "            within a single generation [4]_. This can lead to faster convergence as\n",
      "            trial vectors can take advantage of continuous improvements in the best\n",
      "            solution.\n",
      "            With ``'deferred'``, the best solution vector is updated once per\n",
      "            generation. Only ``'deferred'`` is compatible with parallelization, and\n",
      "            the `workers` keyword can over-ride this option.\n",
      "        \n",
      "            .. versionadded:: 1.2.0\n",
      "        \n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the population is subdivided into `workers`\n",
      "            sections and evaluated in parallel\n",
      "            (uses `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply -1 to use all available CPU cores.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the population in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            This option will override the `updating` keyword to\n",
      "            ``updating='deferred'`` if ``workers != 1``.\n",
      "            Requires that `func` be pickleable.\n",
      "        \n",
      "            .. versionadded:: 1.2.0\n",
      "        \n",
      "        constraints : {NonLinearConstraint, LinearConstraint, Bounds}\n",
      "            Constraints on the solver, over and above those applied by the `bounds`\n",
      "            kwd. Uses the approach by Lampinen [5]_.\n",
      "        \n",
      "            .. versionadded:: 1.4.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes. If `polish`\n",
      "            was employed, and a lower minimum was obtained by the polishing, then\n",
      "            OptimizeResult also contains the ``jac`` attribute.\n",
      "            If the eventual solution does not satisfy the applied constraints\n",
      "            ``success`` will be `False`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Differential evolution is a stochastic population based method that is\n",
      "        useful for global optimization problems. At each pass through the population\n",
      "        the algorithm mutates each candidate solution by mixing with other candidate\n",
      "        solutions to create a trial candidate. There are several strategies [2]_ for\n",
      "        creating trial candidates, which suit some problems more than others. The\n",
      "        'best1bin' strategy is a good starting point for many systems. In this\n",
      "        strategy two members of the population are randomly chosen. Their difference\n",
      "        is used to mutate the best member (the 'best' in 'best1bin'), :math:`b_0`,\n",
      "        so far:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            b' = b_0 + mutation * (population[rand0] - population[rand1])\n",
      "        \n",
      "        A trial vector is then constructed. Starting with a randomly chosen ith\n",
      "        parameter the trial is sequentially filled (in modulo) with parameters from\n",
      "        ``b'`` or the original candidate. The choice of whether to use ``b'`` or the\n",
      "        original candidate is made with a binomial distribution (the 'bin' in\n",
      "        'best1bin') - a random number in [0, 1) is generated. If this number is\n",
      "        less than the `recombination` constant then the parameter is loaded from\n",
      "        ``b'``, otherwise it is loaded from the original candidate. The final\n",
      "        parameter is always loaded from ``b'``. Once the trial candidate is built\n",
      "        its fitness is assessed. If the trial is better than the original candidate\n",
      "        then it takes its place. If it is also better than the best overall\n",
      "        candidate it also replaces that.\n",
      "        To improve your chances of finding a global minimum use higher `popsize`\n",
      "        values, with higher `mutation` and (dithering), but lower `recombination`\n",
      "        values. This has the effect of widening the search radius, but slowing\n",
      "        convergence.\n",
      "        By default the best solution vector is updated continuously within a single\n",
      "        iteration (``updating='immediate'``). This is a modification [4]_ of the\n",
      "        original differential evolution algorithm which can lead to faster\n",
      "        convergence as trial vectors can immediately benefit from improved\n",
      "        solutions. To use the original Storn and Price behaviour, updating the best\n",
      "        solution once per iteration, set ``updating='deferred'``.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function is implemented in `rosen` in `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import rosen, differential_evolution\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "        \n",
      "        Now repeat, but with parallelization.\n",
      "        \n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds, updating='deferred',\n",
      "        ...                                 workers=2)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "        \n",
      "        Let's try and do a constrained minimization\n",
      "        \n",
      "        >>> from scipy.optimize import NonlinearConstraint, Bounds\n",
      "        >>> def constr_f(x):\n",
      "        ...     return np.array(x[0] + x[1])\n",
      "        >>>\n",
      "        >>> # the sum of x[0] and x[1] must be less than 1.9\n",
      "        >>> nlc = NonlinearConstraint(constr_f, -np.inf, 1.9)\n",
      "        >>> # specify limits using a `Bounds` object.\n",
      "        >>> bounds = Bounds([0., 0.], [2., 2.])\n",
      "        >>> result = differential_evolution(rosen, bounds, constraints=(nlc),\n",
      "        ...                                 seed=1)\n",
      "        >>> result.x, result.fun\n",
      "        (array([0.96633867, 0.93363577]), 0.0011361355854792312)\n",
      "        \n",
      "        Next find the minimum of the Ackley function\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization).\n",
      "        \n",
      "        >>> from scipy.optimize import differential_evolution\n",
      "        >>> import numpy as np\n",
      "        >>> def ackley(x):\n",
      "        ...     arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n",
      "        ...     arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n",
      "        ...     return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
      "        >>> bounds = [(-5, 5), (-5, 5)]\n",
      "        >>> result = differential_evolution(ackley, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 0.,  0.]), 4.4408920985006262e-16)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Storn, R and Price, K, Differential Evolution - a Simple and\n",
      "               Efficient Heuristic for Global Optimization over Continuous Spaces,\n",
      "               Journal of Global Optimization, 1997, 11, 341 - 359.\n",
      "        .. [2] http://www1.icsi.berkeley.edu/~storn/code.html\n",
      "        .. [3] http://en.wikipedia.org/wiki/Differential_evolution\n",
      "        .. [4] Wormington, M., Panaccione, C., Matney, K. M., Bowen, D. K., -\n",
      "               Characterization of structures from X-ray scattering data using\n",
      "               genetic algorithms, Phil. Trans. R. Soc. Lond. A, 1999, 357,\n",
      "               2827-2848\n",
      "        .. [5] Lampinen, J., A constraint handling approach for the differential\n",
      "               evolution algorithm. Proceedings of the 2002 Congress on\n",
      "               Evolutionary Computation. CEC'02 (Cat. No. 02TH8600). Vol. 2. IEEE,\n",
      "               2002.\n",
      "    \n",
      "    dual_annealing(func, bounds, args=(), maxiter=1000, local_search_options={}, initial_temp=5230.0, restart_temp_ratio=2e-05, visit=2.62, accept=-5.0, maxfun=10000000.0, seed=None, no_local_search=False, callback=None, x0=None)\n",
      "        Find the global minimum of a function using Dual Annealing.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence, shape (n, 2)\n",
      "            Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
      "            defining bounds for the objective function parameter.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of global search iterations. Default value is 1000.\n",
      "        local_search_options : dict, optional\n",
      "            Extra keyword arguments to be passed to the local minimizer\n",
      "            (`minimize`). Some important options could be:\n",
      "            ``method`` for the minimizer method to use and ``args`` for\n",
      "            objective function additional arguments.\n",
      "        initial_temp : float, optional\n",
      "            The initial temperature, use higher values to facilitates a wider\n",
      "            search of the energy landscape, allowing dual_annealing to escape\n",
      "            local minima that it is trapped in. Default value is 5230. Range is\n",
      "            (0.01, 5.e4].\n",
      "        restart_temp_ratio : float, optional\n",
      "            During the annealing process, temperature is decreasing, when it\n",
      "            reaches ``initial_temp * restart_temp_ratio``, the reannealing process\n",
      "            is triggered. Default value of the ratio is 2e-5. Range is (0, 1).\n",
      "        visit : float, optional\n",
      "            Parameter for visiting distribution. Default value is 2.62. Higher\n",
      "            values give the visiting distribution a heavier tail, this makes\n",
      "            the algorithm jump to a more distant region. The value range is (0, 3].\n",
      "        accept : float, optional\n",
      "            Parameter for acceptance distribution. It is used to control the\n",
      "            probability of acceptance. The lower the acceptance parameter, the\n",
      "            smaller the probability of acceptance. Default value is -5.0 with\n",
      "            a range (-1e4, -5].\n",
      "        maxfun : int, optional\n",
      "            Soft limit for the number of objective function calls. If the\n",
      "            algorithm is in the middle of a local search, this number will be\n",
      "            exceeded, the algorithm will stop just after the local search is\n",
      "            done. Default value is 1e7.\n",
      "        seed : {int, `~numpy.random.RandomState`, `~numpy.random.Generator`}, optional\n",
      "            If `seed` is not specified the `~numpy.random.RandomState` singleton is\n",
      "            used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
      "            with `seed`.\n",
      "            If `seed` is already a ``RandomState`` or ``Generator`` instance, then\n",
      "            that instance is used.\n",
      "            Specify `seed` for repeatable minimizations. The random numbers\n",
      "            generated with this seed only affect the visiting distribution function\n",
      "            and new coordinates generation.\n",
      "        no_local_search : bool, optional\n",
      "            If `no_local_search` is set to True, a traditional Generalized\n",
      "            Simulated Annealing will be performed with no local search\n",
      "            strategy applied.\n",
      "        callback : callable, optional\n",
      "            A callback function with signature ``callback(x, f, context)``,\n",
      "            which will be called for all minima found.\n",
      "            ``x`` and ``f`` are the coordinates and function value of the\n",
      "            latest minimum found, and ``context`` has value in [0, 1, 2], with the\n",
      "            following meaning:\n",
      "        \n",
      "                - 0: minimum detected in the annealing process.\n",
      "                - 1: detection occurred in the local search process.\n",
      "                - 2: detection done in the dual annealing process.\n",
      "        \n",
      "            If the callback implementation returns True, the algorithm will stop.\n",
      "        x0 : ndarray, shape(n,), optional\n",
      "            Coordinates of a single N-D starting point.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination.\n",
      "            See `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements the Dual Annealing optimization. This stochastic\n",
      "        approach derived from [3]_ combines the generalization of CSA (Classical\n",
      "        Simulated Annealing) and FSA (Fast Simulated Annealing) [1]_ [2]_ coupled\n",
      "        to a strategy for applying a local search on accepted locations [4]_.\n",
      "        An alternative implementation of this same algorithm is described in [5]_\n",
      "        and benchmarks are presented in [6]_. This approach introduces an advanced\n",
      "        method to refine the solution found by the generalized annealing\n",
      "        process. This algorithm uses a distorted Cauchy-Lorentz visiting\n",
      "        distribution, with its shape controlled by the parameter :math:`q_{v}`\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            g_{q_{v}}(\\Delta x(t)) \\propto \\frac{ \\\n",
      "            \\left[T_{q_{v}}(t) \\right]^{-\\frac{D}{3-q_{v}}}}{ \\\n",
      "            \\left[{1+(q_{v}-1)\\frac{(\\Delta x(t))^{2}} { \\\n",
      "            \\left[T_{q_{v}}(t)\\right]^{\\frac{2}{3-q_{v}}}}}\\right]^{ \\\n",
      "            \\frac{1}{q_{v}-1}+\\frac{D-1}{2}}}\n",
      "        \n",
      "        Where :math:`t` is the artificial time. This visiting distribution is used\n",
      "        to generate a trial jump distance :math:`\\Delta x(t)` of variable\n",
      "        :math:`x(t)` under artificial temperature :math:`T_{q_{v}}(t)`.\n",
      "        \n",
      "        From the starting point, after calling the visiting distribution\n",
      "        function, the acceptance probability is computed as follows:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            p_{q_{a}} = \\min{\\{1,\\left[1-(1-q_{a}) \\beta \\Delta E \\right]^{ \\\n",
      "            \\frac{1}{1-q_{a}}}\\}}\n",
      "        \n",
      "        Where :math:`q_{a}` is a acceptance parameter. For :math:`q_{a}<1`, zero\n",
      "        acceptance probability is assigned to the cases where\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            [1-(1-q_{a}) \\beta \\Delta E] < 0\n",
      "        \n",
      "        The artificial temperature :math:`T_{q_{v}}(t)` is decreased according to\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            T_{q_{v}}(t) = T_{q_{v}}(1) \\frac{2^{q_{v}-1}-1}{\\left( \\\n",
      "            1 + t\\right)^{q_{v}-1}-1}\n",
      "        \n",
      "        Where :math:`q_{v}` is the visiting parameter.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsallis C. Possible generalization of Boltzmann-Gibbs\n",
      "            statistics. Journal of Statistical Physics, 52, 479-487 (1998).\n",
      "        .. [2] Tsallis C, Stariolo DA. Generalized Simulated Annealing.\n",
      "            Physica A, 233, 395-406 (1996).\n",
      "        .. [3] Xiang Y, Sun DY, Fan W, Gong XG. Generalized Simulated\n",
      "            Annealing Algorithm and Its Application to the Thomson Model.\n",
      "            Physics Letters A, 233, 216-220 (1997).\n",
      "        .. [4] Xiang Y, Gong XG. Efficiency of Generalized Simulated\n",
      "            Annealing. Physical Review E, 62, 4473 (2000).\n",
      "        .. [5] Xiang Y, Gubian S, Suomela B, Hoeng J. Generalized\n",
      "            Simulated Annealing for Efficient Global Optimization: the GenSA\n",
      "            Package for R. The R Journal, Volume 5/1 (2013).\n",
      "        .. [6] Mullen, K. Continuous Global Optimization in R. Journal of\n",
      "            Statistical Software, 60(6), 1 - 45, (2014). DOI:10.18637/jss.v060.i06\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 10-D problem, with many local minima.\n",
      "        The function involved is called Rastrigin\n",
      "        (https://en.wikipedia.org/wiki/Rastrigin_function)\n",
      "        \n",
      "        >>> from scipy.optimize import dual_annealing\n",
      "        >>> func = lambda x: np.sum(x*x - 10*np.cos(2*np.pi*x)) + 10*np.size(x)\n",
      "        >>> lw = [-5.12] * 10\n",
      "        >>> up = [5.12] * 10\n",
      "        >>> ret = dual_annealing(func, bounds=list(zip(lw, up)), seed=1234)\n",
      "        >>> ret.x\n",
      "        array([-4.26437714e-09, -3.91699361e-09, -1.86149218e-09, -3.97165720e-09,\n",
      "               -6.29151648e-09, -6.53145322e-09, -3.93616815e-09, -6.55623025e-09,\n",
      "               -6.05775280e-09, -5.00668935e-09]) # may vary\n",
      "        >>> ret.fun\n",
      "        0.000000\n",
      "    \n",
      "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
      "        \n",
      "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='excitingmixing'`` in particular.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial Jacobian approximation is (-1/alpha).\n",
      "        alphamax : float, optional\n",
      "            The entries of the diagonal Jacobian are kept in the range\n",
      "            ``[alpha, alphamax]``.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "    \n",
      "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500, method='del2')\n",
      "        Find a fixed point of the function.\n",
      "        \n",
      "        Given a function of one or more variables and a starting point, find a\n",
      "        fixed point of the function: i.e., where ``func(x0) == x0``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            Function to evaluate.\n",
      "        x0 : array_like\n",
      "            Fixed point of function.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to `func`.\n",
      "        xtol : float, optional\n",
      "            Convergence tolerance, defaults to 1e-08.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations, defaults to 500.\n",
      "        method : {\"del2\", \"iteration\"}, optional\n",
      "            Method of finding the fixed-point, defaults to \"del2\",\n",
      "            which uses Steffensen's Method with Aitken's ``Del^2``\n",
      "            convergence acceleration [1]_. The \"iteration\" method simply iterates\n",
      "            the function until convergence is detected, without attempting to\n",
      "            accelerate the convergence.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c1, c2):\n",
      "        ...    return np.sqrt(c1/(x+c2))\n",
      "        >>> c1 = np.array([10,12.])\n",
      "        >>> c2 = np.array([3, 5.])\n",
      "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
      "        array([ 1.4920333 ,  1.37228132])\n",
      "    \n",
      "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, initial_simplex=None)\n",
      "        Minimize a function using the downhill simplex algorithm.\n",
      "        \n",
      "        This algorithm only uses function values, not derivatives or second\n",
      "        derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            The objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func, i.e., ``f(x,*args)``.\n",
      "        xtol : float, optional\n",
      "            Absolute error in xopt between iterations that is acceptable for\n",
      "            convergence.\n",
      "        ftol : number, optional\n",
      "            Absolute error in func(xopt) between iterations that is acceptable for\n",
      "            convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : number, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            Set to True if fopt and warnflag outputs are desired.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages.\n",
      "        retall : bool, optional\n",
      "            Set to True to return list of solutions at each iteration.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        initial_simplex : array_like of shape (N + 1, N), optional\n",
      "            Initial simplex. If given, overrides `x0`.\n",
      "            ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "            the jth vertex of the ``N+1`` vertices in the simplex, where\n",
      "            ``N`` is the dimension.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter that minimizes function.\n",
      "        fopt : float\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        iter : int\n",
      "            Number of iterations performed.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            1 : Maximum number of function evaluations made.\n",
      "            2 : Maximum number of iterations reached.\n",
      "        allvecs : list\n",
      "            Solution at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Nelder-Mead' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
      "        one or more variables.\n",
      "        \n",
      "        This algorithm has a long history of successful use in applications.\n",
      "        But it will usually be slower than an algorithm that uses first or\n",
      "        second derivative information. In practice, it can have poor\n",
      "        performance in high-dimensional problems and is not robust to\n",
      "        minimizing complicated functions. Additionally, there currently is no\n",
      "        complete theory describing when the algorithm will successfully\n",
      "        converge to the minimum, or how fast it will if it does. Both the ftol and\n",
      "        xtol criteria must be met for convergence.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin(f, 1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 17\n",
      "                 Function evaluations: 34\n",
      "        >>> minimum[0]\n",
      "        -8.8817841970012523e-16\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
      "               minimization\", The Computer Journal, 7, pp. 308-313\n",
      "        \n",
      "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
      "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
      "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
      "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
      "               Harlow, UK, pp. 191-208.\n",
      "    \n",
      "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using the BFGS algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable f'(x,*args), optional\n",
      "            Gradient of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f and fprime.\n",
      "        gtol : float, optional\n",
      "            Gradient norm must be less than gtol before successful termination.\n",
      "        norm : float, optional\n",
      "            Order of norm (Inf is max, -Inf is min)\n",
      "        epsilon : int or ndarray, optional\n",
      "            If fprime is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function to call after each\n",
      "            iteration. Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True,return fopt, func_calls, grad_calls, and warnflag\n",
      "            in addition to xopt.\n",
      "        disp : bool, optional\n",
      "            Print convergence message if True.\n",
      "        retall : bool, optional\n",
      "            Return a list of results at each iteration if True.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., f(xopt) == fopt.\n",
      "        fopt : float\n",
      "            Minimum value.\n",
      "        gopt : ndarray\n",
      "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
      "        Bopt : ndarray\n",
      "            Value of 1/f''(xopt), i.e., the inverse Hessian matrix.\n",
      "        func_calls : int\n",
      "            Number of function_calls made.\n",
      "        grad_calls : int\n",
      "            Number of gradient calls made.\n",
      "        warnflag : integer\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Gradient and/or function calls not changing.\n",
      "            3 : NaN result encountered.\n",
      "        allvecs  :  list\n",
      "            The value of xopt at each iteration. Only returned if retall is True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'BFGS' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Optimize the function, f, whose gradient is given by fprime\n",
      "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
      "        and Shanno (BFGS)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright, and Nocedal 'Numerical Optimization', 1999, p. 198.\n",
      "    \n",
      "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable, ``f(x, *args)``\n",
      "            Objective function to be minimized. Here `x` must be a 1-D array of\n",
      "            the variables that are to be changed in the search for a minimum, and\n",
      "            `args` are the other (fixed) parameters of `f`.\n",
      "        x0 : ndarray\n",
      "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
      "            It must be a 1-D array of values.\n",
      "        fprime : callable, ``fprime(x, *args)``, optional\n",
      "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
      "            are as described above for `f`. The returned value must be a 1-D array.\n",
      "            Defaults to None, in which case the gradient is approximated\n",
      "            numerically (see `epsilon`, below).\n",
      "        args : tuple, optional\n",
      "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
      "            additional fixed parameters are needed to completely specify the\n",
      "            functions `f` and `fprime`.\n",
      "        gtol : float, optional\n",
      "            Stop when the norm of the gradient is less than `gtol`.\n",
      "        norm : float, optional\n",
      "            Order to use for the norm of the gradient\n",
      "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
      "        epsilon : float or ndarray, optional\n",
      "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
      "            scalar or a 1-D array. Defaults to ``sqrt(eps)``, with eps the\n",
      "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
      "            1.5e-8.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
      "        full_output : bool, optional\n",
      "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
      "            addition to `xopt`.  See the Returns section below for additional\n",
      "            information on optional return values.\n",
      "        disp : bool, optional\n",
      "            If True, return a convergence message, followed by `xopt`.\n",
      "        retall : bool, optional\n",
      "            If True, add to the returned values the results of each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each iteration.\n",
      "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float, optional\n",
      "            Minimum value found, f(xopt). Only returned if `full_output` is True.\n",
      "        func_calls : int, optional\n",
      "            The number of function_calls made. Only returned if `full_output`\n",
      "            is True.\n",
      "        grad_calls : int, optional\n",
      "            The number of gradient calls made. Only returned if `full_output` is\n",
      "            True.\n",
      "        warnflag : int, optional\n",
      "            Integer value with warning status, only returned if `full_output` is\n",
      "            True.\n",
      "        \n",
      "            0 : Success.\n",
      "        \n",
      "            1 : The maximum number of iterations was exceeded.\n",
      "        \n",
      "            2 : Gradient and/or function calls were not changing. May indicate\n",
      "                that precision was lost, i.e., the routine did not converge.\n",
      "        \n",
      "            3 : NaN result encountered.\n",
      "        \n",
      "        allvecs : list of ndarray, optional\n",
      "            List of arrays, containing the results at each iteration.\n",
      "            Only returned if `retall` is True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize : common interface to all `scipy.optimize` algorithms for\n",
      "                   unconstrained and constrained minimization of multivariate\n",
      "                   functions. It provides an alternative way to call\n",
      "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
      "        [1]_.\n",
      "        \n",
      "        Conjugate gradient methods tend to work better when:\n",
      "        \n",
      "        1. `f` has a unique global minimizing point, and no local minima or\n",
      "           other stationary points,\n",
      "        2. `f` is, at least locally, reasonably well approximated by a\n",
      "           quadratic function of the variables,\n",
      "        3. `f` is continuous and has a continuous gradient,\n",
      "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
      "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
      "           minimizing point, `xopt`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Example 1: seek the minimum value of the expression\n",
      "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
      "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
      "        \n",
      "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
      "        >>> def f(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
      "        >>> def gradf(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
      "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
      "        ...     return np.asarray((gu, gv))\n",
      "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
      "        >>> from scipy import optimize\n",
      "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 1.617021\n",
      "                 Iterations: 4\n",
      "                 Function evaluations: 8\n",
      "                 Gradient evaluations: 8\n",
      "        >>> res1\n",
      "        array([-1.80851064, -0.25531915])\n",
      "        \n",
      "        Example 2: solve the same problem using the `minimize` function.\n",
      "        (This `myopts` dictionary shows all of the available options,\n",
      "        although in practice only non-default values would be needed.\n",
      "        The returned value will be a dictionary.)\n",
      "        \n",
      "        >>> opts = {'maxiter' : None,    # default value.\n",
      "        ...         'disp' : True,    # non-default value.\n",
      "        ...         'gtol' : 1e-5,    # default value.\n",
      "        ...         'norm' : np.inf,  # default value.\n",
      "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
      "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
      "        ...                          method='CG', options=opts)\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 1.617021\n",
      "                Iterations: 4\n",
      "                Function evaluations: 8\n",
      "                Gradient evaluations: 8\n",
      "        >>> res2.x  # minimum found\n",
      "        array([-1.80851064, -0.25531915])\n",
      "    \n",
      "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, maxfun=1000, disp=None, catol=0.0002)\n",
      "        Minimize a function using the Constrained Optimization By Linear\n",
      "        Approximation (COBYLA) method. This method wraps a FORTRAN\n",
      "        implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Function to minimize. In the form func(x, \\*args).\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        cons : sequence\n",
      "            Constraint functions; must all be ``>=0`` (a single function\n",
      "            if only 1 constraint). Each function takes the parameters `x`\n",
      "            as its first argument, and it can return either a single number or\n",
      "            an array or list of numbers.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function.\n",
      "        consargs : tuple, optional\n",
      "            Extra arguments to pass to constraint functions (default of None means\n",
      "            use same extra arguments as those passed to func).\n",
      "            Use ``()`` for no extra arguments.\n",
      "        rhobeg : float, optional\n",
      "            Reasonable initial changes to the variables.\n",
      "        rhoend : float, optional\n",
      "            Final accuracy in the optimization (not precisely guaranteed). This\n",
      "            is a lower bound on the size of the trust region.\n",
      "        disp : {0, 1, 2, 3}, optional\n",
      "            Controls the frequency of output; 0 implies no output.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        catol : float, optional\n",
      "            Absolute tolerance for constraint violations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The argument that minimises `f`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'COBYLA' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm is based on linear approximations to the objective\n",
      "        function and each constraint. We briefly describe the algorithm.\n",
      "        \n",
      "        Suppose the function is being minimized over k variables. At the\n",
      "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
      "        an approximate solution x_j, and a radius RHO_j.\n",
      "        (i.e., linear plus a constant) approximations to the objective\n",
      "        function and constraint functions such that their function values\n",
      "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
      "        This gives a linear program to solve (where the linear approximations\n",
      "        of the constraint functions are constrained to be non-negative).\n",
      "        \n",
      "        However, the linear approximations are likely only good\n",
      "        approximations near the current simplex, so the linear program is\n",
      "        given the further requirement that the solution, which\n",
      "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
      "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
      "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
      "        like a trust region algorithm.\n",
      "        \n",
      "        Additionally, the linear program may be inconsistent, or the\n",
      "        approximation may give poor improvement. For details about\n",
      "        how these issues are resolved, as well as how the points v_i are\n",
      "        updated, refer to the source code or the references below.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
      "        the objective and constraint functions by linear interpolation.\", in\n",
      "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
      "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
      "        \n",
      "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
      "        calculations\", Acta Numerica 7, 287-336\n",
      "        \n",
      "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
      "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Minimize the objective function f(x,y) = x*y subject\n",
      "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
      "        \n",
      "            >>> def objective(x):\n",
      "            ...     return x[0]*x[1]\n",
      "            ...\n",
      "            >>> def constr1(x):\n",
      "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
      "            ...\n",
      "            >>> def constr2(x):\n",
      "            ...     return x[1]\n",
      "            ...\n",
      "            >>> from scipy.optimize import fmin_cobyla\n",
      "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
      "            array([-0.70710685,  0.70710671])\n",
      "        \n",
      "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
      "    \n",
      "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=-1, maxfun=15000, maxiter=15000, disp=None, callback=None, maxls=20)\n",
      "        Minimize a function func using the L-BFGS-B algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Function to minimize.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable fprime(x,*args), optional\n",
      "            The gradient of `func`. If None, then `func` returns the function\n",
      "            value and the gradient (``f, g = func(x, *args)``), unless\n",
      "            `approx_grad` is True in which case `func` returns only ``f``.\n",
      "        args : sequence, optional\n",
      "            Arguments to pass to `func` and `fprime`.\n",
      "        approx_grad : bool, optional\n",
      "            Whether to approximate the gradient numerically (in which case\n",
      "            `func` returns only the function value).\n",
      "        bounds : list, optional\n",
      "            ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the bounds on that parameter. Use None or +-inf for one of ``min`` or\n",
      "            ``max`` when there is no bound in that direction.\n",
      "        m : int, optional\n",
      "            The maximum number of variable metric corrections\n",
      "            used to define the limited memory matrix. (The limited memory BFGS\n",
      "            method does not store the full hessian but uses this many terms in an\n",
      "            approximation to it.)\n",
      "        factr : float, optional\n",
      "            The iteration stops when\n",
      "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
      "            where ``eps`` is the machine precision, which is automatically\n",
      "            generated by the code. Typical values for `factr` are: 1e12 for\n",
      "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
      "            high accuracy. See Notes for relationship to `ftol`, which is exposed\n",
      "            (instead of `factr`) by the `scipy.optimize.minimize` interface to\n",
      "            L-BFGS-B.\n",
      "        pgtol : float, optional\n",
      "            The iteration will stop when\n",
      "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
      "            where ``pg_i`` is the i-th component of the projected gradient.\n",
      "        epsilon : float, optional\n",
      "            Step size used when `approx_grad` is True, for numerically\n",
      "            calculating the gradient\n",
      "        iprint : int, optional\n",
      "            Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "            ``iprint = 0``    print only one line at the last iteration;\n",
      "            ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n",
      "            ``iprint = 99``   print details of every iteration except n-vectors;\n",
      "            ``iprint = 100``  print also the changes of active set and final x;\n",
      "            ``iprint > 100``  print details of every iteration including x and g.\n",
      "        disp : int, optional\n",
      "            If zero, then no output. If a positive number, then this over-rides\n",
      "            `iprint` (i.e., `iprint` gets the value of `disp`).\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        maxls : int, optional\n",
      "            Maximum number of line search steps (per iteration). Default is 20.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : array_like\n",
      "            Estimated position of the minimum.\n",
      "        f : float\n",
      "            Value of `func` at the minimum.\n",
      "        d : dict\n",
      "            Information dictionary.\n",
      "        \n",
      "            * d['warnflag'] is\n",
      "        \n",
      "              - 0 if converged,\n",
      "              - 1 if too many function evaluations or too many iterations,\n",
      "              - 2 if stopped for another reason, given in d['task']\n",
      "        \n",
      "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
      "            * d['funcalls'] is the number of function calls made.\n",
      "            * d['nit'] is the number of iterations.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'L-BFGS-B' `method` in particular. Note that the\n",
      "            `ftol` option is made available via that interface, while `factr` is\n",
      "            provided via this interface, where `factr` is the factor multiplying\n",
      "            the default machine floating-point precision to arrive at `ftol`:\n",
      "            ``ftol = factr * numpy.finfo(float).eps``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        License of L-BFGS-B (FORTRAN code):\n",
      "        \n",
      "        The version included here (in fortran code) is 3.0\n",
      "        (released April 25, 2011). It was written by Ciyou Zhu, Richard Byrd,\n",
      "        and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
      "        condition for use:\n",
      "        \n",
      "        This software is freely available, but we expect that all publications\n",
      "        describing work using this software, or all commercial products using it,\n",
      "        quote at least one of the references given below. This software is released\n",
      "        under the BSD License.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
      "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
      "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
      "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
      "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
      "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
      "          ACM Transactions on Mathematical Software, 38, 1.\n",
      "    \n",
      "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Unconstrained minimization of a function using the Newton-CG method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable ``f(x, *args)``\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable ``f'(x, *args)``\n",
      "            Gradient of f.\n",
      "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
      "            Function which computes the Hessian of f times an\n",
      "            arbitrary vector, p.\n",
      "        fhess : callable ``fhess(x, *args)``, optional\n",
      "            Function to compute the Hessian matrix of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
      "            (the same set of extra arguments is supplied to all of\n",
      "            these functions).\n",
      "        epsilon : float or ndarray, optional\n",
      "            If fhess is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function which is called after\n",
      "            each iteration. Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        avextol : float, optional\n",
      "            Convergence is assumed when the average relative error in\n",
      "            the minimizer falls below this amount.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True, return the optional outputs.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence message.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of results at each iteration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float\n",
      "            Value of the function at xopt, i.e., ``fopt = f(xopt)``.\n",
      "        fcalls : int\n",
      "            Number of function calls made.\n",
      "        gcalls : int\n",
      "            Number of gradient calls made.\n",
      "        hcalls : int\n",
      "            Number of Hessian calls made.\n",
      "        warnflag : int\n",
      "            Warnings generated by the algorithm.\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Line search failure (precision loss).\n",
      "            3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            The result at each iteration, if retall is True (see below).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Newton-CG' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
      "        is provided, then `fhess_p` will be ignored. If neither `fhess`\n",
      "        nor `fhess_p` is provided, then the hessian product will be\n",
      "        approximated using finite differences on `fprime`. `fhess_p`\n",
      "        must compute the hessian times an arbitrary vector. If it is not\n",
      "        given, finite-differences on `fprime` are used to compute\n",
      "        it.\n",
      "        \n",
      "        Newton-CG methods are also called truncated Newton methods. This\n",
      "        function differs from scipy.optimize.fmin_tnc because\n",
      "        \n",
      "        1. scipy.optimize.fmin_ncg is written purely in Python using NumPy\n",
      "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
      "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
      "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
      "            or box constrained minimization. (Box constraints give\n",
      "            lower and upper bounds for each variable separately.)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright & Nocedal, 'Numerical Optimization', 1999, p. 140.\n",
      "    \n",
      "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
      "        Minimize a function using modified Powell's method.\n",
      "        \n",
      "        This method only uses function values, not derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func.\n",
      "        xtol : float, optional\n",
      "            Line-search error tolerance.\n",
      "        ftol : float, optional\n",
      "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            If True, ``fopt``, ``xi``, ``direc``, ``iter``, ``funcalls``, and\n",
      "            ``warnflag`` are returned.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence messages.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of the solution at each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each\n",
      "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        direc : ndarray, optional\n",
      "            Initial fitting step and parameter order set as an (N, N) array, where N\n",
      "            is the number of fitting parameters in `x0`. Defaults to step size 1.0\n",
      "            fitting all parameters simultaneously (``np.ones((N, N))``). To\n",
      "            prevent initial consideration of values in a step or to change initial\n",
      "            step size, set to 0 or desired step size in the Jth position in the Mth\n",
      "            block, where J is the position in `x0` and M is the desired evaluation\n",
      "            step, with steps being evaluated in index order. Step size and ordering\n",
      "            will change freely as minimization proceeds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter which minimizes `func`.\n",
      "        fopt : number\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        direc : ndarray\n",
      "            Current direction set.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            Integer warning flag:\n",
      "                1 : Maximum number of function evaluations.\n",
      "                2 : Maximum number of iterations.\n",
      "                3 : NaN result encountered.\n",
      "                4 : The result is out of the provided bounds.\n",
      "        allvecs : list\n",
      "            List of solutions at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to unconstrained minimization algorithms for\n",
      "            multivariate functions. See the 'Powell' method in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a modification of Powell's method to find the minimum of\n",
      "        a function of N variables. Powell's method is a conjugate\n",
      "        direction method.\n",
      "        \n",
      "        The algorithm has two loops. The outer loop merely iterates over the inner\n",
      "        loop. The inner loop minimizes over each current direction in the direction\n",
      "        set. At the end of the inner loop, if certain conditions are met, the\n",
      "        direction that gave the largest decrease is dropped and replaced with the\n",
      "        difference between the current estimated x and the estimated x from the\n",
      "        beginning of the inner-loop.\n",
      "        \n",
      "        The technical conditions for replacing the direction of greatest\n",
      "        increase amount to checking that\n",
      "        \n",
      "        1. No further gain can be made along the direction of greatest increase\n",
      "           from that iteration.\n",
      "        2. The direction of greatest increase accounted for a large sufficient\n",
      "           fraction of the decrease in the function value from that iteration of\n",
      "           the inner loop.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
      "        function of several variables without calculating derivatives,\n",
      "        Computer Journal, 7 (2):155-162.\n",
      "        \n",
      "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
      "        Numerical Recipes (any edition), Cambridge University Press\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin_powell(f, -1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 2\n",
      "                 Function evaluations: 18\n",
      "        >>> minimum\n",
      "        array(0.0)\n",
      "    \n",
      "    fmin_slsqp(func, x0, eqcons=(), f_eqcons=None, ieqcons=(), f_ieqcons=None, bounds=(), fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=1.4901161193847656e-08, callback=None)\n",
      "        Minimize a function using Sequential Least Squares Programming\n",
      "        \n",
      "        Python interface function for the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.  Must return a scalar.\n",
      "        x0 : 1-D ndarray of float\n",
      "            Initial guess for the independent variable(s).\n",
      "        eqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_eqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D array in which each element must equal 0.0 in a\n",
      "            successfully optimized problem. If f_eqcons is specified,\n",
      "            eqcons is ignored.\n",
      "        ieqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_ieqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D ndarray in which each element must be greater or\n",
      "            equal to 0.0 in a successfully optimized problem. If\n",
      "            f_ieqcons is specified, ieqcons is ignored.\n",
      "        bounds : list, optional\n",
      "            A list of tuples specifying the lower and upper bound\n",
      "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
      "            Infinite values will be interpreted as large floating values.\n",
      "        fprime : callable `f(x,*args)`, optional\n",
      "            A function that evaluates the partial derivatives of func.\n",
      "        fprime_eqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of equality constraint normals. If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
      "        fprime_ieqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of inequality constraint normals. If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
      "        args : sequence, optional\n",
      "            Additional arguments passed to func and fprime.\n",
      "        iter : int, optional\n",
      "            The maximum number of iterations.\n",
      "        acc : float, optional\n",
      "            Requested accuracy.\n",
      "        iprint : int, optional\n",
      "            The verbosity of fmin_slsqp :\n",
      "        \n",
      "            * iprint <= 0 : Silent operation\n",
      "            * iprint == 1 : Print summary upon completion (default)\n",
      "            * iprint >= 2 : Print status of each iterate and summary\n",
      "        disp : int, optional\n",
      "            Overrides the iprint interface (preferred).\n",
      "        full_output : bool, optional\n",
      "            If False, return only the minimizer of func (default).\n",
      "            Otherwise, output final objective function and summary\n",
      "            information.\n",
      "        epsilon : float, optional\n",
      "            The step size for finite-difference derivative estimates.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(x)``, where ``x`` is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray of float\n",
      "            The final minimizer of func.\n",
      "        fx : ndarray of float, if full_output is true\n",
      "            The final value of the objective function.\n",
      "        its : int, if full_output is true\n",
      "            The number of iterations.\n",
      "        imode : int, if full_output is true\n",
      "            The exit mode from the optimizer (see below).\n",
      "        smode : string, if full_output is true\n",
      "            Message describing the exit mode from the optimizer.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'SLSQP' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Exit modes are defined as follows ::\n",
      "        \n",
      "            -1 : Gradient evaluation required (g & a)\n",
      "             0 : Optimization terminated successfully\n",
      "             1 : Function evaluation required (f & c)\n",
      "             2 : More equality constraints than independent variables\n",
      "             3 : More than 3*n iterations in LSQ subproblem\n",
      "             4 : Inequality constraints incompatible\n",
      "             5 : Singular matrix E in LSQ subproblem\n",
      "             6 : Singular matrix C in LSQ subproblem\n",
      "             7 : Rank-deficient equality constraint subproblem HFTI\n",
      "             8 : Positive directional derivative for linesearch\n",
      "             9 : Iteration limit reached\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
      "    \n",
      "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
      "        Minimize a function with variables subject to bounds, using\n",
      "        gradient information in a truncated Newton algorithm. This\n",
      "        method wraps a C implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x, *args)``\n",
      "            Function to minimize.  Must do one of:\n",
      "        \n",
      "            1. Return f and g, where f is the value of the function and g its\n",
      "               gradient (a list of floats).\n",
      "        \n",
      "            2. Return the function value but supply gradient function\n",
      "               separately as `fprime`.\n",
      "        \n",
      "            3. Return the function value and set ``approx_grad=True``.\n",
      "        \n",
      "            If the function returns None, the minimization\n",
      "            is aborted.\n",
      "        x0 : array_like\n",
      "            Initial estimate of minimum.\n",
      "        fprime : callable ``fprime(x, *args)``, optional\n",
      "            Gradient of `func`. If None, then either `func` must return the\n",
      "            function value and the gradient (``f,g = func(x, *args)``)\n",
      "            or `approx_grad` must be True.\n",
      "        args : tuple, optional\n",
      "            Arguments to pass to function.\n",
      "        approx_grad : bool, optional\n",
      "            If true, approximate the gradient numerically.\n",
      "        bounds : list, optional\n",
      "            (min, max) pairs for each element in x0, defining the\n",
      "            bounds on that parameter. Use None or +/-inf for one of\n",
      "            min or max when there is no bound in that direction.\n",
      "        epsilon : float, optional\n",
      "            Used if approx_grad is True. The stepsize in a finite\n",
      "            difference approximation for fprime.\n",
      "        scale : array_like, optional\n",
      "            Scaling factors to apply to each variable. If None, the\n",
      "            factors are up-low for interval bounded variables and\n",
      "            1+|x| for the others. Defaults to None.\n",
      "        offset : array_like, optional\n",
      "            Value to subtract from each variable. If None, the\n",
      "            offsets are (up+low)/2 for interval bounded variables\n",
      "            and x for the others.\n",
      "        messages : int, optional\n",
      "            Bit mask used to select messages display during\n",
      "            minimization values defined in the MSGS dict. Defaults to\n",
      "            MGS_ALL.\n",
      "        disp : int, optional\n",
      "            Integer interface to messages. 0 = no message, 5 = all messages\n",
      "        maxCGit : int, optional\n",
      "            Maximum number of hessian*vector evaluations per main\n",
      "            iteration. If maxCGit == 0, the direction chosen is\n",
      "            -gradient if maxCGit < 0, maxCGit is set to\n",
      "            max(1,min(50,n/2)). Defaults to -1.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluation. If None, maxfun is\n",
      "            set to max(100, 10*len(x0)). Defaults to None.\n",
      "        eta : float, optional\n",
      "            Severity of the line search. If < 0 or > 1, set to 0.25.\n",
      "            Defaults to -1.\n",
      "        stepmx : float, optional\n",
      "            Maximum step for the line search. May be increased during\n",
      "            call. If too small, it will be set to 10.0. Defaults to 0.\n",
      "        accuracy : float, optional\n",
      "            Relative precision for finite difference calculations. If\n",
      "            <= machine_precision, set to sqrt(machine_precision).\n",
      "            Defaults to 0.\n",
      "        fmin : float, optional\n",
      "            Minimum function value estimate. Defaults to 0.\n",
      "        ftol : float, optional\n",
      "            Precision goal for the value of f in the stopping criterion.\n",
      "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "        xtol : float, optional\n",
      "            Precision goal for the value of x in the stopping\n",
      "            criterion (after applying x scaling factors). If xtol <\n",
      "            0.0, xtol is set to sqrt(machine_precision). Defaults to\n",
      "            -1.\n",
      "        pgtol : float, optional\n",
      "            Precision goal for the value of the projected gradient in\n",
      "            the stopping criterion (after applying x scaling factors).\n",
      "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
      "            Setting it to 0.0 is not recommended. Defaults to -1.\n",
      "        rescale : float, optional\n",
      "            Scaling factor (in log10) used to trigger f value\n",
      "            rescaling. If 0, rescale at each iteration. If a large\n",
      "            value, never rescale. If < 0, rescale is set to 1.3.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution.\n",
      "        nfeval : int\n",
      "            The number of function evaluations.\n",
      "        rc : int\n",
      "            Return code, see below\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'TNC' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The underlying algorithm is truncated Newton, also called\n",
      "        Newton Conjugate-Gradient. This method differs from\n",
      "        scipy.optimize.fmin_ncg in that\n",
      "        \n",
      "        1. it wraps a C implementation of the algorithm\n",
      "        2. it allows each variable to be given an upper and lower bound.\n",
      "        \n",
      "        The algorithm incorporates the bound constraints by determining\n",
      "        the descent direction as in an unconstrained truncated Newton,\n",
      "        but never taking a step-size large enough to leave the space\n",
      "        of feasible x's. The algorithm keeps track of a set of\n",
      "        currently active constraints, and ignores them when computing\n",
      "        the minimum allowable step size. (The x's associated with the\n",
      "        active constraint are kept fixed.) If the maximum allowable\n",
      "        step size is zero then a new constraint is added. At the end\n",
      "        of each iteration one of the constraints may be deemed no\n",
      "        longer active and removed. A constraint is considered\n",
      "        no longer active is if it is currently active\n",
      "        but the gradient for that variable points inward from the\n",
      "        constraint. The specific constraint removed is the one\n",
      "        associated with the variable of largest index whose\n",
      "        constraint is no longer active.\n",
      "        \n",
      "        Return codes are defined as follows::\n",
      "        \n",
      "            -1 : Infeasible (lower bound > upper bound)\n",
      "             0 : Local minimum reached (|pg| ~= 0)\n",
      "             1 : Converged (|f_n-f_(n-1)| ~= 0)\n",
      "             2 : Converged (|x_n-x_(n-1)| ~= 0)\n",
      "             3 : Max. number of function evaluations reached\n",
      "             4 : Linear search failed\n",
      "             5 : All lower bounds are equal to the upper bounds\n",
      "             6 : Unable to progress\n",
      "             7 : User requested end of minimization\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
      "        \n",
      "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
      "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
      "    \n",
      "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
      "        Bounded minimization for scalar functions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized (must accept and return scalars).\n",
      "        x1, x2 : float or array scalar\n",
      "            The optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to function.\n",
      "        xtol : float, optional\n",
      "            The convergence tolerance.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations allowed.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        disp : int, optional\n",
      "            If non-zero, print messages.\n",
      "                0 : no message printing.\n",
      "                1 : non-convergence notification messages only.\n",
      "                2 : print a message on convergence too.\n",
      "                3 : print iteration results.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters (over given interval) which minimize the\n",
      "            objective function.\n",
      "        fval : number\n",
      "            The function value at the minimum point.\n",
      "        ierr : int\n",
      "            An error flag (0 if converged, 1 if maximum number of\n",
      "            function calls reached).\n",
      "        numfunc : int\n",
      "          The number of function calls made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Bounded' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Finds a local minimizer of the scalar function `func` in the\n",
      "        interval x1 < xopt < x2 using Brent's method. (See `brent`\n",
      "        for auto-bracketing.)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        `fminbound` finds the minimum of the function in the given range.\n",
      "        The following examples illustrate the same\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fminbound(f, -1, 2)\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.fminbound(f, 1, 2)\n",
      "        >>> minimum\n",
      "        1.0000059608609866\n",
      "    \n",
      "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
      "        Find the roots of a function.\n",
      "        \n",
      "        Return the roots of the (non-linear) equations defined by\n",
      "        ``func(x) = 0`` given a starting estimate.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            A function that takes at least one (possibly vector) argument,\n",
      "            and returns a value of the same length.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the roots of ``func(x) = 0``.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to `func`.\n",
      "        fprime : callable ``f(x, *args)``, optional\n",
      "            A function to compute the Jacobian of `func` with derivatives\n",
      "            across the rows. By default, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            Specify whether the Jacobian function computes derivatives down\n",
      "            the columns (faster, because there is no transpose operation).\n",
      "        xtol : float, optional\n",
      "            The calculation will terminate if the relative error between two\n",
      "            consecutive iterates is at most `xtol`.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If zero, then\n",
      "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
      "            in `x0`.\n",
      "        band : tuple, optional\n",
      "            If set to a two-sequence containing the number of sub- and\n",
      "            super-diagonals within the band of the Jacobi matrix, the\n",
      "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
      "        epsfcn : float, optional\n",
      "            A suitable step length for the forward-difference\n",
      "            approximation of the Jacobian (for ``fprime=None``). If\n",
      "            `epsfcn` is less than the machine precision, it is assumed\n",
      "            that the relative errors in the functions are of the order of\n",
      "            the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in the interval\n",
      "            ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the\n",
      "            variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for\n",
      "            an unsuccessful call).\n",
      "        infodict : dict\n",
      "            A dictionary of optional outputs with the keys:\n",
      "        \n",
      "            ``nfev``\n",
      "                number of function calls\n",
      "            ``njev``\n",
      "                number of Jacobian calls\n",
      "            ``fvec``\n",
      "                function evaluated at the output\n",
      "            ``fjac``\n",
      "                the orthogonal matrix, q, produced by the QR\n",
      "                factorization of the final approximate Jacobian\n",
      "                matrix, stored column wise\n",
      "            ``r``\n",
      "                upper triangular matrix produced by QR factorization\n",
      "                of the same matrix\n",
      "            ``qtf``\n",
      "                the vector ``(transpose(q) * fvec)``\n",
      "        \n",
      "        ier : int\n",
      "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
      "            to `mesg` for more information.\n",
      "        mesg : str\n",
      "            If no solution is found, `mesg` details the cause of failure.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See the ``method=='hybr'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Find a solution to the system of equations:\n",
      "        ``x0*cos(x1) = 4,  x1*x0 - x1 = 5``.\n",
      "        \n",
      "        >>> from scipy.optimize import fsolve\n",
      "        >>> def func(x):\n",
      "        ...     return [x[0] * np.cos(x[1]) - 4,\n",
      "        ...             x[1] * x[0] - x[1] - 5]\n",
      "        >>> root = fsolve(func, [1, 1])\n",
      "        >>> root\n",
      "        array([6.50409711, 0.90841421])\n",
      "        >>> np.isclose(func(root), [0.0, 0.0])  # func(root) should be almost 0.0.\n",
      "        array([ True,  True])\n",
      "    \n",
      "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0, maxiter=5000)\n",
      "        Return the minimum of a function of one variable using golden section\n",
      "        method.\n",
      "        \n",
      "        Given a function of one variable and a possible bracketing interval,\n",
      "        return the minimum of the function isolated to a fractional precision of\n",
      "        tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            Objective function to minimize.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to func.\n",
      "        brack : tuple, optional\n",
      "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
      "            func(a),func(c). If bracket consists of two numbers (a,\n",
      "            c), then they are assumed to be a starting interval for a\n",
      "            downhill bracket search (see `bracket`); it doesn't always\n",
      "            mean that obtained solution will satisfy a<=x<=c.\n",
      "        tol : float, optional\n",
      "            x tolerance stop criterion\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Golden' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses analog of bisection method to decrease the bracketed\n",
      "        interval.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3, respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range ``(xa, xb)``.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.golden(f, brack=(1, 2))\n",
      "        >>> minimum\n",
      "        1.5717277788484873e-162\n",
      "        >>> minimum = optimize.golden(f, brack=(-1, 0.5, 2))\n",
      "        >>> minimum\n",
      "        -1.5717277788484873e-162\n",
      "    \n",
      "    least_squares(fun, x0, jac='2-point', bounds=(-inf, inf), method='trf', ftol=1e-08, xtol=1e-08, gtol=1e-08, x_scale=1.0, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={})\n",
      "        Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given the residuals f(x) (an m-D real function of n real\n",
      "        variables) and the loss function rho(s) (a scalar function), `least_squares`\n",
      "        finds a local minimum of the cost function F(x)::\n",
      "        \n",
      "            minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        The purpose of the loss function rho(s) is to reduce the influence of\n",
      "        outliers on the solution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Function which computes the vector of residuals, with the signature\n",
      "            ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\n",
      "            respect to its first argument. The argument ``x`` passed to this\n",
      "            function is an ndarray of shape (n,) (never a scalar, even for n=1).\n",
      "            It must allocate and return a 1-D array_like of shape (m,) or a scalar.\n",
      "            If the argument ``x`` is complex or the function ``fun`` returns\n",
      "            complex residuals, it must be wrapped in a real function of real\n",
      "            arguments, as shown at the end of the Examples section.\n",
      "        x0 : array_like with shape (n,) or float\n",
      "            Initial guess on independent variables. If float, it will be treated\n",
      "            as a 1-D array with one element.\n",
      "        jac : {'2-point', '3-point', 'cs', callable}, optional\n",
      "            Method of computing the Jacobian matrix (an m-by-n matrix, where\n",
      "            element (i, j) is the partial derivative of f[i] with respect to\n",
      "            x[j]). The keywords select a finite difference scheme for numerical\n",
      "            estimation. The scheme '3-point' is more accurate, but requires\n",
      "            twice as many operations as '2-point' (default). The scheme 'cs'\n",
      "            uses complex steps, and while potentially the most accurate, it is\n",
      "            applicable only when `fun` correctly handles complex inputs and\n",
      "            can be analytically continued to the complex plane. Method 'lm'\n",
      "            always uses the '2-point' scheme. If callable, it is used as\n",
      "            ``jac(x, *args, **kwargs)`` and should return a good approximation\n",
      "            (or the exact value) for the Jacobian as an array_like (np.atleast_2d\n",
      "            is applied), a sparse matrix or a `scipy.sparse.linalg.LinearOperator`.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must match the size of `x0` or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : {'trf', 'dogbox', 'lm'}, optional\n",
      "            Algorithm to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm, particularly suitable\n",
      "                  for large sparse problems with bounds. Generally robust method.\n",
      "                * 'dogbox' : dogleg algorithm with rectangular trust regions,\n",
      "                  typical use case is small problems with bounds. Not recommended\n",
      "                  for problems with rank-deficient Jacobian.\n",
      "                * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\n",
      "                  Doesn't handle bounds and sparse Jacobians. Usually the most\n",
      "                  efficient method for small unconstrained problems.\n",
      "        \n",
      "            Default is 'trf'. See Notes for more information.\n",
      "        ftol : float or None, optional\n",
      "            Tolerance for termination by the change of the cost function. Default\n",
      "            is 1e-8. The optimization process is stopped when ``dF < ftol * F``,\n",
      "            and there was an adequate agreement between a local quadratic model and\n",
      "            the true model in the last step. If None, the termination by this\n",
      "            condition is disabled.\n",
      "        xtol : float or None, optional\n",
      "            Tolerance for termination by the change of the independent variables.\n",
      "            Default is 1e-8. The exact condition depends on the `method` used:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``.\n",
      "                * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\n",
      "                  a trust-region radius and ``xs`` is the value of ``x``\n",
      "                  scaled according to `x_scale` parameter (see below).\n",
      "        \n",
      "            If None, the termination by this condition is disabled.\n",
      "        gtol : float or None, optional\n",
      "            Tolerance for termination by the norm of the gradient. Default is 1e-8.\n",
      "            The exact condition depends on a `method` used:\n",
      "        \n",
      "                * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\n",
      "                  ``g_scaled`` is the value of the gradient scaled to account for\n",
      "                  the presence of the bounds [STIR]_.\n",
      "                * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\n",
      "                  ``g_free`` is the gradient with respect to the variables which\n",
      "                  are not in the optimal state on the boundary.\n",
      "                * For 'lm' : the maximum absolute value of the cosine of angles\n",
      "                  between columns of the Jacobian and the residual vector is less\n",
      "                  than `gtol`, or the residual vector is zero.\n",
      "        \n",
      "            If None, the termination by this condition is disabled.\n",
      "        x_scale : array_like or 'jac', optional\n",
      "            Characteristic scale of each variable. Setting `x_scale` is equivalent\n",
      "            to reformulating the problem in scaled variables ``xs = x / x_scale``.\n",
      "            An alternative view is that the size of a trust region along jth\n",
      "            dimension is proportional to ``x_scale[j]``. Improved convergence may\n",
      "            be achieved by setting `x_scale` such that a step of a given size\n",
      "            along any of the scaled variables has a similar effect on the cost\n",
      "            function. If set to 'jac', the scale is iteratively updated using the\n",
      "            inverse norms of the columns of the Jacobian matrix (as described in\n",
      "            [JJMore]_).\n",
      "        loss : str or callable, optional\n",
      "            Determines the loss function. The following keyword values are allowed:\n",
      "        \n",
      "                * 'linear' (default) : ``rho(z) = z``. Gives a standard\n",
      "                  least-squares problem.\n",
      "                * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\n",
      "                  approximation of l1 (absolute value) loss. Usually a good\n",
      "                  choice for robust least squares.\n",
      "                * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\n",
      "                  similarly to 'soft_l1'.\n",
      "                * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\n",
      "                  influence, but may cause difficulties in optimization process.\n",
      "                * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\n",
      "                  a single residual, has properties similar to 'cauchy'.\n",
      "        \n",
      "            If callable, it must take a 1-D ndarray ``z=f**2`` and return an\n",
      "            array_like with shape (3, m) where row 0 contains function values,\n",
      "            row 1 contains first derivatives and row 2 contains second\n",
      "            derivatives. Method 'lm' supports only 'linear' loss.\n",
      "        f_scale : float, optional\n",
      "            Value of soft margin between inlier and outlier residuals, default\n",
      "            is 1.0. The loss function is evaluated as follows\n",
      "            ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\n",
      "            and ``rho`` is determined by `loss` parameter. This parameter has\n",
      "            no effect with ``loss='linear'``, but for other `loss` values it is\n",
      "            of crucial importance.\n",
      "        max_nfev : None or int, optional\n",
      "            Maximum number of function evaluations before the termination.\n",
      "            If None (default), the value is chosen automatically:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : 100 * n.\n",
      "                * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\n",
      "                  otherwise (because 'lm' counts function calls in Jacobian\n",
      "                  estimation).\n",
      "        \n",
      "        diff_step : None or array_like, optional\n",
      "            Determines the relative step size for the finite difference\n",
      "            approximation of the Jacobian. The actual step is computed as\n",
      "            ``x * diff_step``. If None (default), then `diff_step` is taken to be\n",
      "            a conventional \"optimal\" power of machine epsilon for the finite\n",
      "            difference scheme used [NR]_.\n",
      "        tr_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method for solving trust-region subproblems, relevant only for 'trf'\n",
      "            and 'dogbox' methods.\n",
      "        \n",
      "                * 'exact' is suitable for not very large problems with dense\n",
      "                  Jacobian matrices. The computational complexity per iteration is\n",
      "                  comparable to a singular value decomposition of the Jacobian\n",
      "                  matrix.\n",
      "                * 'lsmr' is suitable for problems with sparse and large Jacobian\n",
      "                  matrices. It uses the iterative procedure\n",
      "                  `scipy.sparse.linalg.lsmr` for finding a solution of a linear\n",
      "                  least-squares problem and only requires matrix-vector product\n",
      "                  evaluations.\n",
      "        \n",
      "            If None (default), the solver is chosen based on the type of Jacobian\n",
      "            returned on the first iteration.\n",
      "        tr_options : dict, optional\n",
      "            Keyword options passed to trust-region solver.\n",
      "        \n",
      "                * ``tr_solver='exact'``: `tr_options` are ignored.\n",
      "                * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\n",
      "                  Additionally,  ``method='trf'`` supports  'regularize' option\n",
      "                  (bool, default is True), which adds a regularization term to the\n",
      "                  normal equation, which improves convergence if the Jacobian is\n",
      "                  rank-deficient [Byrd]_ (eq. 3.4).\n",
      "        \n",
      "        jac_sparsity : {None, array_like, sparse matrix}, optional\n",
      "            Defines the sparsity structure of the Jacobian matrix for finite\n",
      "            difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "            only few non-zero elements in *each* row, providing the sparsity\n",
      "            structure will greatly speed up the computations [Curtis]_. A zero\n",
      "            entry means that a corresponding element in the Jacobian is identically\n",
      "            zero. If provided, forces the use of 'lsmr' trust-region solver.\n",
      "            If None (default), then dense differencing will be used. Has no effect\n",
      "            for 'lm' method.\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 (default) : work silently.\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations (not supported by 'lm'\n",
      "                  method).\n",
      "        \n",
      "        args, kwargs : tuple and dict, optional\n",
      "            Additional arguments passed to `fun` and `jac`. Both empty by default.\n",
      "            The calling signature is ``fun(x, *args, **kwargs)`` and the same for\n",
      "            `jac`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `OptimizeResult` with the following fields defined:\n",
      "        x : ndarray, shape (n,)\n",
      "            Solution found.\n",
      "        cost : float\n",
      "            Value of the cost function at the solution.\n",
      "        fun : ndarray, shape (m,)\n",
      "            Vector of residuals at the solution.\n",
      "        jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\n",
      "            Modified Jacobian matrix at the solution, in the sense that J^T J\n",
      "            is a Gauss-Newton approximation of the Hessian of the cost function.\n",
      "            The type is the same as the one used by the algorithm.\n",
      "        grad : ndarray, shape (m,)\n",
      "            Gradient of the cost function at the solution.\n",
      "        optimality : float\n",
      "            First-order optimality measure. In unconstrained problems, it is always\n",
      "            the uniform norm of the gradient. In constrained problems, it is the\n",
      "            quantity which was compared with `gtol` during iterations.\n",
      "        active_mask : ndarray of int, shape (n,)\n",
      "            Each component shows whether a corresponding constraint is active\n",
      "            (that is, whether a variable is at the bound):\n",
      "        \n",
      "                *  0 : a constraint is not active.\n",
      "                * -1 : a lower bound is active.\n",
      "                *  1 : an upper bound is active.\n",
      "        \n",
      "            Might be somewhat arbitrary for 'trf' method as it generates a sequence\n",
      "            of strictly feasible iterates and `active_mask` is determined within a\n",
      "            tolerance threshold.\n",
      "        nfev : int\n",
      "            Number of function evaluations done. Methods 'trf' and 'dogbox' do not\n",
      "            count function calls for numerical Jacobian approximation, as opposed\n",
      "            to 'lm' method.\n",
      "        njev : int or None\n",
      "            Number of Jacobian evaluations done. If numerical Jacobian\n",
      "            approximation is used in 'lm' method, it is set to None.\n",
      "        status : int\n",
      "            The reason for algorithm termination:\n",
      "        \n",
      "                * -1 : improper input parameters status returned from MINPACK.\n",
      "                *  0 : the maximum number of function evaluations is exceeded.\n",
      "                *  1 : `gtol` termination condition is satisfied.\n",
      "                *  2 : `ftol` termination condition is satisfied.\n",
      "                *  3 : `xtol` termination condition is satisfied.\n",
      "                *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\n",
      "        \n",
      "        message : str\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        leastsq : A legacy wrapper for the MINPACK implementation of the\n",
      "                  Levenberg-Marquadt algorithm.\n",
      "        curve_fit : Least-squares minimization applied to a curve-fitting problem.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\n",
      "        algorithms implemented in MINPACK (lmder, lmdif). It runs the\n",
      "        Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\n",
      "        The implementation is based on paper [JJMore]_, it is very robust and\n",
      "        efficient with a lot of smart tricks. It should be your first choice\n",
      "        for unconstrained problems. Note that it doesn't support bounds. Also,\n",
      "        it doesn't work when m < n.\n",
      "        \n",
      "        Method 'trf' (Trust Region Reflective) is motivated by the process of\n",
      "        solving a system of equations, which constitute the first-order optimality\n",
      "        condition for a bound-constrained minimization problem as formulated in\n",
      "        [STIR]_. The algorithm iteratively solves trust-region subproblems\n",
      "        augmented by a special diagonal quadratic term and with trust-region shape\n",
      "        determined by the distance from the bounds and the direction of the\n",
      "        gradient. This enhancements help to avoid making steps directly into bounds\n",
      "        and efficiently explore the whole space of variables. To further improve\n",
      "        convergence, the algorithm considers search directions reflected from the\n",
      "        bounds. To obey theoretical requirements, the algorithm keeps iterates\n",
      "        strictly feasible. With dense Jacobians trust-region subproblems are\n",
      "        solved by an exact method very similar to the one described in [JJMore]_\n",
      "        (and implemented in MINPACK). The difference from the MINPACK\n",
      "        implementation is that a singular value decomposition of a Jacobian\n",
      "        matrix is done once per iteration, instead of a QR decomposition and series\n",
      "        of Givens rotation eliminations. For large sparse Jacobians a 2-D subspace\n",
      "        approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\n",
      "        The subspace is spanned by a scaled gradient and an approximate\n",
      "        Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\n",
      "        constraints are imposed the algorithm is very similar to MINPACK and has\n",
      "        generally comparable performance. The algorithm works quite robust in\n",
      "        unbounded and bounded problems, thus it is chosen as a default algorithm.\n",
      "        \n",
      "        Method 'dogbox' operates in a trust-region framework, but considers\n",
      "        rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\n",
      "        The intersection of a current trust region and initial bounds is again\n",
      "        rectangular, so on each iteration a quadratic minimization problem subject\n",
      "        to bound constraints is solved approximately by Powell's dogleg method\n",
      "        [NumOpt]_. The required Gauss-Newton step can be computed exactly for\n",
      "        dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\n",
      "        sparse Jacobians. The algorithm is likely to exhibit slow convergence when\n",
      "        the rank of Jacobian is less than the number of variables. The algorithm\n",
      "        often outperforms 'trf' in bounded problems with a small number of\n",
      "        variables.\n",
      "        \n",
      "        Robust loss functions are implemented as described in [BA]_. The idea\n",
      "        is to modify a residual vector and a Jacobian matrix on each iteration\n",
      "        such that computed gradient and Gauss-Newton Hessian approximation match\n",
      "        the true gradient and Hessian approximation of the cost function. Then\n",
      "        the algorithm proceeds in a normal way, i.e., robust loss functions are\n",
      "        implemented as a simple wrapper over standard least-squares algorithms.\n",
      "        \n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\n",
      "                Computing. 3rd edition\", Sec. 5.7.\n",
      "        .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\n",
      "                  solution of the trust region problem by minimization over\n",
      "                  two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\n",
      "                  1988.\n",
      "        .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\n",
      "                    sparse Jacobian matrices\", Journal of the Institute of\n",
      "                    Mathematics and its Applications, 13, pp. 117-120, 1974.\n",
      "        .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\n",
      "                    and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\n",
      "                    Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\n",
      "        .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\n",
      "                    Dogleg Approach for Unconstrained and Bound Constrained\n",
      "                    Nonlinear Optimization\", WSEAS International Conference on\n",
      "                    Applied Mathematics, Corfu, Greece, 2004.\n",
      "        .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\n",
      "                    2nd edition\", Chapter 4.\n",
      "        .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\n",
      "                Proceedings of the International Workshop on Vision Algorithms:\n",
      "                Theory and Practice, pp. 298-372, 1999.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example we find a minimum of the Rosenbrock function without bounds\n",
      "        on independent variables.\n",
      "        \n",
      "        >>> def fun_rosenbrock(x):\n",
      "        ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\n",
      "        \n",
      "        Notice that we only provide the vector of the residuals. The algorithm\n",
      "        constructs the cost function as a sum of squares of the residuals, which\n",
      "        gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> x0_rosenbrock = np.array([2, 2])\n",
      "        >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\n",
      "        >>> res_1.x\n",
      "        array([ 1.,  1.])\n",
      "        >>> res_1.cost\n",
      "        9.8669242910846867e-30\n",
      "        >>> res_1.optimality\n",
      "        8.8928864934219529e-14\n",
      "        \n",
      "        We now constrain the variables, in such a way that the previous solution\n",
      "        becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\n",
      "        ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\n",
      "        to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\n",
      "        \n",
      "        We also provide the analytic Jacobian:\n",
      "        \n",
      "        >>> def jac_rosenbrock(x):\n",
      "        ...     return np.array([\n",
      "        ...         [-20 * x[0], 10],\n",
      "        ...         [-1, 0]])\n",
      "        \n",
      "        Putting this all together, we see that the new solution lies on the bound:\n",
      "        \n",
      "        >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\n",
      "        ...                       bounds=([-np.inf, 1.5], np.inf))\n",
      "        >>> res_2.x\n",
      "        array([ 1.22437075,  1.5       ])\n",
      "        >>> res_2.cost\n",
      "        0.025213093946805685\n",
      "        >>> res_2.optimality\n",
      "        1.5885401433157753e-07\n",
      "        \n",
      "        Now we solve a system of equations (i.e., the cost function should be zero\n",
      "        at a minimum) for a Broyden tridiagonal vector-valued function of 100000\n",
      "        variables:\n",
      "        \n",
      "        >>> def fun_broyden(x):\n",
      "        ...     f = (3 - x) * x + 1\n",
      "        ...     f[1:] -= x[:-1]\n",
      "        ...     f[:-1] -= 2 * x[1:]\n",
      "        ...     return f\n",
      "        \n",
      "        The corresponding Jacobian matrix is sparse. We tell the algorithm to\n",
      "        estimate it by finite differences and provide the sparsity structure of\n",
      "        Jacobian to significantly speed up this process.\n",
      "        \n",
      "        >>> from scipy.sparse import lil_matrix\n",
      "        >>> def sparsity_broyden(n):\n",
      "        ...     sparsity = lil_matrix((n, n), dtype=int)\n",
      "        ...     i = np.arange(n)\n",
      "        ...     sparsity[i, i] = 1\n",
      "        ...     i = np.arange(1, n)\n",
      "        ...     sparsity[i, i - 1] = 1\n",
      "        ...     i = np.arange(n - 1)\n",
      "        ...     sparsity[i, i + 1] = 1\n",
      "        ...     return sparsity\n",
      "        ...\n",
      "        >>> n = 100000\n",
      "        >>> x0_broyden = -np.ones(n)\n",
      "        ...\n",
      "        >>> res_3 = least_squares(fun_broyden, x0_broyden,\n",
      "        ...                       jac_sparsity=sparsity_broyden(n))\n",
      "        >>> res_3.cost\n",
      "        4.5687069299604613e-23\n",
      "        >>> res_3.optimality\n",
      "        1.1650454296851518e-11\n",
      "        \n",
      "        Let's also solve a curve fitting problem using robust loss function to\n",
      "        take care of outliers in the data. Define the model function as\n",
      "        ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\n",
      "        observation and a, b, c are parameters to estimate.\n",
      "        \n",
      "        First, define the function which generates the data with noise and\n",
      "        outliers, define the model parameters, and generate data:\n",
      "        \n",
      "        >>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):\n",
      "        ...     y = a + b * np.exp(t * c)\n",
      "        ...\n",
      "        ...     rnd = np.random.RandomState(random_state)\n",
      "        ...     error = noise * rnd.randn(t.size)\n",
      "        ...     outliers = rnd.randint(0, t.size, n_outliers)\n",
      "        ...     error[outliers] *= 10\n",
      "        ...\n",
      "        ...     return y + error\n",
      "        ...\n",
      "        >>> a = 0.5\n",
      "        >>> b = 2.0\n",
      "        >>> c = -1\n",
      "        >>> t_min = 0\n",
      "        >>> t_max = 10\n",
      "        >>> n_points = 15\n",
      "        ...\n",
      "        >>> t_train = np.linspace(t_min, t_max, n_points)\n",
      "        >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\n",
      "        \n",
      "        Define function for computing residuals and initial estimate of\n",
      "        parameters.\n",
      "        \n",
      "        >>> def fun(x, t, y):\n",
      "        ...     return x[0] + x[1] * np.exp(x[2] * t) - y\n",
      "        ...\n",
      "        >>> x0 = np.array([1.0, 1.0, 0.0])\n",
      "        \n",
      "        Compute a standard least-squares solution:\n",
      "        \n",
      "        >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\n",
      "        \n",
      "        Now compute two solutions with two different robust loss functions. The\n",
      "        parameter `f_scale` is set to 0.1, meaning that inlier residuals should\n",
      "        not significantly exceed 0.1 (the noise level used).\n",
      "        \n",
      "        >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\n",
      "        ...                             args=(t_train, y_train))\n",
      "        >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\n",
      "        ...                         args=(t_train, y_train))\n",
      "        \n",
      "        And, finally, plot all the curves. We see that by selecting an appropriate\n",
      "        `loss`  we can get estimates close to optimal even in the presence of\n",
      "        strong outliers. But keep in mind that generally it is recommended to try\n",
      "        'soft_l1' or 'huber' losses first (if at all necessary) as the other two\n",
      "        options may cause difficulties in optimization process.\n",
      "        \n",
      "        >>> t_test = np.linspace(t_min, t_max, n_points * 10)\n",
      "        >>> y_true = gen_data(t_test, a, b, c)\n",
      "        >>> y_lsq = gen_data(t_test, *res_lsq.x)\n",
      "        >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
      "        >>> y_log = gen_data(t_test, *res_log.x)\n",
      "        ...\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(t_train, y_train, 'o')\n",
      "        >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\n",
      "        >>> plt.plot(t_test, y_lsq, label='linear loss')\n",
      "        >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\n",
      "        >>> plt.plot(t_test, y_log, label='cauchy loss')\n",
      "        >>> plt.xlabel(\"t\")\n",
      "        >>> plt.ylabel(\"y\")\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "        \n",
      "        In the next example, we show how complex-valued residual functions of\n",
      "        complex variables can be optimized with ``least_squares()``. Consider the\n",
      "        following function:\n",
      "        \n",
      "        >>> def f(z):\n",
      "        ...     return z - (0.5 + 0.5j)\n",
      "        \n",
      "        We wrap it into a function of real variables that returns real residuals\n",
      "        by simply handling the real and imaginary parts as independent variables:\n",
      "        \n",
      "        >>> def f_wrap(x):\n",
      "        ...     fx = f(x[0] + 1j*x[1])\n",
      "        ...     return np.array([fx.real, fx.imag])\n",
      "        \n",
      "        Thus, instead of the original m-D complex function of n complex\n",
      "        variables we optimize a 2m-D real function of 2n real variables:\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\n",
      "        >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\n",
      "        >>> z\n",
      "        (0.49999999999925893+0.49999999999925893j)\n",
      "    \n",
      "    leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
      "        Minimize the sum of squares of a set of equations.\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            x = arg min(sum(func(y)**2,axis=0))\n",
      "                     y\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Should take at least one (possibly length N vector) argument and\n",
      "            returns M floating point numbers. It must not return NaNs or\n",
      "            fitting might fail.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the minimization.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to func are placed in this tuple.\n",
      "        Dfun : callable, optional\n",
      "            A function or method to compute the Jacobian of func with derivatives\n",
      "            across the rows. If this is None, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            non-zero to return all optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            non-zero to specify that the Jacobian function computes derivatives\n",
      "            down the columns (faster, because there is no transpose operation).\n",
      "        ftol : float, optional\n",
      "            Relative error desired in the sum of squares.\n",
      "        xtol : float, optional\n",
      "            Relative error desired in the approximate solution.\n",
      "        gtol : float, optional\n",
      "            Orthogonality desired between the function vector and the columns of\n",
      "            the Jacobian.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If `Dfun` is provided,\n",
      "            then the default `maxfev` is 100*(N+1) where N is the number of elements\n",
      "            in x0, otherwise the default `maxfev` is 200*(N+1).\n",
      "        epsfcn : float, optional\n",
      "            A variable used in determining a suitable step length for the forward-\n",
      "            difference approximation of the Jacobian (for Dfun=None).\n",
      "            Normally the actual step length will be sqrt(epsfcn)*x\n",
      "            If epsfcn is less than the machine precision, it is assumed that the\n",
      "            relative errors are of the order of the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for an unsuccessful\n",
      "            call).\n",
      "        cov_x : ndarray\n",
      "            The inverse of the Hessian. `fjac` and `ipvt` are used to construct an\n",
      "            estimate of the Hessian. A value of None indicates a singular matrix,\n",
      "            which means the curvature in parameters `x` is numerically flat. To\n",
      "            obtain the covariance matrix of the parameters `x`, `cov_x` must be\n",
      "            multiplied by the variance of the residuals -- see curve_fit.\n",
      "        infodict : dict\n",
      "            a dictionary of optional outputs with the keys:\n",
      "        \n",
      "            ``nfev``\n",
      "                The number of function calls\n",
      "            ``fvec``\n",
      "                The function evaluated at the output\n",
      "            ``fjac``\n",
      "                A permutation of the R matrix of a QR\n",
      "                factorization of the final approximate\n",
      "                Jacobian matrix, stored column wise.\n",
      "                Together with ipvt, the covariance of the\n",
      "                estimate can be approximated.\n",
      "            ``ipvt``\n",
      "                An integer array of length N which defines\n",
      "                a permutation matrix, p, such that\n",
      "                fjac*p = q*r, where r is upper triangular\n",
      "                with diagonal elements of nonincreasing\n",
      "                magnitude. Column j of p is column ipvt(j)\n",
      "                of the identity matrix.\n",
      "            ``qtf``\n",
      "                The vector (transpose(q) * fvec).\n",
      "        \n",
      "        mesg : str\n",
      "            A string message giving information about the cause of failure.\n",
      "        ier : int\n",
      "            An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n",
      "            found. Otherwise, the solution was not found. In either case, the\n",
      "            optional output variable 'mesg' gives more information.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Newer interface to solve nonlinear least-squares problems\n",
      "            with bounds on the variables. See ``method=='lm'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
      "        \n",
      "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
      "        objective function.\n",
      "        This approximation assumes that the objective function is based on the\n",
      "        difference between some observed target data (ydata) and a (non-linear)\n",
      "        function of the parameters `f(xdata, params)` ::\n",
      "        \n",
      "               func(params) = ydata - f(xdata, params)\n",
      "        \n",
      "        so that the objective function is ::\n",
      "        \n",
      "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
      "             params\n",
      "        \n",
      "        The solution, `x`, is always a 1-D array, regardless of the shape of `x0`,\n",
      "        or whether `x0` is a scalar.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import leastsq\n",
      "        >>> def func(x):\n",
      "        ...     return 2*(x-3)**2+1\n",
      "        >>> leastsq(func, 0)\n",
      "        (array([2.99999999]), 1)\n",
      "    \n",
      "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=None, extra_condition=None, maxiter=10)\n",
      "        Find alpha that satisfies strong Wolfe conditions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function.\n",
      "        myfprime : callable f'(x,*args)\n",
      "            Objective function gradient.\n",
      "        xk : ndarray\n",
      "            Starting point.\n",
      "        pk : ndarray\n",
      "            Search direction.\n",
      "        gfk : ndarray, optional\n",
      "            Gradient value for x=xk (xk being the current parameter\n",
      "            estimate). Will be recomputed if omitted.\n",
      "        old_fval : float, optional\n",
      "            Function value for x=xk. Will be recomputed if omitted.\n",
      "        old_old_fval : float, optional\n",
      "            Function value for the point preceding x=xk.\n",
      "        args : tuple, optional\n",
      "            Additional arguments passed to objective function.\n",
      "        c1 : float, optional\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, optional\n",
      "            Parameter for curvature condition rule.\n",
      "        amax : float, optional\n",
      "            Maximum step size\n",
      "        extra_condition : callable, optional\n",
      "            A callable of the form ``extra_condition(alpha, x, f, g)``\n",
      "            returning a boolean. Arguments are the proposed step ``alpha``\n",
      "            and the corresponding ``x``, ``f`` and ``g`` values. The line search\n",
      "            accepts the value of ``alpha`` only if this\n",
      "            callable returns ``True``. If the callable returns ``False``\n",
      "            for the step length, the algorithm will continue with\n",
      "            new iterates. The callable is only called for iterates\n",
      "            satisfying the strong Wolfe conditions.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alpha : float or None\n",
      "            Alpha for which ``x_new = x0 + alpha * pk``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        fc : int\n",
      "            Number of function evaluations made.\n",
      "        gc : int\n",
      "            Number of gradient evaluations made.\n",
      "        new_fval : float or None\n",
      "            New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        old_fval : float\n",
      "            Old function value ``f(x0)``.\n",
      "        new_slope : float or None\n",
      "            The local slope along the search direction at the\n",
      "            new value ``<myfprime(x_new), pk>``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses the line search algorithm to enforce strong Wolfe\n",
      "        conditions. See Wright and Nocedal, 'Numerical Optimization',\n",
      "        1999, pp. 59-61.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import line_search\n",
      "        \n",
      "        A objective function and its gradient are defined.\n",
      "        \n",
      "        >>> def obj_func(x):\n",
      "        ...     return (x[0])**2+(x[1])**2\n",
      "        >>> def obj_grad(x):\n",
      "        ...     return [2*x[0], 2*x[1]]\n",
      "        \n",
      "        We can find alpha that satisfies strong Wolfe conditions.\n",
      "        \n",
      "        >>> start_point = np.array([1.8, 1.7])\n",
      "        >>> search_gradient = np.array([-1.0, -1.0])\n",
      "        >>> line_search(obj_func, obj_grad, start_point, search_gradient)\n",
      "        (1.0, 2, 1, 1.1300000000000001, 6.13, [1.6, 1.4])\n",
      "    \n",
      "    linear_sum_assignment(cost_matrix, maximize=False)\n",
      "        Solve the linear sum assignment problem.\n",
      "        \n",
      "        The linear sum assignment problem is also known as minimum weight matching\n",
      "        in bipartite graphs. A problem instance is described by a matrix C, where\n",
      "        each C[i,j] is the cost of matching vertex i of the first partite set\n",
      "        (a \"worker\") and vertex j of the second set (a \"job\"). The goal is to find\n",
      "        a complete assignment of workers to jobs of minimal cost.\n",
      "        \n",
      "        Formally, let X be a boolean matrix where :math:`X[i,j] = 1` iff row i is\n",
      "        assigned to column j. Then the optimal assignment has cost\n",
      "        \n",
      "        .. math::\n",
      "            \\min \\sum_i \\sum_j C_{i,j} X_{i,j}\n",
      "        \n",
      "        where, in the case where the matrix X is square, each row is assigned to\n",
      "        exactly one column, and each column to exactly one row.\n",
      "        \n",
      "        This function can also solve a generalization of the classic assignment\n",
      "        problem where the cost matrix is rectangular. If it has more rows than\n",
      "        columns, then not every row needs to be assigned to a column, and vice\n",
      "        versa.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cost_matrix : array\n",
      "            The cost matrix of the bipartite graph.\n",
      "        \n",
      "        maximize : bool (default: False)\n",
      "            Calculates a maximum weight matching if true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        row_ind, col_ind : array\n",
      "            An array of row indices and one of corresponding column indices giving\n",
      "            the optimal assignment. The cost of the assignment can be computed\n",
      "            as ``cost_matrix[row_ind, col_ind].sum()``. The row indices will be\n",
      "            sorted; in the case of a square cost matrix they will be equal to\n",
      "            ``numpy.arange(cost_matrix.shape[0])``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        1. https://en.wikipedia.org/wiki/Assignment_problem\n",
      "        \n",
      "        2. DF Crouse. On implementing 2D rectangular assignment algorithms.\n",
      "           *IEEE Transactions on Aerospace and Electronic Systems*,\n",
      "           52(4):1679-1696, August 2016, https://doi.org/10.1109/TAES.2016.140952\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cost = np.array([[4, 1, 3], [2, 0, 5], [3, 2, 2]])\n",
      "        >>> from scipy.optimize import linear_sum_assignment\n",
      "        >>> row_ind, col_ind = linear_sum_assignment(cost)\n",
      "        >>> col_ind\n",
      "        array([1, 0, 2])\n",
      "        >>> cost[row_ind, col_ind].sum()\n",
      "        5\n",
      "    \n",
      "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a scalar Jacobian approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            The Jacobian approximation is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='linearmixing'`` in particular.\n",
      "    \n",
      "    linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='interior-point', callback=None, options=None, x0=None)\n",
      "        Linear programming: minimize a linear objective function subject to linear\n",
      "        equality and inequality constraints.\n",
      "        \n",
      "        Linear programming solves problems of the following form:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_x \\ & c^T x \\\\\n",
      "            \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "            & A_{eq} x = b_{eq},\\\\\n",
      "            & l \\leq x \\leq u ,\n",
      "        \n",
      "        where :math:`x` is a vector of decision variables; :math:`c`,\n",
      "        :math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      "        :math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "        \n",
      "        Informally, that's:\n",
      "        \n",
      "        minimize::\n",
      "        \n",
      "            c @ x\n",
      "        \n",
      "        such that::\n",
      "        \n",
      "            A_ub @ x <= b_ub\n",
      "            A_eq @ x == b_eq\n",
      "            lb <= x <= ub\n",
      "        \n",
      "        Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "        ``bounds``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        c : 1-D array\n",
      "            The coefficients of the linear objective function to be minimized.\n",
      "        A_ub : 2-D array, optional\n",
      "            The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "            coefficients of a linear inequality constraint on ``x``.\n",
      "        b_ub : 1-D array, optional\n",
      "            The inequality constraint vector. Each element represents an\n",
      "            upper bound on the corresponding value of ``A_ub @ x``.\n",
      "        A_eq : 2-D array, optional\n",
      "            The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "            coefficients of a linear equality constraint on ``x``.\n",
      "        b_eq : 1-D array, optional\n",
      "            The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "            the corresponding element of ``b_eq``.\n",
      "        bounds : sequence, optional\n",
      "            A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the minimum and maximum values of that decision variable. Use ``None`` to\n",
      "            indicate that there is no bound. By default, bounds are ``(0, None)``\n",
      "            (all decision variables are non-negative).\n",
      "            If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "            ``max`` will serve as bounds for all decision variables.\n",
      "        method : {'interior-point', 'revised simplex', 'simplex'}, optional\n",
      "            The algorithm used to solve the standard form problem.\n",
      "            :ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      "            :ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      "            :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "            are supported.\n",
      "        callback : callable, optional\n",
      "            If a callback function is provided, it will be called at least once per\n",
      "            iteration of the algorithm. The callback function must accept a single\n",
      "            `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "        \n",
      "                x : 1-D array\n",
      "                    The current solution vector.\n",
      "                fun : float\n",
      "                    The current value of the objective function ``c @ x``.\n",
      "                success : bool\n",
      "                    ``True`` when the algorithm has completed successfully.\n",
      "                slack : 1-D array\n",
      "                    The (nominally positive) values of the slack,\n",
      "                    ``b_ub - A_ub @ x``.\n",
      "                con : 1-D array\n",
      "                    The (nominally zero) residuals of the equality constraints,\n",
      "                    ``b_eq - A_eq @ x``.\n",
      "                phase : int\n",
      "                    The phase of the algorithm being executed.\n",
      "                status : int\n",
      "                    An integer representing the status of the algorithm.\n",
      "        \n",
      "                    ``0`` : Optimization proceeding nominally.\n",
      "        \n",
      "                    ``1`` : Iteration limit reached.\n",
      "        \n",
      "                    ``2`` : Problem appears to be infeasible.\n",
      "        \n",
      "                    ``3`` : Problem appears to be unbounded.\n",
      "        \n",
      "                    ``4`` : Numerical difficulties encountered.\n",
      "        \n",
      "                nit : int\n",
      "                    The current iteration number.\n",
      "                message : str\n",
      "                    A string descriptor of the algorithm status.\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            options:\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform.\n",
      "                    Default: see method-specific documentation.\n",
      "                disp : bool\n",
      "                    Set to ``True`` to print convergence messages.\n",
      "                    Default: ``False``.\n",
      "                autoscale : bool\n",
      "                    Set to ``True`` to automatically perform equilibration.\n",
      "                    Consider using this option if the numerical values in the\n",
      "                    constraints are separated by several orders of magnitude.\n",
      "                    Default: ``False``.\n",
      "                presolve : bool\n",
      "                    Set to ``False`` to disable automatic presolve.\n",
      "                    Default: ``True``.\n",
      "                rr : bool\n",
      "                    Set to ``False`` to disable automatic redundancy removal.\n",
      "                    Default: ``True``.\n",
      "        \n",
      "            For method-specific options, see\n",
      "            :func:`show_options('linprog') <show_options>`.\n",
      "        \n",
      "        x0 : 1-D array, optional\n",
      "            Guess values of the decision variables, which will be refined by\n",
      "            the optimization algorithm. This argument is currently used only by the\n",
      "            'revised simplex' method, and can only be used if `x0` represents a\n",
      "            basic feasible solution.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "        \n",
      "                x : 1-D array\n",
      "                    The values of the decision variables that minimizes the\n",
      "                    objective function while satisfying the constraints.\n",
      "                fun : float\n",
      "                    The optimal value of the objective function ``c @ x``.\n",
      "                slack : 1-D array\n",
      "                    The (nominally positive) values of the slack variables,\n",
      "                    ``b_ub - A_ub @ x``.\n",
      "                con : 1-D array\n",
      "                    The (nominally zero) residuals of the equality constraints,\n",
      "                    ``b_eq - A_eq @ x``.\n",
      "                success : bool\n",
      "                    ``True`` when the algorithm succeeds in finding an optimal\n",
      "                    solution.\n",
      "                status : int\n",
      "                    An integer representing the exit status of the algorithm.\n",
      "        \n",
      "                    ``0`` : Optimization terminated successfully.\n",
      "        \n",
      "                    ``1`` : Iteration limit reached.\n",
      "        \n",
      "                    ``2`` : Problem appears to be infeasible.\n",
      "        \n",
      "                    ``3`` : Problem appears to be unbounded.\n",
      "        \n",
      "                    ``4`` : Numerical difficulties encountered.\n",
      "        \n",
      "                nit : int\n",
      "                    The total number of iterations performed in all phases.\n",
      "                message : str\n",
      "                    A string descriptor of the exit status of the algorithm.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "        \n",
      "        :ref:`'interior-point' <optimize.linprog-interior-point>` is the default\n",
      "        as it is typically the fastest and most robust method.\n",
      "        :ref:`'revised simplex' <optimize.linprog-revised_simplex>` is more\n",
      "        accurate for the problems it solves.\n",
      "        :ref:`'simplex' <optimize.linprog-simplex>` is the legacy method and is\n",
      "        included for backwards compatibility and educational purposes.\n",
      "        \n",
      "        Method *interior-point* uses the primal-dual path following algorithm\n",
      "        as outlined in [4]_. This algorithm supports sparse constraint matrices and\n",
      "        is typically faster than the simplex methods, especially for large, sparse\n",
      "        problems. Note, however, that the solution returned may be slightly less\n",
      "        accurate than those of the simplex methods and will not, in general,\n",
      "        correspond with a vertex of the polytope defined by the constraints.\n",
      "        \n",
      "        .. versionadded:: 1.0.0\n",
      "        \n",
      "        Method *revised simplex* uses the revised simplex method as described in\n",
      "        [9]_, except that a factorization [11]_ of the basis matrix, rather than\n",
      "        its inverse, is efficiently maintained and used to solve the linear systems\n",
      "        at each iteration of the algorithm.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Method *simplex* uses a traditional, full-tableau implementation of\n",
      "        Dantzig's simplex algorithm [1]_, [2]_ (*not* the\n",
      "        Nelder-Mead simplex). This algorithm is included for backwards\n",
      "        compatibility and educational purposes.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Before applying any method, a presolve procedure based on [8]_ attempts\n",
      "        to identify trivial infeasibilities, trivial unboundedness, and potential\n",
      "        problem simplifications. Specifically, it checks for:\n",
      "        \n",
      "        - rows of zeros in ``A_eq`` or ``A_ub``, representing trivial constraints;\n",
      "        - columns of zeros in ``A_eq`` `and` ``A_ub``, representing unconstrained\n",
      "          variables;\n",
      "        - column singletons in ``A_eq``, representing fixed variables; and\n",
      "        - column singletons in ``A_ub``, representing simple bounds.\n",
      "        \n",
      "        If presolve reveals that the problem is unbounded (e.g. an unconstrained\n",
      "        and unbounded variable has negative cost) or infeasible (e.g., a row of\n",
      "        zeros in ``A_eq`` corresponds with a nonzero in ``b_eq``), the solver\n",
      "        terminates with the appropriate status code. Note that presolve terminates\n",
      "        as soon as any sign of unboundedness is detected; consequently, a problem\n",
      "        may be reported as unbounded when in reality the problem is infeasible\n",
      "        (but infeasibility has not been detected yet). Therefore, if it is\n",
      "        important to know whether the problem is actually infeasible, solve the\n",
      "        problem again with option ``presolve=False``.\n",
      "        \n",
      "        If neither infeasibility nor unboundedness are detected in a single pass\n",
      "        of the presolve, bounds are tightened where possible and fixed\n",
      "        variables are removed from the problem. Then, linearly dependent rows\n",
      "        of the ``A_eq`` matrix are removed, (unless they represent an\n",
      "        infeasibility) to avoid numerical difficulties in the primary solve\n",
      "        routine. Note that rows that are nearly linearly dependent (within a\n",
      "        prescribed tolerance) may also be removed, which can change the optimal\n",
      "        solution in rare cases. If this is a concern, eliminate redundancy from\n",
      "        your problem formulation and run with option ``rr=False`` or\n",
      "        ``presolve=False``.\n",
      "        \n",
      "        Several potential improvements can be made here: additional presolve\n",
      "        checks outlined in [8]_ should be implemented, the presolve routine should\n",
      "        be run multiple times (until no further simplifications can be made), and\n",
      "        more of the efficiency improvements from [5]_ should be implemented in the\n",
      "        redundancy removal routines.\n",
      "        \n",
      "        After presolve, the problem is transformed to standard form by converting\n",
      "        the (tightened) simple bounds to upper bound constraints, introducing\n",
      "        non-negative slack variables for inequality constraints, and expressing\n",
      "        unbounded variables as the difference between two non-negative variables.\n",
      "        Optionally, the problem is automatically scaled via equilibration [12]_.\n",
      "        The selected algorithm solves the standard form problem, and a\n",
      "        postprocessing routine converts the result to a solution to the original\n",
      "        problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "               Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "               1963\n",
      "        .. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "               Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      "        .. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "               Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "        .. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "               optimizer for linear programming: an implementation of the\n",
      "               homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "               2000. 197-232.\n",
      "        .. [5] Andersen, Erling D. \"Finding all linearly dependent rows in\n",
      "               large-scale linear programming.\" Optimization Methods and Software\n",
      "               6.3 (1995): 219-227.\n",
      "        .. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "               Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "               March 2004. Available 2/25/2017 at\n",
      "               https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      "        .. [7] Fourer, Robert. \"Solving Linear Programs by Interior-Point Methods.\"\n",
      "               Unpublished Course Notes, August 26, 2005. Available 2/25/2017 at\n",
      "               http://www.4er.org/CourseNotes/Book%20B/B-III.pdf\n",
      "        .. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "               programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      "        .. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "               programming.\" Athena Scientific 1 (1997): 997.\n",
      "        .. [10] Andersen, Erling D., et al. Implementation of interior point\n",
      "                methods for large scale linear programming. HEC/Universite de\n",
      "                Geneve, 1996.\n",
      "        .. [11] Bartels, Richard H. \"A stabilization of the simplex method.\"\n",
      "                Journal in  Numerische Mathematik 16.5 (1971): 414-434.\n",
      "        .. [12] Tomlin, J. A. \"On scaling linear programming problems.\"\n",
      "                Mathematical Programming Study 4 (1975): 146-166.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the following problem:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_{x_0, x_1} \\ -x_0 + 4x_1 & \\\\\n",
      "            \\mbox{such that} \\ -3x_0 + x_1 & \\leq 6,\\\\\n",
      "            -x_0 - 2x_1 & \\geq -4,\\\\\n",
      "            x_1 & \\geq -3.\n",
      "        \n",
      "        The problem is not presented in the form accepted by `linprog`. This is\n",
      "        easily remedied by converting the \"greater than\" inequality\n",
      "        constraint to a \"less than\" inequality constraint by\n",
      "        multiplying both sides by a factor of :math:`-1`. Note also that the last\n",
      "        constraint is really the simple bound :math:`-3 \\leq x_1 \\leq \\infty`.\n",
      "        Finally, since there are no bounds on :math:`x_0`, we must explicitly\n",
      "        specify the bounds :math:`-\\infty \\leq x_0 \\leq \\infty`, as the\n",
      "        default is for variables to be non-negative. After collecting coeffecients\n",
      "        into arrays and tuples, the input for this problem is:\n",
      "        \n",
      "        >>> c = [-1, 4]\n",
      "        >>> A = [[-3, 1], [1, 2]]\n",
      "        >>> b = [6, 4]\n",
      "        >>> x0_bounds = (None, None)\n",
      "        >>> x1_bounds = (-3, None)\n",
      "        >>> from scipy.optimize import linprog\n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds])\n",
      "        \n",
      "        Note that the default method for `linprog` is 'interior-point', which is\n",
      "        approximate by nature.\n",
      "        \n",
      "        >>> print(res)\n",
      "             con: array([], dtype=float64)\n",
      "             fun: -21.99999984082494 # may vary\n",
      "         message: 'Optimization terminated successfully.'\n",
      "             nit: 6 # may vary\n",
      "           slack: array([3.89999997e+01, 8.46872439e-08] # may vary\n",
      "          status: 0\n",
      "         success: True\n",
      "               x: array([ 9.99999989, -2.99999999]) # may vary\n",
      "        \n",
      "        If you need greater accuracy, try 'revised simplex'.\n",
      "        \n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds], method='revised simplex')\n",
      "        >>> print(res)\n",
      "             con: array([], dtype=float64)\n",
      "             fun: -22.0 # may vary\n",
      "         message: 'Optimization terminated successfully.'\n",
      "             nit: 1 # may vary\n",
      "           slack: array([39.,  0.]) # may vary\n",
      "          status: 0\n",
      "         success: True\n",
      "               x: array([10., -3.]) # may vary\n",
      "    \n",
      "    linprog_verbose_callback(res)\n",
      "        A sample callback function demonstrating the linprog callback interface.\n",
      "        This callback produces detailed output to sys.stdout before each iteration\n",
      "        and after the final iteration of the simplex algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        res : A `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "        \n",
      "            x : 1-D array\n",
      "                The independent variable vector which optimizes the linear\n",
      "                programming problem.\n",
      "            fun : float\n",
      "                Value of the objective function.\n",
      "            success : bool\n",
      "                True if the algorithm succeeded in finding an optimal solution.\n",
      "            slack : 1-D array\n",
      "                The values of the slack variables. Each slack variable corresponds\n",
      "                to an inequality constraint. If the slack is zero, then the\n",
      "                corresponding constraint is active.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints, that is,\n",
      "                ``b - A_eq @ x``\n",
      "            phase : int\n",
      "                The phase of the optimization being executed. In phase 1 a basic\n",
      "                feasible solution is sought and the T has an additional row\n",
      "                representing an alternate objective function.\n",
      "            status : int\n",
      "                An integer representing the exit status of the optimization::\n",
      "        \n",
      "                     0 : Optimization terminated successfully\n",
      "                     1 : Iteration limit reached\n",
      "                     2 : Problem appears to be infeasible\n",
      "                     3 : Problem appears to be unbounded\n",
      "                     4 : Serious numerical difficulties encountered\n",
      "        \n",
      "            nit : int\n",
      "                The number of iterations performed.\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the optimization.\n",
      "    \n",
      "    lsq_linear(A, b, bounds=(-inf, inf), method='trf', tol=1e-10, lsq_solver=None, lsmr_tol=None, max_iter=None, verbose=0)\n",
      "        Solve a linear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given a m-by-n design matrix A and a target vector b with m elements,\n",
      "        `lsq_linear` solves the following optimization problem::\n",
      "        \n",
      "            minimize 0.5 * ||A x - b||**2\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        This optimization problem is convex, hence a found minimum (if iterations\n",
      "        have converged) is guaranteed to be global.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : array_like, sparse matrix of LinearOperator, shape (m, n)\n",
      "            Design matrix. Can be `scipy.sparse.linalg.LinearOperator`.\n",
      "        b : array_like, shape (m,)\n",
      "            Target vector.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must have shape (n,) or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : 'trf' or 'bvls', optional\n",
      "            Method to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm adapted for a linear\n",
      "                  least-squares problem. This is an interior-point-like method\n",
      "                  and the required number of iterations is weakly correlated with\n",
      "                  the number of variables.\n",
      "                * 'bvls' : Bounded-variable least-squares algorithm. This is\n",
      "                  an active set method, which requires the number of iterations\n",
      "                  comparable to the number of variables. Can't be used when `A` is\n",
      "                  sparse or LinearOperator.\n",
      "        \n",
      "            Default is 'trf'.\n",
      "        tol : float, optional\n",
      "            Tolerance parameter. The algorithm terminates if a relative change\n",
      "            of the cost function is less than `tol` on the last iteration.\n",
      "            Additionally, the first-order optimality measure is considered:\n",
      "        \n",
      "                * ``method='trf'`` terminates if the uniform norm of the gradient,\n",
      "                  scaled to account for the presence of the bounds, is less than\n",
      "                  `tol`.\n",
      "                * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n",
      "                  are satisfied within `tol` tolerance.\n",
      "        \n",
      "        lsq_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method of solving unbounded least-squares problems throughout\n",
      "            iterations:\n",
      "        \n",
      "                * 'exact' : Use dense QR or SVD decomposition approach. Can't be\n",
      "                  used when `A` is sparse or LinearOperator.\n",
      "                * 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n",
      "                  which requires only matrix-vector product evaluations. Can't\n",
      "                  be used with ``method='bvls'``.\n",
      "        \n",
      "            If None (default), the solver is chosen based on type of `A`.\n",
      "        lsmr_tol : None, float or 'auto', optional\n",
      "            Tolerance parameters 'atol' and 'btol' for `scipy.sparse.linalg.lsmr`\n",
      "            If None (default), it is set to ``1e-2 * tol``. If 'auto', the\n",
      "            tolerance will be adjusted based on the optimality of the current\n",
      "            iterate, which can speed up the optimization process, but is not always\n",
      "            reliable.\n",
      "        max_iter : None or int, optional\n",
      "            Maximum number of iterations before termination. If None (default), it\n",
      "            is set to 100 for ``method='trf'`` or to the number of variables for\n",
      "            ``method='bvls'`` (not counting iterations for 'bvls' initialization).\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 : work silently (default).\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        OptimizeResult with the following fields defined:\n",
      "        x : ndarray, shape (n,)\n",
      "            Solution found.\n",
      "        cost : float\n",
      "            Value of the cost function at the solution.\n",
      "        fun : ndarray, shape (m,)\n",
      "            Vector of residuals at the solution.\n",
      "        optimality : float\n",
      "            First-order optimality measure. The exact meaning depends on `method`,\n",
      "            refer to the description of `tol` parameter.\n",
      "        active_mask : ndarray of int, shape (n,)\n",
      "            Each component shows whether a corresponding constraint is active\n",
      "            (that is, whether a variable is at the bound):\n",
      "        \n",
      "                *  0 : a constraint is not active.\n",
      "                * -1 : a lower bound is active.\n",
      "                *  1 : an upper bound is active.\n",
      "        \n",
      "            Might be somewhat arbitrary for the `trf` method as it generates a\n",
      "            sequence of strictly feasible iterates and active_mask is determined\n",
      "            within a tolerance threshold.\n",
      "        nit : int\n",
      "            Number of iterations. Zero if the unconstrained solution is optimal.\n",
      "        status : int\n",
      "            Reason for algorithm termination:\n",
      "        \n",
      "                * -1 : the algorithm was not able to make progress on the last\n",
      "                  iteration.\n",
      "                *  0 : the maximum number of iterations is exceeded.\n",
      "                *  1 : the first-order optimality measure is less than `tol`.\n",
      "                *  2 : the relative change of the cost function is less than `tol`.\n",
      "                *  3 : the unconstrained solution is optimal.\n",
      "        \n",
      "        message : str\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nnls : Linear least squares with non-negativity constraint.\n",
      "        least_squares : Nonlinear least squares with bounds on the variables.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm first computes the unconstrained least-squares solution by\n",
      "        `numpy.linalg.lstsq` or `scipy.sparse.linalg.lsmr` depending on\n",
      "        `lsq_solver`. This solution is returned as optimal if it lies within the\n",
      "        bounds.\n",
      "        \n",
      "        Method 'trf' runs the adaptation of the algorithm described in [STIR]_ for\n",
      "        a linear least-squares problem. The iterations are essentially the same as\n",
      "        in the nonlinear least-squares algorithm, but as the quadratic function\n",
      "        model is always accurate, we don't need to track or modify the radius of\n",
      "        a trust region. The line search (backtracking) is used as a safety net\n",
      "        when a selected step does not decrease the cost function. Read more\n",
      "        detailed description of the algorithm in `scipy.optimize.least_squares`.\n",
      "        \n",
      "        Method 'bvls' runs a Python implementation of the algorithm described in\n",
      "        [BVLS]_. The algorithm maintains active and free sets of variables, on\n",
      "        each iteration chooses a new variable to move from the active set to the\n",
      "        free set and then solves the unconstrained least-squares problem on free\n",
      "        variables. This algorithm is guaranteed to give an accurate solution\n",
      "        eventually, but may require up to n iterations for a problem with n\n",
      "        variables. Additionally, an ad-hoc initialization procedure is\n",
      "        implemented, that determines which variables to set free or active\n",
      "        initially. It takes some number of iterations before actual BVLS starts,\n",
      "        but can significantly reduce the number of further iterations.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [BVLS] P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:\n",
      "                  an Algorithm and Applications\", Computational Statistics, 10,\n",
      "                  129-141, 1995.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example, a problem with a large sparse matrix and bounds on the\n",
      "        variables is solved.\n",
      "        \n",
      "        >>> from scipy.sparse import rand\n",
      "        >>> from scipy.optimize import lsq_linear\n",
      "        ...\n",
      "        >>> np.random.seed(0)\n",
      "        ...\n",
      "        >>> m = 20000\n",
      "        >>> n = 10000\n",
      "        ...\n",
      "        >>> A = rand(m, n, density=1e-4)\n",
      "        >>> b = np.random.randn(m)\n",
      "        ...\n",
      "        >>> lb = np.random.randn(n)\n",
      "        >>> ub = lb + 1\n",
      "        ...\n",
      "        >>> res = lsq_linear(A, b, bounds=(lb, ub), lsmr_tol='auto', verbose=1)\n",
      "        # may vary\n",
      "        The relative change of the cost function is less than `tol`.\n",
      "        Number of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\n",
      "        first-order optimality 4.66e-08.\n",
      "    \n",
      "    minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "        Minimization of scalar function of one or more variables.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            The objective function to be minimized.\n",
      "        \n",
      "                ``fun(x, *args) -> float``\n",
      "        \n",
      "            where ``x`` is an 1-D array with shape (n,) and ``args``\n",
      "            is a tuple of the fixed parameters needed to completely\n",
      "            specify the function.\n",
      "        x0 : ndarray, shape (n,)\n",
      "            Initial guess. Array of real elements of size (n,),\n",
      "            where 'n' is the number of independent variables.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its\n",
      "            derivatives (`fun`, `jac` and `hess` functions).\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "                - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "                - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "                - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "                - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "                - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "                - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "                - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "                - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "                - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "                - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "                - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "                - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "                - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "                - custom - a callable object (added in version 0.14.0),\n",
      "                  see below for description.\n",
      "        \n",
      "            If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "            depending if the problem has constraints or bounds.\n",
      "        jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "            Method for computing the gradient vector. Only for CG, BFGS,\n",
      "            Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "            trust-exact and trust-constr.\n",
      "            If it is a callable, it should be a function that returns the gradient\n",
      "            vector:\n",
      "        \n",
      "                ``jac(x, *args) -> array_like, shape (n,)``\n",
      "        \n",
      "            where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
      "            the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
      "            assumed to return and objective and gradient as and ``(f, g)`` tuple.\n",
      "            Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
      "            'trust-krylov' require that either a callable be supplied, or that\n",
      "            `fun` return the objective and gradient.\n",
      "            If None or False, the gradient will be estimated using 2-point finite\n",
      "            difference estimation with an absolute step size.\n",
      "            Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
      "            to select a finite difference scheme for numerical estimation of the\n",
      "            gradient with a relative step size. These finite difference schemes\n",
      "            obey any specified `bounds`.\n",
      "        hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
      "            Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "            trust-ncg,  trust-krylov, trust-exact and trust-constr. If it is\n",
      "            callable, it should return the  Hessian matrix:\n",
      "        \n",
      "                ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "        \n",
      "            where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
      "            parameters. LinearOperator and sparse matrix returns are\n",
      "            allowed only for 'trust-constr' method. Alternatively, the keywords\n",
      "            {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
      "            for numerical estimation. Or, objects implementing\n",
      "            `HessianUpdateStrategy` interface can be used to approximate\n",
      "            the Hessian. Available quasi-Newton methods implementing\n",
      "            this interface are:\n",
      "        \n",
      "                - `BFGS`;\n",
      "                - `SR1`.\n",
      "        \n",
      "            Whenever the gradient is estimated via finite-differences,\n",
      "            the Hessian cannot be estimated with options\n",
      "            {'2-point', '3-point', 'cs'} and needs to be\n",
      "            estimated using one of the quasi-Newton strategies.\n",
      "            Finite-difference options {'2-point', '3-point', 'cs'} and\n",
      "            `HessianUpdateStrategy` are available only for 'trust-constr' method.\n",
      "        hessp : callable, optional\n",
      "            Hessian of objective function times an arbitrary vector p. Only for\n",
      "            Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "            Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "            provided, then `hessp` will be ignored.  `hessp` must compute the\n",
      "            Hessian times an arbitrary vector:\n",
      "        \n",
      "                ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "        \n",
      "            where x is a (n,) ndarray, p is an arbitrary vector with\n",
      "            dimension (n,) and `args` is a tuple with the fixed\n",
      "            parameters.\n",
      "        bounds : sequence or `Bounds`, optional\n",
      "            Bounds on variables for L-BFGS-B, TNC, SLSQP, Powell, and\n",
      "            trust-constr methods. There are two ways to specify the bounds:\n",
      "        \n",
      "                1. Instance of `Bounds` class.\n",
      "                2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "                   is used to specify no bound.\n",
      "        \n",
      "        constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "            Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
      "            Constraints for 'trust-constr' are defined as a single object or a\n",
      "            list of objects specifying constraints to the optimization problem.\n",
      "            Available constraints are:\n",
      "        \n",
      "                - `LinearConstraint`\n",
      "                - `NonlinearConstraint`\n",
      "        \n",
      "            Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "            Each dictionary with fields:\n",
      "        \n",
      "                type : str\n",
      "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "                fun : callable\n",
      "                    The function defining the constraint.\n",
      "                jac : callable, optional\n",
      "                    The Jacobian of `fun` (only for SLSQP).\n",
      "                args : sequence, optional\n",
      "                    Extra arguments to be passed to the function and Jacobian.\n",
      "        \n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "            Note that COBYLA only supports inequality constraints.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            generic options:\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform. Depending on the\n",
      "                    method each iteration may use several function evaluations.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            For method-specific options, see :func:`show_options()`.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration. For 'trust-constr' it is a callable with\n",
      "            the signature:\n",
      "        \n",
      "                ``callback(xk, OptimizeResult state) -> bool``\n",
      "        \n",
      "            where ``xk`` is the current parameter vector. and ``state``\n",
      "            is an `OptimizeResult` object, with the same fields\n",
      "            as the ones from the return. If callback returns True\n",
      "            the algorithm execution is terminated.\n",
      "            For all the other methods, the signature is:\n",
      "        \n",
      "                ``callback(xk)``\n",
      "        \n",
      "            where ``xk`` is the current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar : Interface to minimization algorithms for scalar\n",
      "            univariate functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *BFGS*.\n",
      "        \n",
      "        **Unconstrained minimization**\n",
      "        \n",
      "        Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "        Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "        applications. However, if numerical computation of derivative can be\n",
      "        trusted, other algorithms using the first and/or second derivatives\n",
      "        information might be preferred for their better performance in\n",
      "        general.\n",
      "        \n",
      "        Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "        gradient algorithm by Polak and Ribiere, a variant of the\n",
      "        Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
      "        first derivatives are used.\n",
      "        \n",
      "        Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "        method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "        pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "        performance even for non-smooth optimizations. This method also\n",
      "        returns an approximation of the Hessian inverse, stored as\n",
      "        `hess_inv` in the OptimizeResult object.\n",
      "        \n",
      "        Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "        Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "        Newton method). It uses a CG method to the compute the search\n",
      "        direction. See also *TNC* method for a box-constrained\n",
      "        minimization with a similar algorithm. Suitable for large-scale\n",
      "        problems.\n",
      "        \n",
      "        Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "        trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "        algorithm requires the gradient and Hessian; furthermore the\n",
      "        Hessian is required to be positive definite.\n",
      "        \n",
      "        Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "        Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "        unconstrained minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "        \n",
      "        Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "        the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "        minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "        On indefinite problems it requires usually less iterations than the\n",
      "        `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "        \n",
      "        Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "        is a trust-region method for unconstrained minimization in which\n",
      "        quadratic subproblems are solved almost exactly [13]_. This\n",
      "        algorithm requires the gradient and the Hessian (which is\n",
      "        *not* required to be positive definite). It is, in many\n",
      "        situations, the Newton method to converge in fewer iteraction\n",
      "        and the most recommended for small and medium-size problems.\n",
      "        \n",
      "        **Bound-Constrained minimization**\n",
      "        \n",
      "        Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "        algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "        \n",
      "        Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "        of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "        method. It performs sequential one-dimensional minimizations along\n",
      "        each vector of the directions set (`direc` field in `options` and\n",
      "        `info`), which is updated at each iteration of the main\n",
      "        minimization loop. The function need not be differentiable, and no\n",
      "        derivatives are taken. If bounds are not provided, then an\n",
      "        unbounded line search will be used. If bounds are provided and\n",
      "        the initial guess is within the bounds, then every function\n",
      "        evaluation throughout the minimization procedure will be within\n",
      "        the bounds. If bounds are provided, the initial guess is outside\n",
      "        the bounds, and `direc` is full rank (default has full rank), then\n",
      "        some function evaluations during the first iteration may be\n",
      "        outside the bounds, but every function evaluation after the first\n",
      "        iteration will be within the bounds. If `direc` is not full rank,\n",
      "        then some parameters may not be optimized and the solution is not\n",
      "        guaranteed to be within the bounds.\n",
      "        \n",
      "        Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "        algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "        to bounds. This algorithm uses gradient information; it is also\n",
      "        called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "        method described above as it wraps a C implementation and allows\n",
      "        each variable to be given upper and lower bounds.\n",
      "        \n",
      "        **Constrained Minimization**\n",
      "        \n",
      "        Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "        Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "        [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "        approximations to the objective function and each constraint. The\n",
      "        method wraps a FORTRAN implementation of the algorithm. The\n",
      "        constraints functions 'fun' may return either a single number\n",
      "        or an array or list of numbers.\n",
      "        \n",
      "        Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "        Least SQuares Programming to minimize a function of several\n",
      "        variables with any combination of bounds, equality and inequality\n",
      "        constraints. The method wraps the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft [12]_. Note that the\n",
      "        wrapper handles infinite values in bounds by converting them into\n",
      "        large floating values.\n",
      "        \n",
      "        Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "        trust-region algorithm for constrained optimization. It swiches\n",
      "        between two implementations depending on the problem definition.\n",
      "        It is the most versatile constrained minimization algorithm\n",
      "        implemented in SciPy and the most appropriate for large-scale problems.\n",
      "        For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "        Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "        inequality constraints  are imposed as well, it swiches to the trust-region\n",
      "        interior point  method described in [16]_. This interior point algorithm,\n",
      "        in turn, solves inequality constraints by introducing slack variables\n",
      "        and solving a sequence of equality-constrained barrier problems\n",
      "        for progressively smaller values of the barrier parameter.\n",
      "        The previously described equality constrained SQP method is\n",
      "        used to solve the subproblems with increasing levels of accuracy\n",
      "        as the iterate gets closer to a solution.\n",
      "        \n",
      "        **Finite-Difference Options**\n",
      "        \n",
      "        For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "        the gradient and the Hessian may be approximated using\n",
      "        three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "        The scheme 'cs' is, potentially, the most accurate but it\n",
      "        requires the function to correctly handles complex inputs and to\n",
      "        be differentiable in the complex plane. The scheme '3-point' is more\n",
      "        accurate than '2-point' but requires twice as many operations.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "        or a different library.  You can simply pass a callable as the ``method``\n",
      "        parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "        `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "        `fun` returns just the function values and `jac` is converted to a function\n",
      "        returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "        object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method.  You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "            Minimization. The Computer Journal 7: 308-13.\n",
      "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "            191-208.\n",
      "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "           a function of several variables without calculating derivatives. The\n",
      "           Computer Journal 7: 155-162.\n",
      "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "           Numerical Recipes (any edition), Cambridge University Press.\n",
      "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "           Springer New York.\n",
      "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "           550-560.\n",
      "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "        .. [9] Powell, M J D. A direct search optimization method that models\n",
      "           the objective and constraint functions by linear interpolation.\n",
      "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "           calculations. 1998. Acta Numerica 7: 287-336.\n",
      "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "           2007/NA03\n",
      "        .. [12] Kraft, D. A software package for sequential quadratic\n",
      "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "        .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "           Trust region methods. 2000. Siam. pp. 169-200.\n",
      "        .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "           implementation of the GLTR method for iterative solution of\n",
      "           the trust region problem\", https://arxiv.org/abs/1611.04718\n",
      "        .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "           Trust-Region Subproblem using the Lanczos Method\",\n",
      "           SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "        .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "            An interior point algorithm for large-scale nonlinear  programming.\n",
      "            SIAM Journal on Optimization 9.4: 877-900.\n",
      "        .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "            implementation of an algorithm for large-scale equality constrained\n",
      "            optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function (and its respective derivatives) is implemented in `rosen`\n",
      "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "        \n",
      "        A simple application of the *Nelder-Mead* method is:\n",
      "        \n",
      "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "        >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        \n",
      "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "        options:\n",
      "        \n",
      "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "        ...                options={'gtol': 1e-6, 'disp': True})\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 26\n",
      "                 Function evaluations: 31\n",
      "                 Gradient evaluations: 31\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        >>> print(res.message)\n",
      "        Optimization terminated successfully.\n",
      "        >>> res.hess_inv\n",
      "        array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "               [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "               [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "               [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "               [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "        \n",
      "        \n",
      "        Next, consider a minimization problem with several constraints (namely\n",
      "        Example 16.4 from [5]_). The objective function is:\n",
      "        \n",
      "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "        \n",
      "        There are three constraints defined as:\n",
      "        \n",
      "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "        \n",
      "        And variables must be positive, hence the following bounds:\n",
      "        \n",
      "        >>> bnds = ((0, None), (0, None))\n",
      "        \n",
      "        The optimization problem is solved using the SLSQP method as:\n",
      "        \n",
      "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "        ...                constraints=cons)\n",
      "        \n",
      "        It should converge to the theoretical solution (1.4 ,1.7).\n",
      "    \n",
      "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)\n",
      "        Minimization of scalar function of one variable.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Objective function.\n",
      "            Scalar function, must return a scalar.\n",
      "        bracket : sequence, optional\n",
      "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
      "            interval and can either have three items ``(a, b, c)`` so that\n",
      "            ``a < b < c`` and ``fun(b) < fun(a), fun(c)`` or two items ``a`` and\n",
      "            ``c`` which are assumed to be a starting interval for a downhill\n",
      "            bracket search (see `bracket`); it doesn't always mean that the\n",
      "            obtained solution will satisfy ``a <= x <= c``.\n",
      "        bounds : sequence, optional\n",
      "            For method 'bounded', `bounds` is mandatory and must have two items\n",
      "            corresponding to the optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function.\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of:\n",
      "        \n",
      "                - 'Brent'     :ref:`(see here) <optimize.minimize_scalar-brent>`\n",
      "                - 'Bounded'   :ref:`(see here) <optimize.minimize_scalar-bounded>`\n",
      "                - 'Golden'    :ref:`(see here) <optimize.minimize_scalar-golden>`\n",
      "                - custom - a callable object (added in version 0.14.0), see below\n",
      "        \n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options.\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            See :func:`show_options()` for solver-specific options.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize : Interface to minimization algorithms for scalar multivariate\n",
      "            functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *Brent*.\n",
      "        \n",
      "        Method :ref:`Brent <optimize.minimize_scalar-brent>` uses Brent's\n",
      "        algorithm to find a local minimum.  The algorithm uses inverse\n",
      "        parabolic interpolation when possible to speed up convergence of\n",
      "        the golden section method.\n",
      "        \n",
      "        Method :ref:`Golden <optimize.minimize_scalar-golden>` uses the\n",
      "        golden section search technique. It uses analog of the bisection\n",
      "        method to decrease the bracketed interval. It is usually\n",
      "        preferable to use the *Brent* method.\n",
      "        \n",
      "        Method :ref:`Bounded <optimize.minimize_scalar-bounded>` can\n",
      "        perform bounded minimization. It uses the Brent method to find a\n",
      "        local minimum in the interval x1 < xopt < x2.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using some library frontend to minimize_scalar. You can simply\n",
      "        pass a callable as the ``method`` parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `bracket`, `tol`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  The method\n",
      "        shall return an `OptimizeResult` object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method. You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the problem of minimizing the following function.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x - 2) * x * (x + 2)**2\n",
      "        \n",
      "        Using the *Brent* method, we find the local minimum as:\n",
      "        \n",
      "        >>> from scipy.optimize import minimize_scalar\n",
      "        >>> res = minimize_scalar(f)\n",
      "        >>> res.x\n",
      "        1.28077640403\n",
      "        \n",
      "        Using the *Bounded* method, we find a local minimum with specified\n",
      "        bounds as:\n",
      "        \n",
      "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
      "        >>> res.x\n",
      "        -2.0000002026\n",
      "    \n",
      "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None, x1=None, rtol=0.0, full_output=False, disp=True)\n",
      "        Find a zero of a real or complex function using the Newton-Raphson\n",
      "        (or secant or Halley's) method.\n",
      "        \n",
      "        Find a zero of the function `func` given a nearby starting point `x0`.\n",
      "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
      "        is provided, otherwise the secant method is used. If the second order\n",
      "        derivative `fprime2` of `func` is also provided, then Halley's method is\n",
      "        used.\n",
      "        \n",
      "        If `x0` is a sequence with more than one item, then `newton` returns an\n",
      "        array, and `func` must be vectorized and return a sequence or array of the\n",
      "        same shape as its first argument. If `fprime` or `fprime2` is given, then\n",
      "        its return must also have the same shape.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The function whose zero is wanted. It must be a function of a\n",
      "            single variable of the form ``f(x,a,b,c...)``, where ``a,b,c...``\n",
      "            are extra arguments that can be passed in the `args` parameter.\n",
      "        x0 : float, sequence, or ndarray\n",
      "            An initial estimate of the zero that should be somewhere near the\n",
      "            actual zero. If not scalar, then `func` must be vectorized and return\n",
      "            a sequence or array of the same shape as its first argument.\n",
      "        fprime : callable, optional\n",
      "            The derivative of the function when available and convenient. If it\n",
      "            is None (default), then the secant method is used.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to be used in the function call.\n",
      "        tol : float, optional\n",
      "            The allowable error of the zero value. If `func` is complex-valued,\n",
      "            a larger `tol` is recommended as both the real and imaginary parts\n",
      "            of `x` contribute to ``|x - x0|``.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        fprime2 : callable, optional\n",
      "            The second order derivative of the function when available and\n",
      "            convenient. If it is None (default), then the normal Newton-Raphson\n",
      "            or the secant method is used. If it is not None, then Halley's method\n",
      "            is used.\n",
      "        x1 : float, optional\n",
      "            Another estimate of the zero that should be somewhere near the\n",
      "            actual zero. Used if `fprime` is not provided.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False (default), the root is returned.\n",
      "            If True and `x0` is scalar, the return value is ``(x, r)``, where ``x``\n",
      "            is the root and ``r`` is a `RootResults` object.\n",
      "            If True and `x0` is non-scalar, the return value is ``(x, converged,\n",
      "            zero_der)`` (see Returns section for details).\n",
      "        disp : bool, optional\n",
      "            If True, raise a RuntimeError if the algorithm didn't converge, with\n",
      "            the error message containing the number of iterations and current\n",
      "            function value. Otherwise, the convergence status is recorded in a\n",
      "            `RootResults` return object.\n",
      "            Ignored if `x0` is not scalar.\n",
      "            *Note: this has little to do with displaying, however,\n",
      "            the `disp` keyword cannot be renamed for backwards compatibility.*\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        root : float, sequence, or ndarray\n",
      "            Estimated location where function is zero.\n",
      "        r : `RootResults`, optional\n",
      "            Present if ``full_output=True`` and `x0` is scalar.\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        converged : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements converged successfully.\n",
      "        zero_der : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements had a zero derivative.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect\n",
      "        fsolve : find zeros in N dimensions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The convergence rate of the Newton-Raphson method is quadratic,\n",
      "        the Halley method is cubic, and the secant method is\n",
      "        sub-quadratic. This means that if the function is well-behaved\n",
      "        the actual error in the estimated zero after the nth iteration\n",
      "        is approximately the square (cube for Halley) of the error\n",
      "        after the (n-1)th step. However, the stopping criterion used\n",
      "        here is the step size and there is no guarantee that a zero\n",
      "        has been found. Consequently, the result should be verified.\n",
      "        Safer algorithms are brentq, brenth, ridder, and bisect,\n",
      "        but they all require that the root first be bracketed in an\n",
      "        interval where the function changes sign. The brentq algorithm\n",
      "        is recommended for general use in one dimensional problems\n",
      "        when such an interval has been found.\n",
      "        \n",
      "        When `newton` is used with arrays, it is best suited for the following\n",
      "        types of problems:\n",
      "        \n",
      "        * The initial guesses, `x0`, are all relatively the same distance from\n",
      "          the roots.\n",
      "        * Some or all of the extra arguments, `args`, are also arrays so that a\n",
      "          class of similar problems can be solved together.\n",
      "        * The size of the initial guesses, `x0`, is larger than O(100) elements.\n",
      "          Otherwise, a naive loop may perform as well or better than a vector.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        ``fprime`` is not provided, use the secant method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "        >>> root = optimize.newton(f, 1.5, fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "        \n",
      "        Only ``fprime`` is provided, use the Newton-Raphson method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        Both ``fprime2`` and ``fprime`` are provided, use Halley's method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2,\n",
      "        ...                        fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        When we want to find zeros for a set of related starting values and/or\n",
      "        function parameters, we can provide both of those as an array of inputs:\n",
      "        \n",
      "        >>> f = lambda x, a: x**3 - a\n",
      "        >>> fder = lambda x, a: 3 * x**2\n",
      "        >>> np.random.seed(4321)\n",
      "        >>> x = np.random.randn(100)\n",
      "        >>> a = np.arange(-50, 50)\n",
      "        >>> vec_res = optimize.newton(f, x, fprime=fder, args=(a, ))\n",
      "        \n",
      "        The above is the equivalent of solving for each value in ``(x, a)``\n",
      "        separately in a for-loop, just faster:\n",
      "        \n",
      "        >>> loop_res = [optimize.newton(f, x0, fprime=fder, args=(a0,))\n",
      "        ...             for x0, a0 in zip(x, a)]\n",
      "        >>> np.allclose(vec_res, loop_res)\n",
      "        True\n",
      "        \n",
      "        Plot the results found for all values of ``a``:\n",
      "        \n",
      "        >>> analytical_result = np.sign(a) * np.abs(a)**(1/3)\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(a, analytical_result, 'o')\n",
      "        >>> ax.plot(a, vec_res, '.')\n",
      "        >>> ax.set_xlabel('$a$')\n",
      "        >>> ax.set_ylabel('$x$ where $f(x, a)=0$')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
      "        \n",
      "        This method is suitable for solving large-scale problems.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        rdiff : float, optional\n",
      "            Relative step size to use in numerical differentiation.\n",
      "        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
      "            Krylov method to use to approximate the Jacobian.\n",
      "            Can be a string, or a function implementing the same interface as\n",
      "            the iterative solvers in `scipy.sparse.linalg`.\n",
      "        \n",
      "            The default is `scipy.sparse.linalg.lgmres`.\n",
      "        inner_maxiter : int, optional\n",
      "            Parameter to pass to the \"inner\" Krylov solver: maximum number of\n",
      "            iterations. Iteration will stop after maxiter steps even if the\n",
      "            specified tolerance has not been achieved.\n",
      "        inner_M : LinearOperator or InverseJacobian\n",
      "            Preconditioner for the inner Krylov iteration.\n",
      "            Note that you can use also inverse Jacobians as (adaptive)\n",
      "            preconditioners. For example,\n",
      "        \n",
      "            >>> from scipy.optimize.nonlin import BroydenFirst, KrylovJacobian\n",
      "            >>> from scipy.optimize.nonlin import InverseJacobian\n",
      "            >>> jac = BroydenFirst()\n",
      "            >>> kjac = KrylovJacobian(inner_M=InverseJacobian(jac))\n",
      "        \n",
      "            If the preconditioner has a method named 'update', it will be called\n",
      "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
      "            the current point, and ``f`` the current function value.\n",
      "        outer_k : int, optional\n",
      "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
      "            See `scipy.sparse.linalg.lgmres` for details.\n",
      "        inner_kwargs : kwargs\n",
      "            Keyword parameters for the \"inner\" Krylov solver\n",
      "            (defined with `method`). Parameter names must start with\n",
      "            the `inner_` prefix which will be stripped before passing on\n",
      "            the inner method. See, e.g., `scipy.sparse.linalg.gmres` for details.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='krylov'`` in particular.\n",
      "        scipy.sparse.linalg.gmres\n",
      "        scipy.sparse.linalg.lgmres\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements a Newton-Krylov solver. The basic idea is\n",
      "        to compute the inverse of the Jacobian with an iterative Krylov\n",
      "        method. These methods require only evaluating the Jacobian-vector\n",
      "        products, which are conveniently approximated by a finite difference:\n",
      "        \n",
      "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
      "        \n",
      "        Due to the use of iterative matrix inverses, these methods can\n",
      "        deal with large nonlinear problems.\n",
      "        \n",
      "        SciPy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
      "        solvers to choose from. The default here is `lgmres`, which is a\n",
      "        variant of restarted GMRES iteration that reuses some of the\n",
      "        information obtained in the previous Newton steps to invert\n",
      "        Jacobians in subsequent steps.\n",
      "        \n",
      "        For a review on Newton-Krylov methods, see for example [1]_,\n",
      "        and for the LGMRES sparse inverse method, see [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2004).\n",
      "               :doi:`10.1016/j.jcp.2003.08.010`\n",
      "        .. [2] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
      "               SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
      "               :doi:`10.1137/S0895479803422014`\n",
      "    \n",
      "    nnls(A, b, maxiter=None)\n",
      "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``. This is a wrapper\n",
      "        for a FORTRAN non-negative least squares solver.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : ndarray\n",
      "            Matrix ``A`` as shown above.\n",
      "        b : ndarray\n",
      "            Right-hand side vector.\n",
      "        maxiter: int, optional\n",
      "            Maximum number of iterations, optional.\n",
      "            Default is ``3 * A.shape[1]``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Solution vector.\n",
      "        rnorm : float\n",
      "            The residual, ``|| Ax-b ||_2``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lsq_linear : Linear least squares with bounds on the variables\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The FORTRAN code was published in the book below. The algorithm\n",
      "        is an active set method. It solves the KKT (Karush-Kuhn-Tucker)\n",
      "        conditions for the non-negative least squares problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Lawson C., Hanson R.J., (1987) Solving Least Squares Problems, SIAM\n",
      "        \n",
      "         Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import nnls\n",
      "        ...\n",
      "        >>> A = np.array([[1, 0], [1, 0], [0, 1]])\n",
      "        >>> b = np.array([2, 1, 1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([1.5, 1. ]), 0.7071067811865475)\n",
      "        \n",
      "        >>> b = np.array([-1, -1, -1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([0., 0.]), 1.7320508075688772)\n",
      "    \n",
      "    ridder(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in an interval using Ridder's method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.\n",
      "            In particular, ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton : 1-D root-finding\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses [Ridders1979]_ method to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
      "        generally as fast as the Brent routines. [Ridders1979]_ provides the\n",
      "        classic description and source of the algorithm. A description can also be\n",
      "        found in any recent edition of Numerical Recipes.\n",
      "        \n",
      "        The routine used here diverges slightly from standard presentations in\n",
      "        order to be a bit more careful of tolerance.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ridders1979]\n",
      "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
      "           Single Root of a Real Continuous Function.\"\n",
      "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.ridder(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        >>> root = optimize.ridder(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "    \n",
      "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
      "        Find a root of a vector function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            A vector function to find a root of.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its Jacobian.\n",
      "        method : str, optional\n",
      "            Type of solver. Should be one of\n",
      "        \n",
      "                - 'hybr'             :ref:`(see here) <optimize.root-hybr>`\n",
      "                - 'lm'               :ref:`(see here) <optimize.root-lm>`\n",
      "                - 'broyden1'         :ref:`(see here) <optimize.root-broyden1>`\n",
      "                - 'broyden2'         :ref:`(see here) <optimize.root-broyden2>`\n",
      "                - 'anderson'         :ref:`(see here) <optimize.root-anderson>`\n",
      "                - 'linearmixing'     :ref:`(see here) <optimize.root-linearmixing>`\n",
      "                - 'diagbroyden'      :ref:`(see here) <optimize.root-diagbroyden>`\n",
      "                - 'excitingmixing'   :ref:`(see here) <optimize.root-excitingmixing>`\n",
      "                - 'krylov'           :ref:`(see here) <optimize.root-krylov>`\n",
      "                - 'df-sane'          :ref:`(see here) <optimize.root-dfsane>`\n",
      "        \n",
      "        jac : bool or callable, optional\n",
      "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "            value of Jacobian along with the objective function. If False, the\n",
      "            Jacobian will be estimated numerically.\n",
      "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
      "            this case, it must accept the same arguments as `fun`.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g., `xtol` or `maxiter`, see\n",
      "            :obj:`show_options()` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : OptimizeResult\n",
      "            The solution represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the algorithm exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *hybr*.\n",
      "        \n",
      "        Method *hybr* uses a modification of the Powell hybrid method as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *lm* solves the system of nonlinear equations in a least squares\n",
      "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *df-sane* is a derivative-free spectral method. [3]_\n",
      "        \n",
      "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
      "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
      "        with backtracking or full line searches [2]_. Each method corresponds\n",
      "        to a particular Jacobian approximations. See `nonlin` for details.\n",
      "        \n",
      "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
      "          known as Broyden's good method.\n",
      "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
      "          is known as Broyden's bad method.\n",
      "        - Method *anderson* uses (extended) Anderson mixing.\n",
      "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
      "          is suitable for large-scale problem.\n",
      "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
      "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
      "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
      "          approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            The algorithms implemented for methods *diagbroyden*,\n",
      "            *linearmixing* and *excitingmixing* may be useful for specific\n",
      "            problems, but whether they will work may depend strongly on the\n",
      "            problem.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
      "           1980. User Guide for MINPACK-1.\n",
      "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
      "           Equations. Society for Industrial and Applied Mathematics.\n",
      "           <https://archive.siam.org/books/kelley/fr16/>\n",
      "        .. [3] W. La Cruz, J.M. Martinez, M. Raydan. Math. Comp. 75, 1429 (2006).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations and its\n",
      "        jacobian.\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        >>> def jac(x):\n",
      "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
      "        ...                       -1.5 * (x[0] - x[1])**2],\n",
      "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
      "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
      "        >>> sol.x\n",
      "        array([ 0.8411639,  0.1588361])\n",
      "    \n",
      "    root_scalar(f, args=(), method=None, bracket=None, fprime=None, fprime2=None, x0=None, x1=None, xtol=None, rtol=None, maxiter=None, options=None)\n",
      "        Find a root of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            A function to find a root of.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its derivative(s).\n",
      "        method : str, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'bisect'    :ref:`(see here) <optimize.root_scalar-bisect>`\n",
      "                - 'brentq'    :ref:`(see here) <optimize.root_scalar-brentq>`\n",
      "                - 'brenth'    :ref:`(see here) <optimize.root_scalar-brenth>`\n",
      "                - 'ridder'    :ref:`(see here) <optimize.root_scalar-ridder>`\n",
      "                - 'toms748'    :ref:`(see here) <optimize.root_scalar-toms748>`\n",
      "                - 'newton'    :ref:`(see here) <optimize.root_scalar-newton>`\n",
      "                - 'secant'    :ref:`(see here) <optimize.root_scalar-secant>`\n",
      "                - 'halley'    :ref:`(see here) <optimize.root_scalar-halley>`\n",
      "        \n",
      "        bracket: A sequence of 2 floats, optional\n",
      "            An interval bracketing a root.  `f(x, *args)` must have different\n",
      "            signs at the two endpoints.\n",
      "        x0 : float, optional\n",
      "            Initial guess.\n",
      "        x1 : float, optional\n",
      "            A second guess.\n",
      "        fprime : bool or callable, optional\n",
      "            If `fprime` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the derivative.\n",
      "            `fprime` can also be a callable returning the derivative of `f`. In\n",
      "            this case, it must accept the same arguments as `f`.\n",
      "        fprime2 : bool or callable, optional\n",
      "            If `fprime2` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the\n",
      "            first and second derivatives.\n",
      "            `fprime2` can also be a callable returning the second derivative of `f`.\n",
      "            In this case, it must accept the same arguments as `f`.\n",
      "        xtol : float, optional\n",
      "            Tolerance (absolute) for termination.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g., ``k``, see\n",
      "            :obj:`show_options()` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : RootResults\n",
      "            The solution represented as a ``RootResults`` object.\n",
      "            Important attributes are: ``root`` the solution , ``converged`` a\n",
      "            boolean flag indicating if the algorithm exited successfully and\n",
      "            ``flag`` which describes the cause of the termination. See\n",
      "            `RootResults` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        root : Find a root of a vector function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "        \n",
      "        The default is to use the best method available for the situation\n",
      "        presented.\n",
      "        If a bracket is provided, it may use one of the bracketing methods.\n",
      "        If a derivative and an initial value are specified, it may\n",
      "        select one of the derivative-based methods.\n",
      "        If no method is judged applicable, it will raise an Exception.\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Find the root of a simple cubic\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        >>> def fprime(x):\n",
      "        ...     return 3*x**2\n",
      "        \n",
      "        The `brentq` method takes as input a bracket\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f, bracket=[0, 3], method='brentq')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 10, 11)\n",
      "        \n",
      "        The `newton` method takes as input a single point and uses the derivative(s)\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f, x0=0.2, fprime=fprime, method='newton')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 22)\n",
      "        \n",
      "        The function can provide the value and derivative(s) in a single call.\n",
      "        \n",
      "        >>> def f_p_pp(x):\n",
      "        ...     return (x**3 - 1), 3*x**2, 6*x\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f_p_pp, x0=0.2, fprime=True, method='newton')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 11)\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f_p_pp, x0=0.2, fprime=True, fprime2=True, method='halley')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 7, 8)\n",
      "    \n",
      "    rosen(x)\n",
      "        The Rosenbrock function.\n",
      "        \n",
      "        The function computed is::\n",
      "        \n",
      "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Rosenbrock function is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f : float\n",
      "            The value of the Rosenbrock function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen_der, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen\n",
      "        >>> X = 0.1 * np.arange(10)\n",
      "        >>> rosen(X)\n",
      "        76.56\n",
      "    \n",
      "    rosen_der(x)\n",
      "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the derivative is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_der : (N,) ndarray\n",
      "            The gradient of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_der\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> rosen_der(X)\n",
      "        array([ -2. ,  10.6,  15.6,  13.4,   6.4,  -3. , -12.4, -19.4,  62. ])\n",
      "    \n",
      "    rosen_hess(x)\n",
      "        The Hessian matrix of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess\n",
      "        >>> X = 0.1 * np.arange(4)\n",
      "        >>> rosen_hess(X)\n",
      "        array([[-38.,   0.,   0.,   0.],\n",
      "               [  0., 134., -40.,   0.],\n",
      "               [  0., -40., 130., -80.],\n",
      "               [  0.,   0., -80., 200.]])\n",
      "    \n",
      "    rosen_hess_prod(x, p)\n",
      "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        p : array_like\n",
      "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess_prod : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
      "            by the vector `p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess_prod\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> p = 0.5 * np.arange(9)\n",
      "        >>> rosen_hess_prod(X, p)\n",
      "        array([  -0.,   27.,  -10.,  -95., -192., -265., -278., -195., -180.])\n",
      "    \n",
      "    shgo(func, bounds, args=(), constraints=None, n=100, iters=1, callback=None, minimizer_kwargs=None, options=None, sampling_method='simplicial')\n",
      "        Finds the global minimum of a function using SHG optimization.\n",
      "        \n",
      "        SHGO stands for \"simplicial homology global optimization\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized.  Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence\n",
      "            Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
      "            defining the lower and upper bounds for the optimizing argument of\n",
      "            `func`. It is required to have ``len(bounds) == len(x)``.\n",
      "            ``len(bounds)`` is used to determine the number of parameters in ``x``.\n",
      "            Use ``None`` for one of min or max when there is no bound in that\n",
      "            direction. By default bounds are ``(None, None)``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        constraints : dict or sequence of dict, optional\n",
      "            Constraints definition.\n",
      "            Function(s) ``R**n`` in the form::\n",
      "        \n",
      "                g(x) >= 0 applied as g : R^n -> R^m\n",
      "                h(x) == 0 applied as h : R^n -> R^p\n",
      "        \n",
      "            Each constraint is defined in a dictionary with fields:\n",
      "        \n",
      "                type : str\n",
      "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "                fun : callable\n",
      "                    The function defining the constraint.\n",
      "                jac : callable, optional\n",
      "                    The Jacobian of `fun` (only for SLSQP).\n",
      "                args : sequence, optional\n",
      "                    Extra arguments to be passed to the function and Jacobian.\n",
      "        \n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "            Note that COBYLA only supports inequality constraints.\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "               Only the COBYLA and SLSQP local minimize methods currently\n",
      "               support constraint arguments. If the ``constraints`` sequence\n",
      "               used in the local optimization problem is not defined in\n",
      "               ``minimizer_kwargs`` and a constrained method is used then the\n",
      "               global ``constraints`` will be used.\n",
      "               (Defining a ``constraints`` sequence in ``minimizer_kwargs``\n",
      "               means that ``constraints`` will not be added so if equality\n",
      "               constraints and so forth need to be added then the inequality\n",
      "               functions in ``constraints`` need to be added to\n",
      "               ``minimizer_kwargs`` too).\n",
      "        \n",
      "        n : int, optional\n",
      "            Number of sampling points used in the construction of the simplicial\n",
      "            complex. Note that this argument is only used for ``sobol`` and other\n",
      "            arbitrary `sampling_methods`.\n",
      "        iters : int, optional\n",
      "            Number of iterations used in the construction of the simplicial complex.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the minimizer\n",
      "            ``scipy.optimize.minimize`` Some important options could be:\n",
      "        \n",
      "                * method : str\n",
      "                    The minimization method (e.g. ``SLSQP``).\n",
      "                * args : tuple\n",
      "                    Extra arguments passed to the objective function (``func``) and\n",
      "                    its derivatives (Jacobian, Hessian).\n",
      "                * options : dict, optional\n",
      "                    Note that by default the tolerance is specified as\n",
      "                    ``{ftol: 1e-12}``\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. Many of the options specified for the\n",
      "            global routine are also passed to the scipy.optimize.minimize routine.\n",
      "            The options that are also passed to the local routine are marked with\n",
      "            \"(L)\".\n",
      "        \n",
      "            Stopping criteria, the algorithm will terminate if any of the specified\n",
      "            criteria are met. However, the default algorithm does not require any to\n",
      "            be specified:\n",
      "        \n",
      "            * maxfev : int (L)\n",
      "                Maximum number of function evaluations in the feasible domain.\n",
      "                (Note only methods that support this option will terminate\n",
      "                the routine at precisely exact specified value. Otherwise the\n",
      "                criterion will only terminate during a global iteration)\n",
      "            * f_min\n",
      "                Specify the minimum objective function value, if it is known.\n",
      "            * f_tol : float\n",
      "                Precision goal for the value of f in the stopping\n",
      "                criterion. Note that the global routine will also\n",
      "                terminate if a sampling point in the global routine is\n",
      "                within this tolerance.\n",
      "            * maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            * maxev : int\n",
      "                Maximum number of sampling evaluations to perform (includes\n",
      "                searching in infeasible points).\n",
      "            * maxtime : float\n",
      "                Maximum processing runtime allowed\n",
      "            * minhgrd : int\n",
      "                Minimum homology group rank differential. The homology group of the\n",
      "                objective function is calculated (approximately) during every\n",
      "                iteration. The rank of this group has a one-to-one correspondence\n",
      "                with the number of locally convex subdomains in the objective\n",
      "                function (after adequate sampling points each of these subdomains\n",
      "                contain a unique global minimum). If the difference in the hgr is 0\n",
      "                between iterations for ``maxhgrd`` specified iterations the\n",
      "                algorithm will terminate.\n",
      "        \n",
      "            Objective function knowledge:\n",
      "        \n",
      "            * symmetry : bool\n",
      "                Specify True if the objective function contains symmetric variables.\n",
      "                The search space (and therefore performance) is decreased by O(n!).\n",
      "        \n",
      "            * jac : bool or callable, optional\n",
      "                Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "                Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. If ``jac`` is a\n",
      "                boolean and is True, ``fun`` is assumed to return the gradient along\n",
      "                with the objective function. If False, the gradient will be\n",
      "                estimated numerically. ``jac`` can also be a callable returning the\n",
      "                gradient of the objective. In this case, it must accept the same\n",
      "                arguments as ``fun``. (Passed to `scipy.optimize.minmize` automatically)\n",
      "        \n",
      "            * hess, hessp : callable, optional\n",
      "                Hessian (matrix of second-order derivatives) of objective function\n",
      "                or Hessian of objective function times an arbitrary vector p.\n",
      "                Only for Newton-CG, dogleg, trust-ncg. Only one of ``hessp`` or\n",
      "                ``hess`` needs to be given. If ``hess`` is provided, then\n",
      "                ``hessp`` will be ignored. If neither ``hess`` nor ``hessp`` is\n",
      "                provided, then the Hessian product will be approximated using\n",
      "                finite differences on ``jac``. ``hessp`` must compute the Hessian\n",
      "                times an arbitrary vector. (Passed to `scipy.optimize.minmize`\n",
      "                automatically)\n",
      "        \n",
      "            Algorithm settings:\n",
      "        \n",
      "            * minimize_every_iter : bool\n",
      "                If True then promising global sampling points will be passed to a\n",
      "                local minimization routine every iteration. If False then only the\n",
      "                final minimizer pool will be run. Defaults to False.\n",
      "            * local_iter : int\n",
      "                Only evaluate a few of the best minimizer pool candidates every\n",
      "                iteration. If False all potential points are passed to the local\n",
      "                minimization routine.\n",
      "            * infty_constraints: bool\n",
      "                If True then any sampling points generated which are outside will\n",
      "                the feasible domain will be saved and given an objective function\n",
      "                value of ``inf``. If False then these points will be discarded.\n",
      "                Using this functionality could lead to higher performance with\n",
      "                respect to function evaluations before the global minimum is found,\n",
      "                specifying False will use less memory at the cost of a slight\n",
      "                decrease in performance. Defaults to True.\n",
      "        \n",
      "            Feedback:\n",
      "        \n",
      "            * disp : bool (L)\n",
      "                Set to True to print convergence messages.\n",
      "        \n",
      "        sampling_method : str or function, optional\n",
      "            Current built in sampling method options are ``sobol`` and\n",
      "            ``simplicial``. The default ``simplicial`` uses less memory and provides\n",
      "            the theoretical guarantee of convergence to the global minimum in finite\n",
      "            time. The ``sobol`` method is faster in terms of sampling point\n",
      "            generation at the cost of higher memory resources and the loss of\n",
      "            guaranteed convergence. It is more appropriate for most \"easier\"\n",
      "            problems where the convergence is relatively fast.\n",
      "            User defined sampling functions must accept two arguments of ``n``\n",
      "            sampling points of dimension ``dim`` per call and output an array of\n",
      "            sampling points with shape `n x dim`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are:\n",
      "            ``x`` the solution array corresponding to the global minimum,\n",
      "            ``fun`` the function output at the global solution,\n",
      "            ``xl`` an ordered list of local minima solutions,\n",
      "            ``funl`` the function output at the corresponding local solutions,\n",
      "            ``success`` a Boolean flag indicating if the optimizer exited\n",
      "            successfully,\n",
      "            ``message`` which describes the cause of the termination,\n",
      "            ``nfev`` the total number of objective function evaluations including\n",
      "            the sampling calls,\n",
      "            ``nlfev`` the total number of objective function evaluations\n",
      "            culminating from all local search optimizations,\n",
      "            ``nit`` number of iterations performed by the global routine.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Global optimization using simplicial homology global optimization [1]_.\n",
      "        Appropriate for solving general purpose NLP and blackbox optimization\n",
      "        problems to global optimality (low-dimensional problems).\n",
      "        \n",
      "        In general, the optimization problems are of the form::\n",
      "        \n",
      "            minimize f(x) subject to\n",
      "        \n",
      "            g_i(x) >= 0,  i = 1,...,m\n",
      "            h_j(x)  = 0,  j = 1,...,p\n",
      "        \n",
      "        where x is a vector of one or more variables. ``f(x)`` is the objective\n",
      "        function ``R^n -> R``, ``g_i(x)`` are the inequality constraints, and\n",
      "        ``h_j(x)`` are the equality constraints.\n",
      "        \n",
      "        Optionally, the lower and upper bounds for each element in x can also be\n",
      "        specified using the `bounds` argument.\n",
      "        \n",
      "        While most of the theoretical advantages of SHGO are only proven for when\n",
      "        ``f(x)`` is a Lipschitz smooth function, the algorithm is also proven to\n",
      "        converge to the global optimum for the more general case where ``f(x)`` is\n",
      "        non-continuous, non-convex and non-smooth, if the default sampling method\n",
      "        is used [1]_.\n",
      "        \n",
      "        The local search method may be specified using the ``minimizer_kwargs``\n",
      "        parameter which is passed on to ``scipy.optimize.minimize``. By default,\n",
      "        the ``SLSQP`` method is used. In general, it is recommended to use the\n",
      "        ``SLSQP`` or ``COBYLA`` local minimization if inequality constraints\n",
      "        are defined for the problem since the other methods do not use constraints.\n",
      "        \n",
      "        The ``sobol`` method points are generated using the Sobol (1967) [2]_\n",
      "        sequence. The primitive polynomials and various sets of initial direction\n",
      "        numbers for generating Sobol sequences is provided by [3]_ by Frances Kuo\n",
      "        and Stephen Joe. The original program sobol.cc (MIT) is available and\n",
      "        described at https://web.maths.unsw.edu.au/~fkuo/sobol/ translated to\n",
      "        Python 3 by Carl Sandrock 2016-03-31.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Endres, SC, Sandrock, C, Focke, WW (2018) \"A simplicial homology\n",
      "               algorithm for lipschitz optimisation\", Journal of Global Optimization.\n",
      "        .. [2] Sobol, IM (1967) \"The distribution of points in a cube and the\n",
      "               approximate evaluation of integrals\", USSR Comput. Math. Math. Phys.\n",
      "               7, 86-112.\n",
      "        .. [3] Joe, SW and Kuo, FY (2008) \"Constructing Sobol sequences with\n",
      "               better  two-dimensional projections\", SIAM J. Sci. Comput. 30,\n",
      "               2635-2654.\n",
      "        .. [4] Hoch, W and Schittkowski, K (1981) \"Test examples for nonlinear\n",
      "               programming codes\", Lecture Notes in Economics and Mathematical\n",
      "               Systems, 187. Springer-Verlag, New York.\n",
      "               http://www.ai7.uni-bayreuth.de/test_problem_coll.pdf\n",
      "        .. [5] Wales, DJ (2015) \"Perspective: Insight into reaction coordinates and\n",
      "               dynamics from the potential energy landscape\",\n",
      "               Journal of Chemical Physics, 142(13), 2015.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First consider the problem of minimizing the Rosenbrock function, `rosen`:\n",
      "        \n",
      "        >>> from scipy.optimize import rosen, shgo\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 1.,  1.,  1.,  1.,  1.]), 2.9203923741900809e-18)\n",
      "        \n",
      "        Note that bounds determine the dimensionality of the objective\n",
      "        function and is therefore a required input, however you can specify\n",
      "        empty bounds using ``None`` or objects like ``np.inf`` which will be\n",
      "        converted to large float numbers.\n",
      "        \n",
      "        >>> bounds = [(None, None), ]*4\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x\n",
      "        array([ 0.99999851,  0.99999704,  0.99999411,  0.9999882 ])\n",
      "        \n",
      "        Next, we consider the Eggholder function, a problem with several local\n",
      "        minima and one global minimum. We will demonstrate the use of arguments and\n",
      "        the capabilities of `shgo`.\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization)\n",
      "        \n",
      "        >>> def eggholder(x):\n",
      "        ...     return (-(x[1] + 47.0)\n",
      "        ...             * np.sin(np.sqrt(abs(x[0]/2.0 + (x[1] + 47.0))))\n",
      "        ...             - x[0] * np.sin(np.sqrt(abs(x[0] - (x[1] + 47.0))))\n",
      "        ...             )\n",
      "        ...\n",
      "        >>> bounds = [(-512, 512), (-512, 512)]\n",
      "        \n",
      "        `shgo` has two built-in low discrepancy sampling sequences. First, we will\n",
      "        input 30 initial sampling points of the Sobol sequence:\n",
      "        \n",
      "        >>> result = shgo(eggholder, bounds, n=30, sampling_method='sobol')\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 512.        ,  404.23180542]), -959.64066272085051)\n",
      "        \n",
      "        `shgo` also has a return for any other local minima that was found, these\n",
      "        can be called using:\n",
      "        \n",
      "        >>> result.xl\n",
      "        array([[ 512.        ,  404.23180542],\n",
      "               [ 283.07593402, -487.12566542],\n",
      "               [-294.66820039, -462.01964031],\n",
      "               [-105.87688985,  423.15324143],\n",
      "               [-242.97923629,  274.38032063],\n",
      "               [-506.25823477,    6.3131022 ],\n",
      "               [-408.71981195, -156.10117154],\n",
      "               [ 150.23210485,  301.31378508],\n",
      "               [  91.00922754, -391.28375925],\n",
      "               [ 202.8966344 , -269.38042147],\n",
      "               [ 361.66625957, -106.96490692],\n",
      "               [-219.40615102, -244.06022436],\n",
      "               [ 151.59603137, -100.61082677]])\n",
      "        \n",
      "        >>> result.funl\n",
      "        array([-959.64066272, -718.16745962, -704.80659592, -565.99778097,\n",
      "               -559.78685655, -557.36868733, -507.87385942, -493.9605115 ,\n",
      "               -426.48799655, -421.15571437, -419.31194957, -410.98477763,\n",
      "               -202.53912972])\n",
      "        \n",
      "        These results are useful in applications where there are many global minima\n",
      "        and the values of other global minima are desired or where the local minima\n",
      "        can provide insight into the system (for example morphologies\n",
      "        in physical chemistry [5]_).\n",
      "        \n",
      "        If we want to find a larger number of local minima, we can increase the\n",
      "        number of sampling points or the number of iterations. We'll increase the\n",
      "        number of sampling points to 60 and the number of iterations from the\n",
      "        default of 1 to 5. This gives us 60 x 5 = 300 initial sampling points.\n",
      "        \n",
      "        >>> result_2 = shgo(eggholder, bounds, n=60, iters=5, sampling_method='sobol')\n",
      "        >>> len(result.xl), len(result_2.xl)\n",
      "        (13, 39)\n",
      "        \n",
      "        Note the difference between, e.g., ``n=180, iters=1`` and ``n=60, iters=3``.\n",
      "        In the first case the promising points contained in the minimiser pool\n",
      "        is processed only once. In the latter case it is processed every 60 sampling\n",
      "        points for a total of 3 times.\n",
      "        \n",
      "        To demonstrate solving problems with non-linear constraints consider the\n",
      "        following example from Hock and Schittkowski problem 73 (cattle-feed) [4]_::\n",
      "        \n",
      "            minimize: f = 24.55 * x_1 + 26.75 * x_2 + 39 * x_3 + 40.50 * x_4\n",
      "        \n",
      "            subject to: 2.3 * x_1 + 5.6 * x_2 + 11.1 * x_3 + 1.3 * x_4 - 5     >= 0,\n",
      "        \n",
      "                        12 * x_1 + 11.9 * x_2 + 41.8 * x_3 + 52.1 * x_4 - 21\n",
      "                            -1.645 * sqrt(0.28 * x_1**2 + 0.19 * x_2**2 +\n",
      "                                          20.5 * x_3**2 + 0.62 * x_4**2)       >= 0,\n",
      "        \n",
      "                        x_1 + x_2 + x_3 + x_4 - 1                              == 0,\n",
      "        \n",
      "                        1 >= x_i >= 0 for all i\n",
      "        \n",
      "        The approximate answer given in [4]_ is::\n",
      "        \n",
      "            f([0.6355216, -0.12e-11, 0.3127019, 0.05177655]) = 29.894378\n",
      "        \n",
      "        >>> def f(x):  # (cattle-feed)\n",
      "        ...     return 24.55*x[0] + 26.75*x[1] + 39*x[2] + 40.50*x[3]\n",
      "        ...\n",
      "        >>> def g1(x):\n",
      "        ...     return 2.3*x[0] + 5.6*x[1] + 11.1*x[2] + 1.3*x[3] - 5  # >=0\n",
      "        ...\n",
      "        >>> def g2(x):\n",
      "        ...     return (12*x[0] + 11.9*x[1] +41.8*x[2] + 52.1*x[3] - 21\n",
      "        ...             - 1.645 * np.sqrt(0.28*x[0]**2 + 0.19*x[1]**2\n",
      "        ...                             + 20.5*x[2]**2 + 0.62*x[3]**2)\n",
      "        ...             ) # >=0\n",
      "        ...\n",
      "        >>> def h1(x):\n",
      "        ...     return x[0] + x[1] + x[2] + x[3] - 1  # == 0\n",
      "        ...\n",
      "        >>> cons = ({'type': 'ineq', 'fun': g1},\n",
      "        ...         {'type': 'ineq', 'fun': g2},\n",
      "        ...         {'type': 'eq', 'fun': h1})\n",
      "        >>> bounds = [(0, 1.0),]*4\n",
      "        >>> res = shgo(f, bounds, iters=3, constraints=cons)\n",
      "        >>> res\n",
      "             fun: 29.894378159142136\n",
      "            funl: array([29.89437816])\n",
      "         message: 'Optimization terminated successfully.'\n",
      "            nfev: 114\n",
      "             nit: 3\n",
      "           nlfev: 35\n",
      "           nlhev: 0\n",
      "           nljev: 5\n",
      "         success: True\n",
      "               x: array([6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02])\n",
      "              xl: array([[6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02]])\n",
      "        \n",
      "        >>> g1(res.x), g2(res.x), h1(res.x)\n",
      "        (-5.0626169922907138e-14, -2.9594104944408173e-12, 0.0)\n",
      "    \n",
      "    show_options(solver=None, method=None, disp=True)\n",
      "        Show documentation for additional options of optimization solvers.\n",
      "        \n",
      "        These are method-specific options that can be supplied through the\n",
      "        ``options`` dict.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        solver : str\n",
      "            Type of optimization solver. One of 'minimize', 'minimize_scalar',\n",
      "            'root', or 'linprog'.\n",
      "        method : str, optional\n",
      "            If not given, shows all methods of the specified solver. Otherwise,\n",
      "            show only the options for the specified method. Valid values\n",
      "            corresponds to methods' names of respective solver (e.g., 'BFGS' for\n",
      "            'minimize').\n",
      "        disp : bool, optional\n",
      "            Whether to print the result rather than returning it.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        text\n",
      "            Either None (for disp=True) or the text string (disp=False)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The solver-specific methods are:\n",
      "        \n",
      "        `scipy.optimize.minimize`\n",
      "        \n",
      "        - :ref:`Nelder-Mead <optimize.minimize-neldermead>`\n",
      "        - :ref:`Powell      <optimize.minimize-powell>`\n",
      "        - :ref:`CG          <optimize.minimize-cg>`\n",
      "        - :ref:`BFGS        <optimize.minimize-bfgs>`\n",
      "        - :ref:`Newton-CG   <optimize.minimize-newtoncg>`\n",
      "        - :ref:`L-BFGS-B    <optimize.minimize-lbfgsb>`\n",
      "        - :ref:`TNC         <optimize.minimize-tnc>`\n",
      "        - :ref:`COBYLA      <optimize.minimize-cobyla>`\n",
      "        - :ref:`SLSQP       <optimize.minimize-slsqp>`\n",
      "        - :ref:`dogleg      <optimize.minimize-dogleg>`\n",
      "        - :ref:`trust-ncg   <optimize.minimize-trustncg>`\n",
      "        \n",
      "        `scipy.optimize.root`\n",
      "        \n",
      "        - :ref:`hybr              <optimize.root-hybr>`\n",
      "        - :ref:`lm                <optimize.root-lm>`\n",
      "        - :ref:`broyden1          <optimize.root-broyden1>`\n",
      "        - :ref:`broyden2          <optimize.root-broyden2>`\n",
      "        - :ref:`anderson          <optimize.root-anderson>`\n",
      "        - :ref:`linearmixing      <optimize.root-linearmixing>`\n",
      "        - :ref:`diagbroyden       <optimize.root-diagbroyden>`\n",
      "        - :ref:`excitingmixing    <optimize.root-excitingmixing>`\n",
      "        - :ref:`krylov            <optimize.root-krylov>`\n",
      "        - :ref:`df-sane           <optimize.root-dfsane>`\n",
      "        \n",
      "        `scipy.optimize.minimize_scalar`\n",
      "        \n",
      "        - :ref:`brent       <optimize.minimize_scalar-brent>`\n",
      "        - :ref:`golden      <optimize.minimize_scalar-golden>`\n",
      "        - :ref:`bounded     <optimize.minimize_scalar-bounded>`\n",
      "        \n",
      "        `scipy.optimize.linprog`\n",
      "        \n",
      "        - :ref:`simplex         <optimize.linprog-simplex>`\n",
      "        - :ref:`interior-point  <optimize.linprog-interior-point>`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We can print documentations of a solver in stdout:\n",
      "        \n",
      "        >>> from scipy.optimize import show_options\n",
      "        >>> show_options(solver=\"minimize\")\n",
      "        ...\n",
      "        \n",
      "        Specifying a method is possible:\n",
      "        \n",
      "        >>> show_options(solver=\"minimize\", method=\"Nelder-Mead\")\n",
      "        ...\n",
      "        \n",
      "        We can also get the documentations as a string:\n",
      "        \n",
      "        >>> show_options(solver=\"minimize\", method=\"Nelder-Mead\", disp=False)\n",
      "        Minimization of scalar function of one or more variables using the ...\n",
      "    \n",
      "    toms748(f, a, b, args=(), k=1, xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a zero using TOMS Algorithm 748 method.\n",
      "        \n",
      "        Implements the Algorithm 748 method of Alefeld, Potro and Shi to find a\n",
      "        zero of the function `f` on the interval `[a , b]`, where `f(a)` and\n",
      "        `f(b)` must have opposite signs.\n",
      "        \n",
      "        It uses a mixture of inverse cubic interpolation and\n",
      "        \"Newton-quadratic\" steps. [APS1995].\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a scalar. The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)`\n",
      "            have opposite signs.\n",
      "        a : scalar,\n",
      "            lower boundary of the search interval\n",
      "        b : scalar,\n",
      "            upper boundary of the search interval\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``f(x, *args)``.\n",
      "        k : int, optional\n",
      "            The number of Newton quadratic steps to perform each\n",
      "            iteration. ``k>=1``.\n",
      "        xtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in the `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Approximate Zero of `f`\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect, newton\n",
      "        fsolve : find zeroes in N dimensions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.\n",
      "        Algorithm 748 with ``k=2`` is asymptotically the most efficient\n",
      "        algorithm known for finding roots of a four times continuously\n",
      "        differentiable function.\n",
      "        In contrast with Brent's algorithm, which may only decrease the length of\n",
      "        the enclosing bracket on the last step, Algorithm 748 decreases it each\n",
      "        iteration with the same asymptotic efficiency as it finds the root.\n",
      "        \n",
      "        For easy statement of efficiency indices, assume that `f` has 4\n",
      "        continuouous deriviatives.\n",
      "        For ``k=1``, the convergence order is at least 2.7, and with about\n",
      "        asymptotically 2 function evaluations per iteration, the efficiency\n",
      "        index is approximately 1.65.\n",
      "        For ``k=2``, the order is about 4.6 with asymptotically 3 function\n",
      "        evaluations per iteration, and the efficiency index 1.66.\n",
      "        For higher values of `k`, the efficiency index approaches\n",
      "        the kth root of ``(3k-2)``, hence ``k=1`` or ``k=2`` are\n",
      "        usually appropriate.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [APS1995]\n",
      "           Alefeld, G. E. and Potra, F. A. and Shi, Yixun,\n",
      "           *Algorithm 748: Enclosing Zeros of Continuous Functions*,\n",
      "           ACM Trans. Math. Softw. Volume 221(1995)\n",
      "           doi = {10.1145/210089.210111}\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> root, results = optimize.toms748(f, 0, 2, full_output=True)\n",
      "        >>> root\n",
      "        1.0\n",
      "        >>> results\n",
      "              converged: True\n",
      "                   flag: 'converged'\n",
      "         function_calls: 11\n",
      "             iterations: 5\n",
      "                   root: 1.0\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BFGS', 'Bounds', 'HessianUpdateStrategy', 'LbfgsInvHessPro...\n",
      "\n",
      "FILE\n",
      "    /usr/local/lib/python3.8/site-packages/scipy/optimize/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51, 46, 52, 53, 54, 59, 56, 52, 47, 48, 46, 41, 50, 53, 47, 48, 51,\n",
       "       44, 45, 50, 54, 49, 48, 49, 44, 54, 48, 49, 45, 55, 54, 45, 51, 49,\n",
       "       49, 49, 37, 46, 48, 48, 46, 47, 48, 51, 48, 46, 50, 50, 45, 56, 48,\n",
       "       53, 38, 57, 49, 48, 51, 47, 55, 44, 47, 48, 48, 44, 53, 45, 51, 51,\n",
       "       49, 51, 55, 57, 46, 50, 49, 45, 55, 56, 52, 52, 54, 43, 57, 39, 43,\n",
       "       53, 52, 54, 48, 51, 54, 47, 51, 52, 57, 50, 49, 53, 46, 43])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coin Flip\n",
    "iterations = 100\n",
    "np.array([np.sum(npr.random(iterations) < 0.5) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_mean (flips=int(1e3), num_experiments=100):\n",
    "    return np.mean(np.array([np.sum(npr.random(flips) < 0.5) for _ in range(num_experiments)]))\n",
    "\n",
    "means = []\n",
    "for i in range(1, int(1e3)):\n",
    "    means.append(get_mean(num_experiments=i))\n",
    "plt.plot(means);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bin_edges, _ = plt.hist(npr.random(100), bins=int(1e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_size = bin_edges[1] - bin_edges[0]\n",
    "new_widths = bin_size * counts / counts.max()\n",
    "plt.bar(bin_edges[:-1], counts, width=new_widths, color=['r', 'g', 'b']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log2bins = np.logspace(-8, 0, num=9, base=2)\n",
    "log2bins[0] = 0.0\n",
    "counts, bin_edges, _ = plt.hist(npr.random(100), bins=log2bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_size = bin_edges[1] - bin_edges[0]\n",
    "plt.bar(bin_edges[:-1], counts, width=bin_size, align='edge');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contour Plot\n",
    "x_vals = np.linspace(-3, 3, 21)\n",
    "y_vals = np.linspace(0, 10, 11)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "R = np.sqrt(X**2 + Y**2)\n",
    "Z = np.cos(X) * np.sin(Y)\n",
    "cs = plt.contourf(X, Y, Z, 10, cmap=None)\n",
    "plt.clabel(cs, fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Surface Plot\n",
    "ax = Axes3D(plt.figure())\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_roots (f, points=0):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    plt.plot(x, f(x));\n",
    "    roots, infodict, ier, mesg = fsolve(f, points, full_output=True)\n",
    "    plt.scatter(roots, np.zeros(len(roots)));\n",
    "    if ier == 1:\n",
    "        print('solution found')\n",
    "    else:\n",
    "        print('solution not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution found\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f (x): return x**2 - 1\n",
    "find_roots(f, [-0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution not found\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f (x): return np.sin(x)**10\n",
    "find_roots(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution found\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f (x): return x * (1 + x**3) - 1\n",
    "find_roots(f, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.22074408+0.j        ,  0.24812606+1.03398206j,\n",
       "        0.24812606-1.03398206j,  0.72449196+0.j        ])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.roots([1,0,0,1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-373-9df5c294a716>:1: RuntimeWarning: invalid value encountered in power\n",
      "  def f (x): return x**3.4 + x - 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f (x): return x**3.4 + x - 1\n",
    "find_roots(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
