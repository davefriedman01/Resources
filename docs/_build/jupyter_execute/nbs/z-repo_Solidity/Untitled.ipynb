{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "from scipy.optimize import fsolve # finding roots\n",
    "\n",
    "%matplotlib inline\n",
    "from pylab import mpl, plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # surface plot\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BFGS',\n",
       " 'Bounds',\n",
       " 'HessianUpdateStrategy',\n",
       " 'LbfgsInvHessProduct',\n",
       " 'LinearConstraint',\n",
       " 'NonlinearConstraint',\n",
       " 'OptimizeResult',\n",
       " 'OptimizeWarning',\n",
       " 'RootResults',\n",
       " 'SR1',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__nnls',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_basinhopping',\n",
       " '_bglu_dense',\n",
       " '_cobyla',\n",
       " '_constraints',\n",
       " '_differentiable_functions',\n",
       " '_differentialevolution',\n",
       " '_dual_annealing',\n",
       " '_group_columns',\n",
       " '_hessian_update_strategy',\n",
       " '_highs',\n",
       " '_lbfgsb',\n",
       " '_linprog',\n",
       " '_linprog_doc',\n",
       " '_linprog_highs',\n",
       " '_linprog_ip',\n",
       " '_linprog_rs',\n",
       " '_linprog_simplex',\n",
       " '_linprog_util',\n",
       " '_lsap',\n",
       " '_lsap_module',\n",
       " '_lsq',\n",
       " '_minimize',\n",
       " '_minpack',\n",
       " '_nnls',\n",
       " '_numdiff',\n",
       " '_qap',\n",
       " '_remove_redundancy',\n",
       " '_root',\n",
       " '_root_scalar',\n",
       " '_shgo',\n",
       " '_shgo_lib',\n",
       " '_slsqp',\n",
       " '_spectral',\n",
       " '_trlib',\n",
       " '_trustregion',\n",
       " '_trustregion_constr',\n",
       " '_trustregion_dogleg',\n",
       " '_trustregion_exact',\n",
       " '_trustregion_krylov',\n",
       " '_trustregion_ncg',\n",
       " '_zeros',\n",
       " 'anderson',\n",
       " 'approx_fprime',\n",
       " 'basinhopping',\n",
       " 'bisect',\n",
       " 'bracket',\n",
       " 'brent',\n",
       " 'brenth',\n",
       " 'brentq',\n",
       " 'broyden1',\n",
       " 'broyden2',\n",
       " 'brute',\n",
       " 'check_grad',\n",
       " 'cobyla',\n",
       " 'curve_fit',\n",
       " 'diagbroyden',\n",
       " 'differential_evolution',\n",
       " 'dual_annealing',\n",
       " 'excitingmixing',\n",
       " 'fixed_point',\n",
       " 'fmin',\n",
       " 'fmin_bfgs',\n",
       " 'fmin_cg',\n",
       " 'fmin_cobyla',\n",
       " 'fmin_l_bfgs_b',\n",
       " 'fmin_ncg',\n",
       " 'fmin_powell',\n",
       " 'fmin_slsqp',\n",
       " 'fmin_tnc',\n",
       " 'fminbound',\n",
       " 'fsolve',\n",
       " 'golden',\n",
       " 'lbfgsb',\n",
       " 'least_squares',\n",
       " 'leastsq',\n",
       " 'line_search',\n",
       " 'linear_sum_assignment',\n",
       " 'linearmixing',\n",
       " 'linesearch',\n",
       " 'linprog',\n",
       " 'linprog_verbose_callback',\n",
       " 'lsq_linear',\n",
       " 'minimize',\n",
       " 'minimize_scalar',\n",
       " 'minpack',\n",
       " 'minpack2',\n",
       " 'moduleTNC',\n",
       " 'newton',\n",
       " 'newton_krylov',\n",
       " 'nnls',\n",
       " 'nonlin',\n",
       " 'optimize',\n",
       " 'quadratic_assignment',\n",
       " 'ridder',\n",
       " 'root',\n",
       " 'root_scalar',\n",
       " 'rosen',\n",
       " 'rosen_der',\n",
       " 'rosen_hess',\n",
       " 'rosen_hess_prod',\n",
       " 'shgo',\n",
       " 'show_options',\n",
       " 'slsqp',\n",
       " 'test',\n",
       " 'tnc',\n",
       " 'toms748',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "dir(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.optimize in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.optimize\n",
      "\n",
      "DESCRIPTION\n",
      "    =====================================================\n",
      "    Optimization and root finding (:mod:`scipy.optimize`)\n",
      "    =====================================================\n",
      "    \n",
      "    .. currentmodule:: scipy.optimize\n",
      "    \n",
      "    SciPy ``optimize`` provides functions for minimizing (or maximizing)\n",
      "    objective functions, possibly subject to constraints. It includes\n",
      "    solvers for nonlinear problems (with support for both local and global\n",
      "    optimization algorithms), linear programing, constrained\n",
      "    and nonlinear least-squares, root finding, and curve fitting.\n",
      "    \n",
      "    Common functions and objects, shared across different solvers, are:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       show_options - Show specific options optimization solvers.\n",
      "       OptimizeResult - The optimization result returned by some optimizers.\n",
      "       OptimizeWarning - The optimization encountered problems.\n",
      "    \n",
      "    \n",
      "    Optimization\n",
      "    ============\n",
      "    \n",
      "    Scalar functions optimization\n",
      "    -----------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       minimize_scalar - Interface for minimizers of univariate functions\n",
      "    \n",
      "    The `minimize_scalar` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize_scalar-brent\n",
      "       optimize.minimize_scalar-bounded\n",
      "       optimize.minimize_scalar-golden\n",
      "    \n",
      "    Local (multivariate) optimization\n",
      "    ---------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       minimize - Interface for minimizers of multivariate functions.\n",
      "    \n",
      "    The `minimize` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize-neldermead\n",
      "       optimize.minimize-powell\n",
      "       optimize.minimize-cg\n",
      "       optimize.minimize-bfgs\n",
      "       optimize.minimize-newtoncg\n",
      "       optimize.minimize-lbfgsb\n",
      "       optimize.minimize-tnc\n",
      "       optimize.minimize-cobyla\n",
      "       optimize.minimize-slsqp\n",
      "       optimize.minimize-trustconstr\n",
      "       optimize.minimize-dogleg\n",
      "       optimize.minimize-trustncg\n",
      "       optimize.minimize-trustkrylov\n",
      "       optimize.minimize-trustexact\n",
      "    \n",
      "    Constraints are passed to `minimize` function as a single object or\n",
      "    as a list of objects from the following classes:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       NonlinearConstraint - Class defining general nonlinear constraints.\n",
      "       LinearConstraint - Class defining general linear constraints.\n",
      "    \n",
      "    Simple bound constraints are handled separately and there is a special class\n",
      "    for them:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       Bounds - Bound constraints.\n",
      "    \n",
      "    Quasi-Newton strategies implementing `HessianUpdateStrategy`\n",
      "    interface can be used to approximate the Hessian in `minimize`\n",
      "    function (available only for the 'trust-constr' method). Available\n",
      "    quasi-Newton methods implementing this interface are:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       BFGS - Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "       SR1 - Symmetric-rank-1 Hessian update strategy.\n",
      "    \n",
      "    Global optimization\n",
      "    -------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       basinhopping - Basinhopping stochastic optimizer.\n",
      "       brute - Brute force searching optimizer.\n",
      "       differential_evolution - stochastic minimization using differential evolution.\n",
      "    \n",
      "       shgo - simplicial homology global optimisation\n",
      "       dual_annealing - Dual annealing stochastic optimizer.\n",
      "    \n",
      "    \n",
      "    Least-squares and curve fitting\n",
      "    ===============================\n",
      "    \n",
      "    Nonlinear least-squares\n",
      "    -----------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       least_squares - Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "    \n",
      "    Linear least-squares\n",
      "    --------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       nnls - Linear least-squares problem with non-negativity constraint.\n",
      "       lsq_linear - Linear least-squares problem with bound constraints.\n",
      "    \n",
      "    Curve fitting\n",
      "    -------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       curve_fit -- Fit curve to a set of points.\n",
      "    \n",
      "    Root finding\n",
      "    ============\n",
      "    \n",
      "    Scalar functions\n",
      "    ----------------\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       root_scalar - Unified interface for nonlinear solvers of scalar functions.\n",
      "       brentq - quadratic interpolation Brent method.\n",
      "       brenth - Brent method, modified by Harris with hyperbolic extrapolation.\n",
      "       ridder - Ridder's method.\n",
      "       bisect - Bisection method.\n",
      "       newton - Newton's method (also Secant and Halley's methods).\n",
      "       toms748 - Alefeld, Potra & Shi Algorithm 748.\n",
      "       RootResults - The root finding result returned by some root finders.\n",
      "    \n",
      "    The `root_scalar` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.root_scalar-brentq\n",
      "       optimize.root_scalar-brenth\n",
      "       optimize.root_scalar-bisect\n",
      "       optimize.root_scalar-ridder\n",
      "       optimize.root_scalar-newton\n",
      "       optimize.root_scalar-toms748\n",
      "       optimize.root_scalar-secant\n",
      "       optimize.root_scalar-halley\n",
      "    \n",
      "    \n",
      "    \n",
      "    The table below lists situations and appropriate methods, along with\n",
      "    *asymptotic* convergence rates per iteration (and per function evaluation)\n",
      "    for successful convergence to a simple root(*).\n",
      "    Bisection is the slowest of them all, adding one bit of accuracy for each\n",
      "    function evaluation, but is guaranteed to converge.\n",
      "    The other bracketing methods all (eventually) increase the number of accurate\n",
      "    bits by about 50% for every function evaluation.\n",
      "    The derivative-based methods, all built on `newton`, can converge quite quickly\n",
      "    if the initial value is close to the root.  They can also be applied to\n",
      "    functions defined on (a subset of) the complex plane.\n",
      "    \n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | Domain of f | Bracket? |    Derivatives?      | Solvers     |        Convergence           |\n",
      "    +             +          +----------+-----------+             +-------------+----------------+\n",
      "    |             |          | `fprime` | `fprime2` |             | Guaranteed? |  Rate(s)(*)    |\n",
      "    +=============+==========+==========+===========+=============+=============+================+\n",
      "    | `R`         | Yes      | N/A      | N/A       | - bisection | - Yes       | - 1 \"Linear\"   |\n",
      "    |             |          |          |           | - brentq    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - brenth    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - ridder    | - Yes       | - 2.0 (1.41)   |\n",
      "    |             |          |          |           | - toms748   | - Yes       | - 2.7 (1.65)   |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | No       | No        | secant      | No          | 1.62 (1.62)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | No        | newton      | No          | 2.00 (1.41)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | Yes       | halley      | No          | 3.00 (1.44)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "       `scipy.optimize.cython_optimize` -- Typed Cython versions of zeros functions\n",
      "    \n",
      "    Fixed point finding:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fixed_point - Single-variable fixed-point solver.\n",
      "    \n",
      "    Multidimensional\n",
      "    ----------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       root - Unified interface for nonlinear solvers of multivariate functions.\n",
      "    \n",
      "    The `root` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.root-hybr\n",
      "       optimize.root-lm\n",
      "       optimize.root-broyden1\n",
      "       optimize.root-broyden2\n",
      "       optimize.root-anderson\n",
      "       optimize.root-linearmixing\n",
      "       optimize.root-diagbroyden\n",
      "       optimize.root-excitingmixing\n",
      "       optimize.root-krylov\n",
      "       optimize.root-dfsane\n",
      "    \n",
      "    Linear programming\n",
      "    ==================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linprog -- Unified interface for minimizers of linear programming problems.\n",
      "    \n",
      "    The `linprog` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.linprog-simplex\n",
      "       optimize.linprog-interior-point\n",
      "       optimize.linprog-revised_simplex\n",
      "       optimize.linprog-highs-ipm\n",
      "       optimize.linprog-highs-ds\n",
      "       optimize.linprog-highs\n",
      "    \n",
      "    The simplex, interior-point, and revised simplex methods support callback\n",
      "    functions, such as:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linprog_verbose_callback -- Sample callback function for linprog (simplex).\n",
      "    \n",
      "    Assignment problems\n",
      "    ===================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linear_sum_assignment -- Solves the linear-sum assignment problem.\n",
      "       quadratic_assignment -- Solves the quadratic assignment problem.\n",
      "    \n",
      "    The `quadratic_assignment` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.qap-faq\n",
      "       optimize.qap-2opt\n",
      "    \n",
      "    Utilities\n",
      "    =========\n",
      "    \n",
      "    Finite-difference approximation\n",
      "    -------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       approx_fprime - Approximate the gradient of a scalar function.\n",
      "       check_grad - Check the supplied derivative using finite differences.\n",
      "    \n",
      "    \n",
      "    Line search\n",
      "    -----------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       bracket - Bracket a minimum, given two starting points.\n",
      "       line_search - Return a step that satisfies the strong Wolfe conditions.\n",
      "    \n",
      "    Hessian approximation\n",
      "    ---------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       LbfgsInvHessProduct - Linear operator for L-BFGS approximate inverse Hessian.\n",
      "       HessianUpdateStrategy - Interface for implementing Hessian update strategies\n",
      "    \n",
      "    Benchmark problems\n",
      "    ------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       rosen - The Rosenbrock function.\n",
      "       rosen_der - The derivative of the Rosenbrock function.\n",
      "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
      "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
      "    \n",
      "    Legacy functions\n",
      "    ================\n",
      "    \n",
      "    The functions below are not recommended for use in new scripts;\n",
      "    all of these methods are accessible via a newer, more consistent\n",
      "    interfaces, provided by the interfaces above.\n",
      "    \n",
      "    Optimization\n",
      "    ------------\n",
      "    \n",
      "    General-purpose multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin - Nelder-Mead Simplex algorithm.\n",
      "       fmin_powell - Powell's (modified) level set method.\n",
      "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm.\n",
      "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno).\n",
      "       fmin_ncg - Line-search Newton Conjugate Gradient.\n",
      "    \n",
      "    Constrained multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer.\n",
      "       fmin_tnc - Truncated Newton code.\n",
      "       fmin_cobyla - Constrained optimization by linear approximation.\n",
      "       fmin_slsqp - Minimization using sequential least-squares programming.\n",
      "    \n",
      "    Univariate (scalar) minimization methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fminbound - Bounded minimization of a scalar function.\n",
      "       brent - 1-D function minimization using Brent method.\n",
      "       golden - 1-D function minimization using Golden Section method.\n",
      "    \n",
      "    Least-squares\n",
      "    -------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       leastsq - Minimize the sum of squares of M equations in N unknowns.\n",
      "    \n",
      "    Root finding\n",
      "    ------------\n",
      "    \n",
      "    General nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fsolve - Non-linear multivariable equation solver.\n",
      "       broyden1 - Broyden's first method.\n",
      "       broyden2 - Broyden's second method.\n",
      "    \n",
      "    Large-scale nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       newton_krylov\n",
      "       anderson\n",
      "    \n",
      "    Simple iteration solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       excitingmixing\n",
      "       linearmixing\n",
      "       diagbroyden\n",
      "    \n",
      "    :mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __nnls\n",
      "    _basinhopping\n",
      "    _bglu_dense\n",
      "    _cobyla\n",
      "    _constraints\n",
      "    _differentiable_functions\n",
      "    _differentialevolution\n",
      "    _dual_annealing\n",
      "    _group_columns\n",
      "    _hessian_update_strategy\n",
      "    _highs (package)\n",
      "    _lbfgsb\n",
      "    _linprog\n",
      "    _linprog_doc\n",
      "    _linprog_highs\n",
      "    _linprog_ip\n",
      "    _linprog_rs\n",
      "    _linprog_simplex\n",
      "    _linprog_util\n",
      "    _lsap\n",
      "    _lsap_module\n",
      "    _lsq (package)\n",
      "    _minimize\n",
      "    _minpack\n",
      "    _nnls\n",
      "    _numdiff\n",
      "    _qap\n",
      "    _remove_redundancy\n",
      "    _root\n",
      "    _root_scalar\n",
      "    _shgo\n",
      "    _shgo_lib (package)\n",
      "    _slsqp\n",
      "    _spectral\n",
      "    _trlib (package)\n",
      "    _trustregion\n",
      "    _trustregion_constr (package)\n",
      "    _trustregion_dogleg\n",
      "    _trustregion_exact\n",
      "    _trustregion_krylov\n",
      "    _trustregion_ncg\n",
      "    _tstutils\n",
      "    _zeros\n",
      "    cobyla\n",
      "    cython_optimize (package)\n",
      "    lbfgsb\n",
      "    linesearch\n",
      "    minpack\n",
      "    minpack2\n",
      "    moduleTNC\n",
      "    nonlin\n",
      "    optimize\n",
      "    setup\n",
      "    slsqp\n",
      "    tests (package)\n",
      "    tnc\n",
      "    zeros\n",
      "\n",
      "CLASSES\n",
      "    builtins.UserWarning(builtins.Warning)\n",
      "        scipy.optimize.optimize.OptimizeWarning\n",
      "    builtins.dict(builtins.object)\n",
      "        scipy.optimize.optimize.OptimizeResult\n",
      "    builtins.object\n",
      "        scipy.optimize._constraints.Bounds\n",
      "        scipy.optimize._constraints.LinearConstraint\n",
      "        scipy.optimize._constraints.NonlinearConstraint\n",
      "        scipy.optimize._hessian_update_strategy.HessianUpdateStrategy\n",
      "        scipy.optimize.zeros.RootResults\n",
      "    scipy.optimize._hessian_update_strategy.FullHessianUpdateStrategy(scipy.optimize._hessian_update_strategy.HessianUpdateStrategy)\n",
      "        scipy.optimize._hessian_update_strategy.BFGS\n",
      "        scipy.optimize._hessian_update_strategy.SR1\n",
      "    scipy.sparse.linalg.interface.LinearOperator(builtins.object)\n",
      "        scipy.optimize.lbfgsb.LbfgsInvHessProduct\n",
      "    \n",
      "    class BFGS(FullHessianUpdateStrategy)\n",
      "     |  BFGS(exception_strategy='skip_update', min_curvature=None, init_scale='auto')\n",
      "     |  \n",
      "     |  Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  exception_strategy : {'skip_update', 'damp_update'}, optional\n",
      "     |      Define how to proceed when the curvature condition is violated.\n",
      "     |      Set it to 'skip_update' to just skip the update. Or, alternatively,\n",
      "     |      set it to 'damp_update' to interpolate between the actual BFGS\n",
      "     |      result and the unmodified matrix. Both exceptions strategies\n",
      "     |      are explained  in [1]_, p.536-537.\n",
      "     |  min_curvature : float\n",
      "     |      This number, scaled by a normalization factor, defines the\n",
      "     |      minimum curvature ``dot(delta_grad, delta_x)`` allowed to go\n",
      "     |      unaffected by the exception strategy. By default is equal to\n",
      "     |      1e-8 when ``exception_strategy = 'skip_update'`` and equal\n",
      "     |      to 0.2 when ``exception_strategy = 'damp_update'``.\n",
      "     |  init_scale : {float, 'auto'}\n",
      "     |      Matrix scale at first iteration. At the first\n",
      "     |      iteration the Hessian matrix or its inverse will be initialized\n",
      "     |      with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      By default uses 'auto'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.140.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BFGS\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, exception_strategy='skip_update', min_curvature=None, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Bounds(builtins.object)\n",
      "     |  Bounds(lb, ub, keep_feasible=False)\n",
      "     |  \n",
      "     |  Bounds constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= x <= ub\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  lb, ub : array_like, optional\n",
      "     |      Lower and upper bounds on independent variables. Each array must\n",
      "     |      have the same size as x or be a scalar, in which case a bound will be\n",
      "     |      the same for all the variables. Set components of `lb` and `ub` equal\n",
      "     |      to fix a variable. Use ``np.inf`` with an appropriate sign to disable\n",
      "     |      bounds on all or some variables. Note that you can mix constraints of\n",
      "     |      different types: interval, one-sided or equality, by setting different\n",
      "     |      components of `lb` and `ub` as necessary.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lb, ub, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class HessianUpdateStrategy(builtins.object)\n",
      "     |  Interface for implementing Hessian update strategies.\n",
      "     |  \n",
      "     |  Many optimization methods make use of Hessian (or inverse Hessian)\n",
      "     |  approximations, such as the quasi-Newton methods BFGS, SR1, L-BFGS.\n",
      "     |  Some of these  approximations, however, do not actually need to store\n",
      "     |  the entire matrix or can compute the internal matrix product with a\n",
      "     |  given vector in a very efficiently manner. This class serves as an\n",
      "     |  abstract interface between the optimization algorithm and the\n",
      "     |  quasi-Newton update strategies, giving freedom of implementation\n",
      "     |  to store and update the internal matrix as efficiently as possible.\n",
      "     |  Different choices of initialization and update procedure will result\n",
      "     |  in different quasi-Newton strategies.\n",
      "     |  \n",
      "     |  Four methods should be implemented in derived classes: ``initialize``,\n",
      "     |  ``update``, ``dot`` and ``get_matrix``.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Any instance of a class that implements this interface,\n",
      "     |  can be accepted by the method ``minimize`` and used by\n",
      "     |  the compatible solvers to approximate the Hessian (or\n",
      "     |  inverse Hessian) used by the optimization algorithms.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      H : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian\n",
      "     |          or its inverse (depending on how 'approx_type'\n",
      "     |          is defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LbfgsInvHessProduct(scipy.sparse.linalg.interface.LinearOperator)\n",
      "     |  LbfgsInvHessProduct(*args, **kwargs)\n",
      "     |  \n",
      "     |  Linear operator for the L-BFGS approximate inverse Hessian.\n",
      "     |  \n",
      "     |  This operator computes the product of a vector with the approximate inverse\n",
      "     |  of the Hessian of the objective function, using the L-BFGS limited\n",
      "     |  memory approximation to the inverse Hessian, accumulated during the\n",
      "     |  optimization.\n",
      "     |  \n",
      "     |  Objects of this class implement the ``scipy.sparse.linalg.LinearOperator``\n",
      "     |  interface.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the solution vector.\n",
      "     |      (See [1]).\n",
      "     |  yk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the gradient. (See [1]).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge. \"Updating quasi-Newton matrices with limited\n",
      "     |     storage.\" Mathematics of computation 35.151 (1980): 773-782.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LbfgsInvHessProduct\n",
      "     |      scipy.sparse.linalg.interface.LinearOperator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sk, yk)\n",
      "     |      Construct the operator.\n",
      "     |  \n",
      "     |  todense(self)\n",
      "     |      Return a dense array representation of this operator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arr : ndarray, shape=(n, n)\n",
      "     |          An array with the same shape and containing\n",
      "     |          the same data represented by this `LinearOperator`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __add__(self, x)\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __matmul__(self, other)\n",
      "     |  \n",
      "     |  __mul__(self, x)\n",
      "     |  \n",
      "     |  __neg__(self)\n",
      "     |  \n",
      "     |  __pow__(self, p)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmatmul__(self, other)\n",
      "     |  \n",
      "     |  __rmul__(self, x)\n",
      "     |  \n",
      "     |  __sub__(self, x)\n",
      "     |  \n",
      "     |  adjoint(self)\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  dot(self, x)\n",
      "     |      Matrix-matrix or matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          1-d or 2-d array, representing a vector or matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Ax : array\n",
      "     |          1-d or 2-d array (depending on the shape of x) that represents\n",
      "     |          the result of applying this linear operator on x.\n",
      "     |  \n",
      "     |  matmat(self, X)\n",
      "     |      Matrix-matrix multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*X where A is an MxN linear\n",
      "     |      operator and X dense N*K matrix or ndarray.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          An array with shape (N,K).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,K) depending on\n",
      "     |          the type of the X argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matmat wraps any user-specified matmat routine or overridden\n",
      "     |      _matmat method to ensure that y has the correct type.\n",
      "     |  \n",
      "     |  matvec(self, x)\n",
      "     |      Matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (N,) or (N,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,) or (M,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matvec wraps the user-specified matvec routine or overridden\n",
      "     |      _matvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  rmatmat(self, X)\n",
      "     |      Adjoint matrix-matrix multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y = A^H * x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array, or 2-d array.\n",
      "     |      The default implementation defers to the adjoint.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          A matrix or 2D array.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or 2D array depending on the type of the input.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatmat wraps the user-specified rmatmat routine.\n",
      "     |  \n",
      "     |  rmatvec(self, x)\n",
      "     |      Adjoint matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y = A^H * x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (M,) or (M,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (N,) or (N,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatvec wraps the user-specified rmatvec routine or overridden\n",
      "     |      _rmatvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  transpose(self)\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwargs)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  H\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  T\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  ndim = 2\n",
      "    \n",
      "    class LinearConstraint(builtins.object)\n",
      "     |  LinearConstraint(A, lb, ub, keep_feasible=False)\n",
      "     |  \n",
      "     |  Linear constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= A.dot(x) <= ub\n",
      "     |  \n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and the matrix A has shape (m, n).\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  A : {array_like, sparse matrix}, shape (m, n)\n",
      "     |      Matrix defining the constraint.\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, A, lb, ub, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NonlinearConstraint(builtins.object)\n",
      "     |  NonlinearConstraint(fun, lb, ub, jac='2-point', hess=<scipy.optimize._hessian_update_strategy.BFGS object at 0x7fc05cc40250>, keep_feasible=False, finite_diff_rel_step=None, finite_diff_jac_sparsity=None)\n",
      "     |  \n",
      "     |  Nonlinear constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= fun(x) <= ub\n",
      "     |  \n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and ``fun`` returns a vector with m components.\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fun : callable\n",
      "     |      The function defining the constraint.\n",
      "     |      The signature is ``fun(x) -> array_like, shape (m,)``.\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary.\n",
      "     |  jac : {callable,  '2-point', '3-point', 'cs'}, optional\n",
      "     |      Method of computing the Jacobian matrix (an m-by-n matrix,\n",
      "     |      where element (i, j) is the partial derivative of f[i] with\n",
      "     |      respect to x[j]).  The keywords {'2-point', '3-point',\n",
      "     |      'cs'} select a finite difference scheme for the numerical estimation.\n",
      "     |      A callable must have the following signature:\n",
      "     |      ``jac(x) -> {ndarray, sparse matrix}, shape (m, n)``.\n",
      "     |      Default is '2-point'.\n",
      "     |  hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy, None}, optional\n",
      "     |      Method for computing the Hessian matrix. The keywords\n",
      "     |      {'2-point', '3-point', 'cs'} select a finite difference scheme for\n",
      "     |      numerical  estimation.  Alternatively, objects implementing\n",
      "     |      `HessianUpdateStrategy` interface can be used to approximate the\n",
      "     |      Hessian. Currently available implementations are:\n",
      "     |  \n",
      "     |          - `BFGS` (default option)\n",
      "     |          - `SR1`\n",
      "     |  \n",
      "     |      A callable must return the Hessian matrix of ``dot(fun, v)`` and\n",
      "     |      must have the following signature:\n",
      "     |      ``hess(x, v) -> {LinearOperator, sparse matrix, array_like}, shape (n, n)``.\n",
      "     |      Here ``v`` is ndarray with shape (m,) containing Lagrange multipliers.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  finite_diff_rel_step: None or array_like, optional\n",
      "     |      Relative step size for the finite difference approximation. Default is\n",
      "     |      None, which will select a reasonable value automatically depending\n",
      "     |      on a finite difference scheme.\n",
      "     |  finite_diff_jac_sparsity: {None, array_like, sparse matrix}, optional\n",
      "     |      Defines the sparsity structure of the Jacobian matrix for finite\n",
      "     |      difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "     |      only few non-zero elements in *each* row, providing the sparsity\n",
      "     |      structure will greatly speed up the computations. A zero entry means\n",
      "     |      that a corresponding element in the Jacobian is identically zero.\n",
      "     |      If provided, forces the use of 'lsmr' trust-region solver.\n",
      "     |      If None (default) then dense differencing will be used.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Finite difference schemes {'2-point', '3-point', 'cs'} may be used for\n",
      "     |  approximating either the Jacobian or the Hessian. We, however, do not allow\n",
      "     |  its use for approximating both simultaneously. Hence whenever the Jacobian\n",
      "     |  is estimated via finite-differences, we require the Hessian to be estimated\n",
      "     |  using one of the quasi-Newton strategies.\n",
      "     |  \n",
      "     |  The scheme 'cs' is potentially the most accurate, but requires the function\n",
      "     |  to correctly handles complex inputs and be analytically continuable to the\n",
      "     |  complex plane. The scheme '3-point' is more accurate than '2-point' but\n",
      "     |  requires twice as many operations.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Constrain ``x[0] < sin(x[1]) + 1.9``\n",
      "     |  \n",
      "     |  >>> from scipy.optimize import NonlinearConstraint\n",
      "     |  >>> con = lambda x: x[0] - np.sin(x[1])\n",
      "     |  >>> nlc = NonlinearConstraint(con, -np.inf, 1.9)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fun, lb, ub, jac='2-point', hess=<scipy.optimize._hessian_update_strategy.BFGS object at 0x7fc05cc40250>, keep_feasible=False, finite_diff_rel_step=None, finite_diff_jac_sparsity=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class OptimizeResult(builtins.dict)\n",
      "     |  Represents the optimization result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  x : ndarray\n",
      "     |      The solution of the optimization.\n",
      "     |  success : bool\n",
      "     |      Whether or not the optimizer exited successfully.\n",
      "     |  status : int\n",
      "     |      Termination status of the optimizer. Its value depends on the\n",
      "     |      underlying solver. Refer to `message` for details.\n",
      "     |  message : str\n",
      "     |      Description of the cause of the termination.\n",
      "     |  fun, jac, hess: ndarray\n",
      "     |      Values of objective function, its Jacobian and its Hessian (if\n",
      "     |      available). The Hessians may be approximations, see the documentation\n",
      "     |      of the function in question.\n",
      "     |  hess_inv : object\n",
      "     |      Inverse of the objective function's Hessian; may be an approximation.\n",
      "     |      Not available for all solvers. The type of this attribute may be\n",
      "     |      either np.ndarray or scipy.sparse.linalg.LinearOperator.\n",
      "     |  nfev, njev, nhev : int\n",
      "     |      Number of evaluations of the objective functions and of its\n",
      "     |      Jacobian and Hessian.\n",
      "     |  nit : int\n",
      "     |      Number of iterations performed by the optimizer.\n",
      "     |  maxcv : float\n",
      "     |      The maximum constraint violation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There may be additional attributes not listed above depending of the\n",
      "     |  specific solver. Since this class is essentially a subclass of dict\n",
      "     |  with attribute accessors, one can see which attributes are available\n",
      "     |  using the `keys()` method.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeResult\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delattr__ = __delitem__(self, key, /)\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__ = __setitem__(self, key, value, /)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __reversed__(self, /)\n",
      "     |      Return a reverse iterator over the dict keys.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      D.copy() -> a shallow copy of D\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      "     |  \n",
      "     |  popitem(self, /)\n",
      "     |      Remove and return a (key, value) pair as a 2-tuple.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      "     |      Raises KeyError if the dict is empty.\n",
      "     |  \n",
      "     |  setdefault(self, key, default=None, /)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None, /) from builtins.type\n",
      "     |      Create a new dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class OptimizeWarning(builtins.UserWarning)\n",
      "     |  Base class for warnings generated by user code.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeWarning\n",
      "     |      builtins.UserWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class RootResults(builtins.object)\n",
      "     |  RootResults(root, iterations, function_calls, flag)\n",
      "     |  \n",
      "     |  Represents the root finding result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  root : float\n",
      "     |      Estimated root location.\n",
      "     |  iterations : int\n",
      "     |      Number of iterations needed to find the root.\n",
      "     |  function_calls : int\n",
      "     |      Number of times the function was called.\n",
      "     |  converged : bool\n",
      "     |      True if the routine converged.\n",
      "     |  flag : str\n",
      "     |      Description of the cause of termination.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, iterations, function_calls, flag)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SR1(FullHessianUpdateStrategy)\n",
      "     |  SR1(min_denominator=1e-08, init_scale='auto')\n",
      "     |  \n",
      "     |  Symmetric-rank-1 Hessian update strategy.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  min_denominator : float\n",
      "     |      This number, scaled by a normalization factor,\n",
      "     |      defines the minimum denominator magnitude allowed\n",
      "     |      in the update. When the condition is violated we skip\n",
      "     |      the update. By default uses ``1e-8``.\n",
      "     |  init_scale : {float, 'auto'}, optional\n",
      "     |      Matrix scale at first iteration. At the first\n",
      "     |      iteration the Hessian matrix or its inverse will be initialized\n",
      "     |      with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      By default uses 'auto'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.144-146.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SR1\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, min_denominator=1e-08, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using (extended) Anderson mixing.\n",
      "        \n",
      "        The Jacobian is formed by for a 'best' solution in the space\n",
      "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
      "        inversions and MxN multiplications are required. [Ey]_\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        M : float, optional\n",
      "            Number of previous vectors to retain. Defaults to 5.\n",
      "        w0 : float, optional\n",
      "            Regularization parameter for numerical stability.\n",
      "            Compared to unity, good values of the order of 0.01.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='anderson'`` in particular.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.anderson(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116588, 0.15883789])\n",
      "    \n",
      "    approx_fprime(xk, f, epsilon, *args)\n",
      "        Finite-difference approximation of the gradient of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xk : array_like\n",
      "            The coordinate vector at which to determine the gradient of `f`.\n",
      "        f : callable\n",
      "            The function of which to determine the gradient (partial derivatives).\n",
      "            Should take `xk` as first argument, other arguments to `f` can be\n",
      "            supplied in ``*args``. Should return a scalar, the value of the\n",
      "            function at `xk`.\n",
      "        epsilon : array_like\n",
      "            Increment to `xk` to use for determining the function gradient.\n",
      "            If a scalar, uses the same finite difference delta for all partial\n",
      "            derivatives. If an array, should contain one value per element of\n",
      "            `xk`.\n",
      "        \\*args : args, optional\n",
      "            Any other arguments that are to be passed to `f`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        grad : ndarray\n",
      "            The partial derivatives of `f` to `xk`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        check_grad : Check correctness of gradient function against approx_fprime.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function gradient is determined by the forward finite difference\n",
      "        formula::\n",
      "        \n",
      "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
      "            f'[i] = ---------------------------------\n",
      "                                epsilon[i]\n",
      "        \n",
      "        The main use of `approx_fprime` is in scalar function optimizers like\n",
      "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c0, c1):\n",
      "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
      "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
      "        \n",
      "        >>> x = np.ones(2)\n",
      "        >>> c0, c1 = (1, 200)\n",
      "        >>> eps = np.sqrt(np.finfo(float).eps)\n",
      "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
      "        array([   2.        ,  400.00004198])\n",
      "    \n",
      "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None, seed=None)\n",
      "        Find the global minimum of a function using the basin-hopping algorithm\n",
      "        \n",
      "        Basin-hopping is a two-phase method that combines a global stepping\n",
      "        algorithm with local minimization at each step. Designed to mimic\n",
      "        the natural process of energy minimization of clusters of atoms, it works\n",
      "        well for similar problems with \"funnel-like, but rugged\" energy landscapes\n",
      "        [5]_.\n",
      "        \n",
      "        As the step-taking, step acceptance, and minimization methods are all\n",
      "        customizable, this function can also be used to implement other two-phase\n",
      "        methods.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            Function to be optimized.  ``args`` can be passed as an optional item\n",
      "            in the dict ``minimizer_kwargs``\n",
      "        x0 : array_like\n",
      "            Initial guess.\n",
      "        niter : integer, optional\n",
      "            The number of basin-hopping iterations. There will be a total of\n",
      "            ``niter + 1`` runs of the local minimizer.\n",
      "        T : float, optional\n",
      "            The \"temperature\" parameter for the accept or reject criterion. Higher\n",
      "            \"temperatures\" mean that larger jumps in function value will be\n",
      "            accepted.  For best results ``T`` should be comparable to the\n",
      "            separation (in function value) between local minima.\n",
      "        stepsize : float, optional\n",
      "            Maximum step size for use in the random displacement.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the local minimizer\n",
      "            ``scipy.optimize.minimize()`` Some important options could be:\n",
      "        \n",
      "                method : str\n",
      "                    The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
      "                args : tuple\n",
      "                    Extra arguments passed to the objective function (``func``) and\n",
      "                    its derivatives (Jacobian, Hessian).\n",
      "        \n",
      "        take_step : callable ``take_step(x)``, optional\n",
      "            Replace the default step-taking routine with this routine. The default\n",
      "            step-taking routine is a random displacement of the coordinates, but\n",
      "            other step-taking algorithms may be better for some systems.\n",
      "            ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
      "            If this attribute exists, then ``basinhopping`` will adjust\n",
      "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
      "            search.\n",
      "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
      "            Define a test which will be used to judge whether or not to accept the\n",
      "            step.  This will be used in addition to the Metropolis test based on\n",
      "            \"temperature\" ``T``.  The acceptable return values are True,\n",
      "            False, or ``\"force accept\"``. If any of the tests return False\n",
      "            then the step is rejected. If the latter, then this will override any\n",
      "            other tests in order to accept the step. This can be used, for example,\n",
      "            to forcefully escape from a local minimum that ``basinhopping`` is\n",
      "            trapped in.\n",
      "        callback : callable, ``callback(x, f, accept)``, optional\n",
      "            A callback function which will be called for all minima found. ``x``\n",
      "            and ``f`` are the coordinates and function value of the trial minimum,\n",
      "            and ``accept`` is whether or not that minimum was accepted. This can\n",
      "            be used, for example, to save the lowest N minima found. Also,\n",
      "            ``callback`` can be used to specify a user defined stop criterion by\n",
      "            optionally returning True to stop the ``basinhopping`` routine.\n",
      "        interval : integer, optional\n",
      "            interval for how often to update the ``stepsize``\n",
      "        disp : bool, optional\n",
      "            Set to True to print status messages\n",
      "        niter_success : integer, optional\n",
      "            Stop the run if the global minimum candidate remains the same for this\n",
      "            number of iterations.\n",
      "        seed : {int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "            If `seed` is not specified the `~np.random.RandomState` singleton is\n",
      "            used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
      "            with seed.\n",
      "            If `seed` is already a ``RandomState`` or ``Generator`` instance, then\n",
      "            that object is used.\n",
      "            Specify `seed` for repeatable minimizations. The random numbers\n",
      "            generated with this seed only affect the default Metropolis\n",
      "            `accept_test` and the default `take_step`. If you supply your own\n",
      "            `take_step` and `accept_test`, and these functions use random\n",
      "            number generation, then those functions are responsible for the state\n",
      "            of their random number generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination. The ``OptimizeResult`` object returned by the\n",
      "            selected minimizer at the lowest minimum is also contained within this\n",
      "            object and can be accessed through the ``lowest_optimization_result``\n",
      "            attribute.  See `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize :\n",
      "            The local minimization function called once for each basinhopping step.\n",
      "            ``minimizer_kwargs`` is passed to this routine.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
      "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
      "        [4]_. The algorithm in its current form was described by David Wales and\n",
      "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
      "        \n",
      "        The algorithm is iterative with each cycle composed of the following\n",
      "        features\n",
      "        \n",
      "        1) random perturbation of the coordinates\n",
      "        \n",
      "        2) local minimization\n",
      "        \n",
      "        3) accept or reject the new coordinates based on the minimized function\n",
      "           value\n",
      "        \n",
      "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
      "        Carlo algorithms, although there are many other possibilities [3]_.\n",
      "        \n",
      "        This global minimization method has been shown to be extremely efficient\n",
      "        for a wide variety of problems in physics and chemistry. It is\n",
      "        particularly useful when the function has many minima separated by large\n",
      "        barriers. See the Cambridge Cluster Database\n",
      "        http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
      "        that have been optimized primarily using basin-hopping. This database\n",
      "        includes minimization problems exceeding 300 degrees of freedom.\n",
      "        \n",
      "        See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
      "        a Fortran implementation of basin-hopping. This implementation has many\n",
      "        different variations of the procedure described above, including more\n",
      "        advanced step taking algorithms and alternate acceptance criterion.\n",
      "        \n",
      "        For stochastic global optimization there is no way to determine if the true\n",
      "        global minimum has actually been found. Instead, as a consistency check,\n",
      "        the algorithm can be run from a number of different random starting points\n",
      "        to ensure the lowest minimum found in each example has converged to the\n",
      "        global minimum. For this reason, ``basinhopping`` will by default simply\n",
      "        run for the number of iterations ``niter`` and return the lowest minimum\n",
      "        found. It is left to the user to ensure that this is in fact the global\n",
      "        minimum.\n",
      "        \n",
      "        Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
      "        depends on the problem being solved. The step is chosen uniformly in the\n",
      "        region from x0-stepsize to x0+stepsize, in each dimension. Ideally, it\n",
      "        should be comparable to the typical separation (in argument values) between\n",
      "        local minima of the function being optimized. ``basinhopping`` will, by\n",
      "        default, adjust ``stepsize`` to find an optimal value, but this may take\n",
      "        many iterations. You will get quicker results if you set a sensible\n",
      "        initial value for ``stepsize``.\n",
      "        \n",
      "        Choosing ``T``: The parameter ``T`` is the \"temperature\" used in the\n",
      "        Metropolis criterion. Basinhopping steps are always accepted if\n",
      "        ``func(xnew) < func(xold)``. Otherwise, they are accepted with\n",
      "        probability::\n",
      "        \n",
      "            exp( -(func(xnew) - func(xold)) / T )\n",
      "        \n",
      "        So, for best results, ``T`` should to be comparable to the typical\n",
      "        difference (in function values) between local minima. (The height of\n",
      "        \"walls\" between local minima is irrelevant.)\n",
      "        \n",
      "        If ``T`` is 0, the algorithm becomes Monotonic Basin-Hopping, in which all\n",
      "        steps that increase energy are rejected.\n",
      "        \n",
      "        .. versionadded:: 0.12.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
      "            Cambridge, UK.\n",
      "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
      "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
      "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
      "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
      "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
      "            1987, 84, 6611.\n",
      "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
      "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
      "        .. [5] Olson, B., Hashmi, I., Molloy, K., and Shehu1, A., Basin Hopping as\n",
      "            a General and Versatile Optimization Framework for the Characterization\n",
      "            of Biological Macromolecules, Advances in Artificial Intelligence,\n",
      "            Volume 2012 (2012), Article ID 674832, :doi:`10.1155/2012/674832`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 1-D minimization problem, with many\n",
      "        local minima superimposed on a parabola.\n",
      "        \n",
      "        >>> from scipy.optimize import basinhopping\n",
      "        >>> func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
      "        >>> x0=[1.]\n",
      "        \n",
      "        Basinhopping, internally, uses a local minimization algorithm. We will use\n",
      "        the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
      "        use and how to set up that minimizer. This parameter will be passed to\n",
      "        ``scipy.optimize.minimize()``.\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
      "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
      "        global minimum: x = -0.1951, f(x0) = -1.0009\n",
      "        \n",
      "        Next consider a 2-D minimization problem. Also, this time, we\n",
      "        will use gradient information to significantly speed up the search.\n",
      "        \n",
      "        >>> def func2d(x):\n",
      "        ...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
      "        ...                                                            0.2) * x[0]\n",
      "        ...     df = np.zeros(2)\n",
      "        ...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
      "        ...     df[1] = 2. * x[1] + 0.2\n",
      "        ...     return f, df\n",
      "        \n",
      "        We'll also use a different local minimization algorithm. Also, we must tell\n",
      "        the minimizer that our function returns both energy and gradient (Jacobian).\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
      "        >>> x0 = [1.0, 1.0]\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Here is an example using a custom step-taking routine. Imagine you want\n",
      "        the first coordinate to take larger steps than the rest of the coordinates.\n",
      "        This can be implemented like so:\n",
      "        \n",
      "        >>> class MyTakeStep(object):\n",
      "        ...    def __init__(self, stepsize=0.5):\n",
      "        ...        self.stepsize = stepsize\n",
      "        ...    def __call__(self, x):\n",
      "        ...        s = self.stepsize\n",
      "        ...        x[0] += np.random.uniform(-2.*s, 2.*s)\n",
      "        ...        x[1:] += np.random.uniform(-s, s, x[1:].shape)\n",
      "        ...        return x\n",
      "        \n",
      "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
      "        of ``stepsize`` to optimize the search. We'll use the same 2-D function as\n",
      "        before\n",
      "        \n",
      "        >>> mytakestep = MyTakeStep()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200, take_step=mytakestep)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Now, let's do an example using a custom callback function which prints the\n",
      "        value of every minimum found\n",
      "        \n",
      "        >>> def print_fun(x, f, accepted):\n",
      "        ...         print(\"at minimum %.4f accepted %d\" % (f, int(accepted)))\n",
      "        \n",
      "        We'll run it for only 10 basinhopping steps this time.\n",
      "        \n",
      "        >>> np.random.seed(1)\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, callback=print_fun)\n",
      "        at minimum 0.4159 accepted 1\n",
      "        at minimum 0.4159 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum 0.9102 accepted 1\n",
      "        at minimum 0.9102 accepted 1\n",
      "        at minimum 2.2945 accepted 0\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        \n",
      "        \n",
      "        The minimum at -1.0109 is actually the global minimum, found already on the\n",
      "        8th iteration.\n",
      "        \n",
      "        Now let's implement bounds on the problem using a custom ``accept_test``:\n",
      "        \n",
      "        >>> class MyBounds(object):\n",
      "        ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
      "        ...         self.xmax = np.array(xmax)\n",
      "        ...         self.xmin = np.array(xmin)\n",
      "        ...     def __call__(self, **kwargs):\n",
      "        ...         x = kwargs[\"x_new\"]\n",
      "        ...         tmax = bool(np.all(x <= self.xmax))\n",
      "        ...         tmin = bool(np.all(x >= self.xmin))\n",
      "        ...         return tmax and tmin\n",
      "        \n",
      "        >>> mybounds = MyBounds()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, accept_test=mybounds)\n",
      "    \n",
      "    bisect(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find root of a function within an interval using bisection.\n",
      "        \n",
      "        Basic bisection routine to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. `f(a)` and `f(b)` cannot have the same signs.\n",
      "        Slow but sure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  `f` must be continuous, and\n",
      "            f(a) and f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in a `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.bisect(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        >>> root = optimize.bisect(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        fsolve : n-dimensional root-finding\n",
      "    \n",
      "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
      "        Bracket the minimum of the function.\n",
      "        \n",
      "        Given a function and distinct initial points, search in the\n",
      "        downhill direction (as defined by the initial points) and return\n",
      "        new points xa, xb, xc that bracket the minimum of the function\n",
      "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
      "        solution will satisfy xa<=x<=xb.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to minimize.\n",
      "        xa, xb : float, optional\n",
      "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to `func`.\n",
      "        grow_limit : float, optional\n",
      "            Maximum grow limit.  Defaults to 110.0\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Defaults to 1000.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xa, xb, xc : float\n",
      "            Bracket.\n",
      "        fa, fb, fc : float\n",
      "            Objective function values in bracket.\n",
      "        funcalls : int\n",
      "            Number of function evaluations made.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        This function can find a downward convex region of a function:\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import bracket\n",
      "        >>> def f(x):\n",
      "        ...     return 10*x**2 + 3*x + 5\n",
      "        >>> x = np.linspace(-2, 2)\n",
      "        >>> y = f(x)\n",
      "        >>> init_xa, init_xb = 0, 1\n",
      "        >>> xa, xb, xc, fa, fb, fc, funcalls = bracket(f, xa=init_xa, xb=init_xb)\n",
      "        >>> plt.axvline(x=init_xa, color=\"k\", linestyle=\"--\")\n",
      "        >>> plt.axvline(x=init_xb, color=\"k\", linestyle=\"--\")\n",
      "        >>> plt.plot(x, y, \"-k\")\n",
      "        >>> plt.plot(xa, fa, \"bx\")\n",
      "        >>> plt.plot(xb, fb, \"rx\")\n",
      "        >>> plt.plot(xc, fc, \"bx\")\n",
      "        >>> plt.show()\n",
      "    \n",
      "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
      "        Given a function of one variable and a possible bracket, return\n",
      "        the local minimum of the function isolated to a fractional precision\n",
      "        of tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present).\n",
      "        brack : tuple, optional\n",
      "            Either a triple (xa,xb,xc) where xa<xb<xc and func(xb) <\n",
      "            func(xa), func(xc) or a pair (xa,xb) which are used as a\n",
      "            starting interval for a downhill bracket search (see\n",
      "            `bracket`). Providing the pair (xa,xb) does not always mean\n",
      "            the obtained solution will satisfy xa<=x<=xb.\n",
      "        tol : float, optional\n",
      "            Stop if between iteration change is less than `tol`.\n",
      "        full_output : bool, optional\n",
      "            If True, return all output args (xmin, fval, iter,\n",
      "            funcalls).\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations in solution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xmin : ndarray\n",
      "            Optimum point.\n",
      "        fval : float\n",
      "            Optimum value.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of objective function evaluations made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Brent' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses inverse parabolic interpolation when possible to speed up\n",
      "        convergence of golden section method.\n",
      "        \n",
      "        Does not ensure that the minimum lies in the range specified by\n",
      "        `brack`. See `fminbound`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3 respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range (xa,xb).\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.brent(f,brack=(1,2))\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.brent(f,brack=(-1,0.5,2))\n",
      "        >>> minimum\n",
      "        -2.7755575615628914e-17\n",
      "    \n",
      "    brenth(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's\n",
      "        method with hyperbolic extrapolation.\n",
      "        \n",
      "        A variation on the classic Brent routine to find a zero of the function f\n",
      "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
      "        inverse quadratic extrapolation. There was a paper back in the 1980's ...\n",
      "        f(a) and f(b) cannot have the same signs. Generally, on a par with the\n",
      "        brent routine, but not as heavily tested. It is a safe version of the\n",
      "        secant method that uses hyperbolic extrapolation. The version here is by\n",
      "        Chuck Harris.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. As with `brentq`, for nice\n",
      "            functions the method will often satisfy the above condition\n",
      "            with ``xtol/2`` and ``rtol/2``.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. As with `brentq`, for nice functions\n",
      "            the method will often satisfy the above condition with\n",
      "            ``xtol/2`` and ``rtol/2``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.brenth(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        >>> root = optimize.brenth(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fmin, fmin_powell, fmin_cg,\n",
      "               fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
      "        \n",
      "        leastsq : nonlinear least squares minimizer\n",
      "        \n",
      "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
      "        \n",
      "        basinhopping, differential_evolution, brute : global optimizers\n",
      "        \n",
      "        fminbound, brent, golden, bracket : local scalar minimizers\n",
      "        \n",
      "        fsolve : N-D root-finding\n",
      "        \n",
      "        brentq, brenth, ridder, bisect, newton : 1-D root-finding\n",
      "        \n",
      "        fixed_point : scalar fixed-point finder\n",
      "    \n",
      "    brentq(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's method.\n",
      "        \n",
      "        Uses the classic Brent's method to find a zero of the function `f` on\n",
      "        the sign changing interval [a , b]. Generally considered the best of the\n",
      "        rootfinding routines here. It is a safe version of the secant method that\n",
      "        uses inverse quadratic extrapolation. Brent's method combines root\n",
      "        bracketing, interval bisection, and inverse quadratic interpolation. It is\n",
      "        sometimes known as the van Wijngaarden-Dekker-Brent method. Brent (1973)\n",
      "        claims convergence is guaranteed for functions computable within [a,b].\n",
      "        \n",
      "        [Brent1973]_ provides the classic description of the algorithm. Another\n",
      "        description can be found in a recent edition of Numerical Recipes, including\n",
      "        [PressEtal1992]_. A third description is at\n",
      "        http://mathworld.wolfram.com/BrentsMethod.html. It should be easy to\n",
      "        understand the algorithm just by reading our code. Our code diverges a bit\n",
      "        from standard presentations: we choose a different formula for the\n",
      "        extrapolation step.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)` must\n",
      "            have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval :math:`[a, b]`.\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval :math:`[a, b]`.\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
      "        \n",
      "        Related functions fall into several classes:\n",
      "        \n",
      "        multivariate local optimizers\n",
      "          `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
      "        nonlinear least squares minimizer\n",
      "          `leastsq`\n",
      "        constrained multivariate optimizers\n",
      "          `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
      "        global optimizers\n",
      "          `basinhopping`, `brute`, `differential_evolution`\n",
      "        local scalar minimizers\n",
      "          `fminbound`, `brent`, `golden`, `bracket`\n",
      "        N-D root-finding\n",
      "          `fsolve`\n",
      "        1-D root-finding\n",
      "          `brenth`, `ridder`, `bisect`, `newton`\n",
      "        scalar fixed-point finder\n",
      "          `fixed_point`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Brent1973]\n",
      "           Brent, R. P.,\n",
      "           *Algorithms for Minimization Without Derivatives*.\n",
      "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
      "        \n",
      "        .. [PressEtal1992]\n",
      "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
      "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
      "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
      "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.brentq(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        >>> root = optimize.brentq(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "    \n",
      "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \\\"Broyden's good method\\\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='broyden1'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
      "        \n",
      "        which corresponds to Broyden's first Jacobian update\n",
      "        \n",
      "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \\\"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\\\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.broyden1(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116396, 0.15883641])\n",
      "    \n",
      "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \"Broyden's bad method\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='broyden2'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
      "        \n",
      "        corresponding to Broyden's second method.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.broyden2(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116365, 0.15883529])\n",
      "    \n",
      "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin at 0x7fc05cc1ee50>, disp=False, workers=1)\n",
      "        Minimize a function over a given range by brute force.\n",
      "        \n",
      "        Uses the \"brute force\" method, i.e., computes the function's value\n",
      "        at each point of a multidimensional grid of points, to find the global\n",
      "        minimum of the function.\n",
      "        \n",
      "        The function is evaluated everywhere in the range with the datatype of the\n",
      "        first call to the function, as enforced by the ``vectorize`` NumPy\n",
      "        function. The value and type of the function evaluation returned when\n",
      "        ``full_output=True`` are affected in addition by the ``finish`` argument\n",
      "        (see Notes).\n",
      "        \n",
      "        The brute force approach is inefficient because the number of grid points\n",
      "        increases exponentially - the number of grid points to evaluate is\n",
      "        ``Ns ** len(x)``. Consequently, even with coarse grid spacing, even\n",
      "        moderately sized problems can take a long time to run, and/or run into\n",
      "        memory limitations.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the\n",
      "            form ``f(x, *args)``, where ``x`` is the argument in\n",
      "            the form of a 1-D array and ``args`` is a tuple of any\n",
      "            additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        ranges : tuple\n",
      "            Each component of the `ranges` tuple must be either a\n",
      "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
      "            The program uses these to create the grid of points on which\n",
      "            the objective function will be computed. See `Note 2` for\n",
      "            more detail.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        Ns : int, optional\n",
      "            Number of grid points along the axes, if not otherwise\n",
      "            specified. See `Note2`.\n",
      "        full_output : bool, optional\n",
      "            If True, return the evaluation grid and the objective function's\n",
      "            values on it.\n",
      "        finish : callable, optional\n",
      "            An optimization function that is called with the result of brute force\n",
      "            minimization as initial guess. `finish` should take `func` and\n",
      "            the initial guess as positional arguments, and take `args` as\n",
      "            keyword arguments. It may additionally take `full_output`\n",
      "            and/or `disp` as keyword arguments. Use None if no \"polishing\"\n",
      "            function is to be used. See Notes for more details.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages from the `finish` callable.\n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the grid is subdivided into `workers`\n",
      "            sections and evaluated in parallel (uses\n",
      "            `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply `-1` to use all cores available to the Process.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the grid in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            Requires that `func` be pickleable.\n",
      "        \n",
      "            .. versionadded:: 1.3.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : ndarray\n",
      "            A 1-D array containing the coordinates of a point at which the\n",
      "            objective function had its minimum value. (See `Note 1` for\n",
      "            which point is returned.)\n",
      "        fval : float\n",
      "            Function value at the point `x0`. (Returned when `full_output` is\n",
      "            True.)\n",
      "        grid : tuple\n",
      "            Representation of the evaluation grid. It has the same\n",
      "            length as `x0`. (Returned when `full_output` is True.)\n",
      "        Jout : ndarray\n",
      "            Function values at each point of the evaluation\n",
      "            grid, i.e., ``Jout = func(*grid)``. (Returned\n",
      "            when `full_output` is True.)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        basinhopping, differential_evolution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
      "        of the objective function occurs. If `finish` is None, that is the\n",
      "        point returned. When the global minimum occurs within (or not very far\n",
      "        outside) the grid's boundaries, and the grid is fine enough, that\n",
      "        point will be in the neighborhood of the global minimum.\n",
      "        \n",
      "        However, users often employ some other optimization program to\n",
      "        \"polish\" the gridpoint values, i.e., to seek a more precise\n",
      "        (local) minimum near `brute's` best gridpoint.\n",
      "        The `brute` function's `finish` option provides a convenient way to do\n",
      "        that. Any polishing program used must take `brute's` output as its\n",
      "        initial guess as a positional argument, and take `brute's` input values\n",
      "        for `args` as keyword arguments, otherwise an error will be raised.\n",
      "        It may additionally take `full_output` and/or `disp` as keyword arguments.\n",
      "        \n",
      "        `brute` assumes that the `finish` function returns either an\n",
      "        `OptimizeResult` object or a tuple in the form:\n",
      "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing\n",
      "        value of the argument, ``Jmin`` is the minimum value of the objective\n",
      "        function, \"...\" may be some other returned values (which are not used\n",
      "        by `brute`), and ``statuscode`` is the status code of the `finish` program.\n",
      "        \n",
      "        Note that when `finish` is not None, the values returned are those\n",
      "        of the `finish` program, *not* the gridpoint ones. Consequently,\n",
      "        while `brute` confines its search to the input grid points,\n",
      "        the `finish` program's results usually will not coincide with any\n",
      "        gridpoint, and may fall outside the grid's boundary. Thus, if a\n",
      "        minimum only needs to be found over the provided grid points, make\n",
      "        sure to pass in `finish=None`.\n",
      "        \n",
      "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
      "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
      "        Each component of the `ranges` tuple can be either a slice object or a\n",
      "        two-tuple giving a range of values, such as (0, 5). If the component is a\n",
      "        slice object, `brute` uses it directly. If the component is a two-tuple\n",
      "        range, `brute` internally converts it to a slice object that interpolates\n",
      "        `Ns` points from its low-value to its high-value, inclusive.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the use of `brute` to seek the global minimum of a function\n",
      "        of two variables that is given as the sum of a positive-definite\n",
      "        quadratic and two deep \"Gaussian-shaped\" craters. Specifically, define\n",
      "        the objective function `f` as the sum of three other functions,\n",
      "        ``f = f1 + f2 + f3``. We suppose each of these has a signature\n",
      "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
      "        are as defined below.\n",
      "        \n",
      "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
      "        >>> def f1(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
      "        \n",
      "        >>> def f2(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
      "        \n",
      "        >>> def f3(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
      "        \n",
      "        >>> def f(z, *params):\n",
      "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
      "        \n",
      "        Thus, the objective function may have local minima near the minimum\n",
      "        of each of the three functions of which it is composed. To\n",
      "        use `fmin` to polish its gridpoint result, we may then continue as\n",
      "        follows:\n",
      "        \n",
      "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
      "        >>> from scipy import optimize\n",
      "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
      "        ...                           finish=optimize.fmin)\n",
      "        >>> resbrute[0]  # global minimum\n",
      "        array([-1.05665192,  1.80834843])\n",
      "        >>> resbrute[1]  # function value at global minimum\n",
      "        -3.4085818767\n",
      "        \n",
      "        Note that if `finish` had been set to None, we would have gotten the\n",
      "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
      "    \n",
      "    check_grad(func, grad, x0, *args, **kwargs)\n",
      "        Check the correctness of a gradient function by comparing it against a\n",
      "        (forward) finite-difference approximation of the gradient.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x0, *args)``\n",
      "            Function whose derivative is to be checked.\n",
      "        grad : callable ``grad(x0, *args)``\n",
      "            Gradient of `func`.\n",
      "        x0 : ndarray\n",
      "            Points to check `grad` against forward difference approximation of grad\n",
      "            using `func`.\n",
      "        args : \\*args, optional\n",
      "            Extra arguments passed to `func` and `grad`.\n",
      "        epsilon : float, optional\n",
      "            Step size used for the finite difference approximation. It defaults to\n",
      "            ``sqrt(np.finfo(float).eps)``, which is approximately 1.49e-08.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        err : float\n",
      "            The square root of the sum of squares (i.e., the 2-norm) of the\n",
      "            difference between ``grad(x0, *args)`` and the finite difference\n",
      "            approximation of `grad` using func at the points `x0`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        approx_fprime\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def func(x):\n",
      "        ...     return x[0]**2 - 0.5 * x[1]**3\n",
      "        >>> def grad(x):\n",
      "        ...     return [2 * x[0], -1.5 * x[1]**2]\n",
      "        >>> from scipy.optimize import check_grad\n",
      "        >>> check_grad(func, grad, [1.5, -1.5])\n",
      "        2.9802322387695312e-08\n",
      "    \n",
      "    curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=True, bounds=(-inf, inf), method=None, jac=None, **kwargs)\n",
      "        Use non-linear least squares to fit a function, f, to data.\n",
      "        \n",
      "        Assumes ``ydata = f(xdata, *params) + eps``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            The model function, f(x, ...). It must take the independent\n",
      "            variable as the first argument and the parameters to fit as\n",
      "            separate remaining arguments.\n",
      "        xdata : array_like or object\n",
      "            The independent variable where the data is measured.\n",
      "            Should usually be an M-length sequence or an (k,M)-shaped array for\n",
      "            functions with k predictors, but can actually be any object.\n",
      "        ydata : array_like\n",
      "            The dependent data, a length M array - nominally ``f(xdata, ...)``.\n",
      "        p0 : array_like, optional\n",
      "            Initial guess for the parameters (length N). If None, then the\n",
      "            initial values will all be 1 (if the number of parameters for the\n",
      "            function can be determined using introspection, otherwise a\n",
      "            ValueError is raised).\n",
      "        sigma : None or M-length sequence or MxM array, optional\n",
      "            Determines the uncertainty in `ydata`. If we define residuals as\n",
      "            ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n",
      "            depends on its number of dimensions:\n",
      "        \n",
      "                - A 1-D `sigma` should contain values of standard deviations of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = sum((r / sigma) ** 2)``.\n",
      "        \n",
      "                - A 2-D `sigma` should contain the covariance matrix of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = r.T @ inv(sigma) @ r``.\n",
      "        \n",
      "                  .. versionadded:: 0.19\n",
      "        \n",
      "            None (default) is equivalent of 1-D `sigma` filled with ones.\n",
      "        absolute_sigma : bool, optional\n",
      "            If True, `sigma` is used in an absolute sense and the estimated parameter\n",
      "            covariance `pcov` reflects these absolute values.\n",
      "        \n",
      "            If False (default), only the relative magnitudes of the `sigma` values matter.\n",
      "            The returned parameter covariance matrix `pcov` is based on scaling\n",
      "            `sigma` by a constant factor. This constant is set by demanding that the\n",
      "            reduced `chisq` for the optimal parameters `popt` when using the\n",
      "            *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n",
      "            match the sample variance of the residuals after the fit. Default is False.\n",
      "            Mathematically,\n",
      "            ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\n",
      "        check_finite : bool, optional\n",
      "            If True, check that the input arrays do not contain nans of infs,\n",
      "            and raise a ValueError if they do. Setting this parameter to\n",
      "            False may silently produce nonsensical results if the input arrays\n",
      "            do contain nans. Default is True.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on parameters. Defaults to no bounds.\n",
      "            Each element of the tuple must be either an array with the length equal\n",
      "            to the number of parameters, or a scalar (in which case the bound is\n",
      "            taken to be the same for all parameters). Use ``np.inf`` with an\n",
      "            appropriate sign to disable bounds on all or some parameters.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        method : {'lm', 'trf', 'dogbox'}, optional\n",
      "            Method to use for optimization. See `least_squares` for more details.\n",
      "            Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n",
      "            provided. The method 'lm' won't work when the number of observations\n",
      "            is less than the number of variables, use 'trf' or 'dogbox' in this\n",
      "            case.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        jac : callable, string or None, optional\n",
      "            Function with signature ``jac(x, ...)`` which computes the Jacobian\n",
      "            matrix of the model function with respect to parameters as a dense\n",
      "            array_like structure. It will be scaled according to provided `sigma`.\n",
      "            If None (default), the Jacobian will be estimated numerically.\n",
      "            String keywords for 'trf' and 'dogbox' methods can be used to select\n",
      "            a finite difference scheme, see `least_squares`.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        kwargs\n",
      "            Keyword arguments passed to `leastsq` for ``method='lm'`` or\n",
      "            `least_squares` otherwise.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        popt : array\n",
      "            Optimal values for the parameters so that the sum of the squared\n",
      "            residuals of ``f(xdata, *popt) - ydata`` is minimized.\n",
      "        pcov : 2-D array\n",
      "            The estimated covariance of popt. The diagonals provide the variance\n",
      "            of the parameter estimate. To compute one standard deviation errors\n",
      "            on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\n",
      "        \n",
      "            How the `sigma` parameter affects the estimated covariance\n",
      "            depends on `absolute_sigma` argument, as described above.\n",
      "        \n",
      "            If the Jacobian matrix at the solution doesn't have a full rank, then\n",
      "            'lm' method returns a matrix filled with ``np.inf``, on the other hand\n",
      "            'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n",
      "            the covariance matrix.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            if either `ydata` or `xdata` contain NaNs, or if incompatible options\n",
      "            are used.\n",
      "        \n",
      "        RuntimeError\n",
      "            if the least-squares minimization fails.\n",
      "        \n",
      "        OptimizeWarning\n",
      "            if covariance of the parameters can not be estimated.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Minimize the sum of squares of nonlinear functions.\n",
      "        scipy.stats.linregress : Calculate a linear least squares regression for\n",
      "                                 two sets of measurements.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\n",
      "        through `leastsq`. Note that this algorithm can only deal with\n",
      "        unconstrained problems.\n",
      "        \n",
      "        Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\n",
      "        the docstring of `least_squares` for more information.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import curve_fit\n",
      "        \n",
      "        >>> def func(x, a, b, c):\n",
      "        ...     return a * np.exp(-b * x) + c\n",
      "        \n",
      "        Define the data to be fit with some noise:\n",
      "        \n",
      "        >>> xdata = np.linspace(0, 4, 50)\n",
      "        >>> y = func(xdata, 2.5, 1.3, 0.5)\n",
      "        >>> np.random.seed(1729)\n",
      "        >>> y_noise = 0.2 * np.random.normal(size=xdata.size)\n",
      "        >>> ydata = y + y_noise\n",
      "        >>> plt.plot(xdata, ydata, 'b-', label='data')\n",
      "        \n",
      "        Fit for the parameters a, b, c of the function `func`:\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata)\n",
      "        >>> popt\n",
      "        array([ 2.55423706,  1.35190947,  0.47450618])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'r-',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "        \n",
      "        Constrain the optimization to the region of ``0 <= a <= 3``,\n",
      "        ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n",
      "        >>> popt\n",
      "        array([ 2.43708906,  1.        ,  0.35015434])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'g--',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "        \n",
      "        >>> plt.xlabel('x')\n",
      "        >>> plt.ylabel('y')\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "    \n",
      "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
      "        \n",
      "        The Jacobian approximation is derived from previous iterations, by\n",
      "        retaining only the diagonal of Broyden matrices.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='diagbroyden'`` in particular.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.diagbroyden(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116403, 0.15883384])\n",
      "    \n",
      "    differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0, updating='immediate', workers=1, constraints=())\n",
      "        Finds the global minimum of a multivariate function.\n",
      "        \n",
      "        Differential Evolution is stochastic in nature (does not use gradient\n",
      "        methods) to find the minimum, and can search large areas of candidate\n",
      "        space, but often requires larger numbers of function evaluations than\n",
      "        conventional gradient-based techniques.\n",
      "        \n",
      "        The algorithm is due to Storn and Price [1]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence or `Bounds`, optional\n",
      "            Bounds for variables. There are two ways to specify the bounds:\n",
      "            1. Instance of `Bounds` class.\n",
      "            2. ``(min, max)`` pairs for each element in ``x``, defining the finite\n",
      "            lower and upper bounds for the optimizing argument of `func`. It is\n",
      "            required to have ``len(bounds) == len(x)``. ``len(bounds)`` is used\n",
      "            to determine the number of parameters in ``x``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to\n",
      "            completely specify the objective function.\n",
      "        strategy : str, optional\n",
      "            The differential evolution strategy to use. Should be one of:\n",
      "        \n",
      "                - 'best1bin'\n",
      "                - 'best1exp'\n",
      "                - 'rand1exp'\n",
      "                - 'randtobest1exp'\n",
      "                - 'currenttobest1exp'\n",
      "                - 'best2exp'\n",
      "                - 'rand2exp'\n",
      "                - 'randtobest1bin'\n",
      "                - 'currenttobest1bin'\n",
      "                - 'best2bin'\n",
      "                - 'rand2bin'\n",
      "                - 'rand1bin'\n",
      "        \n",
      "            The default is 'best1bin'.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of generations over which the entire population is\n",
      "            evolved. The maximum number of function evaluations (with no polishing)\n",
      "            is: ``(maxiter + 1) * popsize * len(x)``\n",
      "        popsize : int, optional\n",
      "            A multiplier for setting the total population size. The population has\n",
      "            ``popsize * len(x)`` individuals (unless the initial population is\n",
      "            supplied via the `init` keyword).\n",
      "        tol : float, optional\n",
      "            Relative tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        mutation : float or tuple(float, float), optional\n",
      "            The mutation constant. In the literature this is also known as\n",
      "            differential weight, being denoted by F.\n",
      "            If specified as a float it should be in the range [0, 2].\n",
      "            If specified as a tuple ``(min, max)`` dithering is employed. Dithering\n",
      "            randomly changes the mutation constant on a generation by generation\n",
      "            basis. The mutation constant for that generation is taken from\n",
      "            ``U[min, max)``. Dithering can help speed convergence significantly.\n",
      "            Increasing the mutation constant increases the search radius, but will\n",
      "            slow down convergence.\n",
      "        recombination : float, optional\n",
      "            The recombination constant, should be in the range [0, 1]. In the\n",
      "            literature this is also known as the crossover probability, being\n",
      "            denoted by CR. Increasing this value allows a larger number of mutants\n",
      "            to progress into the next generation, but at the risk of population\n",
      "            stability.\n",
      "        seed : {int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "            If `seed` is not specified the `~np.random.RandomState` singleton is\n",
      "            used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with seed.\n",
      "            If `seed` is already a ``RandomState`` or a ``Generator`` instance,\n",
      "            then that object is used.\n",
      "            Specify `seed` for repeatable minimizations.\n",
      "        disp : bool, optional\n",
      "            Prints the evaluated `func` at every iteration.\n",
      "        callback : callable, `callback(xk, convergence=val)`, optional\n",
      "            A function to follow the progress of the minimization. ``xk`` is\n",
      "            the current value of ``x0``. ``val`` represents the fractional\n",
      "            value of the population convergence.  When ``val`` is greater than one\n",
      "            the function halts. If callback returns `True`, then the minimization\n",
      "            is halted (any polishing is still carried out).\n",
      "        polish : bool, optional\n",
      "            If True (default), then `scipy.optimize.minimize` with the `L-BFGS-B`\n",
      "            method is used to polish the best population member at the end, which\n",
      "            can improve the minimization slightly. If a constrained problem is\n",
      "            being studied then the `trust-constr` method is used instead.\n",
      "        init : str or array-like, optional\n",
      "            Specify which type of population initialization is performed. Should be\n",
      "            one of:\n",
      "        \n",
      "                - 'latinhypercube'\n",
      "                - 'random'\n",
      "                - array specifying the initial population. The array should have\n",
      "                  shape ``(M, len(x))``, where M is the total population size and\n",
      "                  len(x) is the number of parameters.\n",
      "                  `init` is clipped to `bounds` before use.\n",
      "        \n",
      "            The default is 'latinhypercube'. Latin Hypercube sampling tries to\n",
      "            maximize coverage of the available parameter space. 'random'\n",
      "            initializes the population randomly - this has the drawback that\n",
      "            clustering can occur, preventing the whole of parameter space being\n",
      "            covered. Use of an array to specify a population subset could be used,\n",
      "            for example, to create a tight bunch of initial guesses in an location\n",
      "            where the solution is known to exist, thereby reducing time for\n",
      "            convergence.\n",
      "        atol : float, optional\n",
      "            Absolute tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        updating : {'immediate', 'deferred'}, optional\n",
      "            If ``'immediate'``, the best solution vector is continuously updated\n",
      "            within a single generation [4]_. This can lead to faster convergence as\n",
      "            trial vectors can take advantage of continuous improvements in the best\n",
      "            solution.\n",
      "            With ``'deferred'``, the best solution vector is updated once per\n",
      "            generation. Only ``'deferred'`` is compatible with parallelization, and\n",
      "            the `workers` keyword can over-ride this option.\n",
      "        \n",
      "            .. versionadded:: 1.2.0\n",
      "        \n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the population is subdivided into `workers`\n",
      "            sections and evaluated in parallel\n",
      "            (uses `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply -1 to use all available CPU cores.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the population in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            This option will override the `updating` keyword to\n",
      "            ``updating='deferred'`` if ``workers != 1``.\n",
      "            Requires that `func` be pickleable.\n",
      "        \n",
      "            .. versionadded:: 1.2.0\n",
      "        \n",
      "        constraints : {NonLinearConstraint, LinearConstraint, Bounds}\n",
      "            Constraints on the solver, over and above those applied by the `bounds`\n",
      "            kwd. Uses the approach by Lampinen [5]_.\n",
      "        \n",
      "            .. versionadded:: 1.4.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes. If `polish`\n",
      "            was employed, and a lower minimum was obtained by the polishing, then\n",
      "            OptimizeResult also contains the ``jac`` attribute.\n",
      "            If the eventual solution does not satisfy the applied constraints\n",
      "            ``success`` will be `False`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Differential evolution is a stochastic population based method that is\n",
      "        useful for global optimization problems. At each pass through the population\n",
      "        the algorithm mutates each candidate solution by mixing with other candidate\n",
      "        solutions to create a trial candidate. There are several strategies [2]_ for\n",
      "        creating trial candidates, which suit some problems more than others. The\n",
      "        'best1bin' strategy is a good starting point for many systems. In this\n",
      "        strategy two members of the population are randomly chosen. Their difference\n",
      "        is used to mutate the best member (the 'best' in 'best1bin'), :math:`b_0`,\n",
      "        so far:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            b' = b_0 + mutation * (population[rand0] - population[rand1])\n",
      "        \n",
      "        A trial vector is then constructed. Starting with a randomly chosen ith\n",
      "        parameter the trial is sequentially filled (in modulo) with parameters from\n",
      "        ``b'`` or the original candidate. The choice of whether to use ``b'`` or the\n",
      "        original candidate is made with a binomial distribution (the 'bin' in\n",
      "        'best1bin') - a random number in [0, 1) is generated. If this number is\n",
      "        less than the `recombination` constant then the parameter is loaded from\n",
      "        ``b'``, otherwise it is loaded from the original candidate. The final\n",
      "        parameter is always loaded from ``b'``. Once the trial candidate is built\n",
      "        its fitness is assessed. If the trial is better than the original candidate\n",
      "        then it takes its place. If it is also better than the best overall\n",
      "        candidate it also replaces that.\n",
      "        To improve your chances of finding a global minimum use higher `popsize`\n",
      "        values, with higher `mutation` and (dithering), but lower `recombination`\n",
      "        values. This has the effect of widening the search radius, but slowing\n",
      "        convergence.\n",
      "        By default the best solution vector is updated continuously within a single\n",
      "        iteration (``updating='immediate'``). This is a modification [4]_ of the\n",
      "        original differential evolution algorithm which can lead to faster\n",
      "        convergence as trial vectors can immediately benefit from improved\n",
      "        solutions. To use the original Storn and Price behaviour, updating the best\n",
      "        solution once per iteration, set ``updating='deferred'``.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function is implemented in `rosen` in `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import rosen, differential_evolution\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "        \n",
      "        Now repeat, but with parallelization.\n",
      "        \n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds, updating='deferred',\n",
      "        ...                                 workers=2)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "        \n",
      "        Let's try and do a constrained minimization\n",
      "        \n",
      "        >>> from scipy.optimize import NonlinearConstraint, Bounds\n",
      "        >>> def constr_f(x):\n",
      "        ...     return np.array(x[0] + x[1])\n",
      "        >>>\n",
      "        >>> # the sum of x[0] and x[1] must be less than 1.9\n",
      "        >>> nlc = NonlinearConstraint(constr_f, -np.inf, 1.9)\n",
      "        >>> # specify limits using a `Bounds` object.\n",
      "        >>> bounds = Bounds([0., 0.], [2., 2.])\n",
      "        >>> result = differential_evolution(rosen, bounds, constraints=(nlc),\n",
      "        ...                                 seed=1)\n",
      "        >>> result.x, result.fun\n",
      "        (array([0.96633867, 0.93363577]), 0.0011361355854792312)\n",
      "        \n",
      "        Next find the minimum of the Ackley function\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization).\n",
      "        \n",
      "        >>> from scipy.optimize import differential_evolution\n",
      "        >>> import numpy as np\n",
      "        >>> def ackley(x):\n",
      "        ...     arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n",
      "        ...     arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n",
      "        ...     return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
      "        >>> bounds = [(-5, 5), (-5, 5)]\n",
      "        >>> result = differential_evolution(ackley, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 0.,  0.]), 4.4408920985006262e-16)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Storn, R and Price, K, Differential Evolution - a Simple and\n",
      "               Efficient Heuristic for Global Optimization over Continuous Spaces,\n",
      "               Journal of Global Optimization, 1997, 11, 341 - 359.\n",
      "        .. [2] http://www1.icsi.berkeley.edu/~storn/code.html\n",
      "        .. [3] http://en.wikipedia.org/wiki/Differential_evolution\n",
      "        .. [4] Wormington, M., Panaccione, C., Matney, K. M., Bowen, D. K., -\n",
      "               Characterization of structures from X-ray scattering data using\n",
      "               genetic algorithms, Phil. Trans. R. Soc. Lond. A, 1999, 357,\n",
      "               2827-2848\n",
      "        .. [5] Lampinen, J., A constraint handling approach for the differential\n",
      "               evolution algorithm. Proceedings of the 2002 Congress on\n",
      "               Evolutionary Computation. CEC'02 (Cat. No. 02TH8600). Vol. 2. IEEE,\n",
      "               2002.\n",
      "    \n",
      "    dual_annealing(func, bounds, args=(), maxiter=1000, local_search_options={}, initial_temp=5230.0, restart_temp_ratio=2e-05, visit=2.62, accept=-5.0, maxfun=10000000.0, seed=None, no_local_search=False, callback=None, x0=None)\n",
      "        Find the global minimum of a function using Dual Annealing.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence, shape (n, 2)\n",
      "            Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
      "            defining bounds for the objective function parameter.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of global search iterations. Default value is 1000.\n",
      "        local_search_options : dict, optional\n",
      "            Extra keyword arguments to be passed to the local minimizer\n",
      "            (`minimize`). Some important options could be:\n",
      "            ``method`` for the minimizer method to use and ``args`` for\n",
      "            objective function additional arguments.\n",
      "        initial_temp : float, optional\n",
      "            The initial temperature, use higher values to facilitates a wider\n",
      "            search of the energy landscape, allowing dual_annealing to escape\n",
      "            local minima that it is trapped in. Default value is 5230. Range is\n",
      "            (0.01, 5.e4].\n",
      "        restart_temp_ratio : float, optional\n",
      "            During the annealing process, temperature is decreasing, when it\n",
      "            reaches ``initial_temp * restart_temp_ratio``, the reannealing process\n",
      "            is triggered. Default value of the ratio is 2e-5. Range is (0, 1).\n",
      "        visit : float, optional\n",
      "            Parameter for visiting distribution. Default value is 2.62. Higher\n",
      "            values give the visiting distribution a heavier tail, this makes\n",
      "            the algorithm jump to a more distant region. The value range is (0, 3].\n",
      "        accept : float, optional\n",
      "            Parameter for acceptance distribution. It is used to control the\n",
      "            probability of acceptance. The lower the acceptance parameter, the\n",
      "            smaller the probability of acceptance. Default value is -5.0 with\n",
      "            a range (-1e4, -5].\n",
      "        maxfun : int, optional\n",
      "            Soft limit for the number of objective function calls. If the\n",
      "            algorithm is in the middle of a local search, this number will be\n",
      "            exceeded, the algorithm will stop just after the local search is\n",
      "            done. Default value is 1e7.\n",
      "        seed : {int, `~numpy.random.RandomState`, `~numpy.random.Generator`}, optional\n",
      "            If `seed` is not specified the `~numpy.random.RandomState` singleton is\n",
      "            used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
      "            with `seed`.\n",
      "            If `seed` is already a ``RandomState`` or ``Generator`` instance, then\n",
      "            that instance is used.\n",
      "            Specify `seed` for repeatable minimizations. The random numbers\n",
      "            generated with this seed only affect the visiting distribution function\n",
      "            and new coordinates generation.\n",
      "        no_local_search : bool, optional\n",
      "            If `no_local_search` is set to True, a traditional Generalized\n",
      "            Simulated Annealing will be performed with no local search\n",
      "            strategy applied.\n",
      "        callback : callable, optional\n",
      "            A callback function with signature ``callback(x, f, context)``,\n",
      "            which will be called for all minima found.\n",
      "            ``x`` and ``f`` are the coordinates and function value of the\n",
      "            latest minimum found, and ``context`` has value in [0, 1, 2], with the\n",
      "            following meaning:\n",
      "        \n",
      "                - 0: minimum detected in the annealing process.\n",
      "                - 1: detection occurred in the local search process.\n",
      "                - 2: detection done in the dual annealing process.\n",
      "        \n",
      "            If the callback implementation returns True, the algorithm will stop.\n",
      "        x0 : ndarray, shape(n,), optional\n",
      "            Coordinates of a single N-D starting point.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination.\n",
      "            See `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements the Dual Annealing optimization. This stochastic\n",
      "        approach derived from [3]_ combines the generalization of CSA (Classical\n",
      "        Simulated Annealing) and FSA (Fast Simulated Annealing) [1]_ [2]_ coupled\n",
      "        to a strategy for applying a local search on accepted locations [4]_.\n",
      "        An alternative implementation of this same algorithm is described in [5]_\n",
      "        and benchmarks are presented in [6]_. This approach introduces an advanced\n",
      "        method to refine the solution found by the generalized annealing\n",
      "        process. This algorithm uses a distorted Cauchy-Lorentz visiting\n",
      "        distribution, with its shape controlled by the parameter :math:`q_{v}`\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            g_{q_{v}}(\\Delta x(t)) \\propto \\frac{ \\\n",
      "            \\left[T_{q_{v}}(t) \\right]^{-\\frac{D}{3-q_{v}}}}{ \\\n",
      "            \\left[{1+(q_{v}-1)\\frac{(\\Delta x(t))^{2}} { \\\n",
      "            \\left[T_{q_{v}}(t)\\right]^{\\frac{2}{3-q_{v}}}}}\\right]^{ \\\n",
      "            \\frac{1}{q_{v}-1}+\\frac{D-1}{2}}}\n",
      "        \n",
      "        Where :math:`t` is the artificial time. This visiting distribution is used\n",
      "        to generate a trial jump distance :math:`\\Delta x(t)` of variable\n",
      "        :math:`x(t)` under artificial temperature :math:`T_{q_{v}}(t)`.\n",
      "        \n",
      "        From the starting point, after calling the visiting distribution\n",
      "        function, the acceptance probability is computed as follows:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            p_{q_{a}} = \\min{\\{1,\\left[1-(1-q_{a}) \\beta \\Delta E \\right]^{ \\\n",
      "            \\frac{1}{1-q_{a}}}\\}}\n",
      "        \n",
      "        Where :math:`q_{a}` is a acceptance parameter. For :math:`q_{a}<1`, zero\n",
      "        acceptance probability is assigned to the cases where\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            [1-(1-q_{a}) \\beta \\Delta E] < 0\n",
      "        \n",
      "        The artificial temperature :math:`T_{q_{v}}(t)` is decreased according to\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            T_{q_{v}}(t) = T_{q_{v}}(1) \\frac{2^{q_{v}-1}-1}{\\left( \\\n",
      "            1 + t\\right)^{q_{v}-1}-1}\n",
      "        \n",
      "        Where :math:`q_{v}` is the visiting parameter.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsallis C. Possible generalization of Boltzmann-Gibbs\n",
      "            statistics. Journal of Statistical Physics, 52, 479-487 (1998).\n",
      "        .. [2] Tsallis C, Stariolo DA. Generalized Simulated Annealing.\n",
      "            Physica A, 233, 395-406 (1996).\n",
      "        .. [3] Xiang Y, Sun DY, Fan W, Gong XG. Generalized Simulated\n",
      "            Annealing Algorithm and Its Application to the Thomson Model.\n",
      "            Physics Letters A, 233, 216-220 (1997).\n",
      "        .. [4] Xiang Y, Gong XG. Efficiency of Generalized Simulated\n",
      "            Annealing. Physical Review E, 62, 4473 (2000).\n",
      "        .. [5] Xiang Y, Gubian S, Suomela B, Hoeng J. Generalized\n",
      "            Simulated Annealing for Efficient Global Optimization: the GenSA\n",
      "            Package for R. The R Journal, Volume 5/1 (2013).\n",
      "        .. [6] Mullen, K. Continuous Global Optimization in R. Journal of\n",
      "            Statistical Software, 60(6), 1 - 45, (2014).\n",
      "            :doi:`10.18637/jss.v060.i06`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 10-D problem, with many local minima.\n",
      "        The function involved is called Rastrigin\n",
      "        (https://en.wikipedia.org/wiki/Rastrigin_function)\n",
      "        \n",
      "        >>> from scipy.optimize import dual_annealing\n",
      "        >>> func = lambda x: np.sum(x*x - 10*np.cos(2*np.pi*x)) + 10*np.size(x)\n",
      "        >>> lw = [-5.12] * 10\n",
      "        >>> up = [5.12] * 10\n",
      "        >>> ret = dual_annealing(func, bounds=list(zip(lw, up)), seed=1234)\n",
      "        >>> ret.x\n",
      "        array([-4.26437714e-09, -3.91699361e-09, -1.86149218e-09, -3.97165720e-09,\n",
      "               -6.29151648e-09, -6.53145322e-09, -3.93616815e-09, -6.55623025e-09,\n",
      "               -6.05775280e-09, -5.00668935e-09]) # may vary\n",
      "        >>> ret.fun\n",
      "        0.000000\n",
      "    \n",
      "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
      "        \n",
      "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='excitingmixing'`` in particular.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial Jacobian approximation is (-1/alpha).\n",
      "        alphamax : float, optional\n",
      "            The entries of the diagonal Jacobian are kept in the range\n",
      "            ``[alpha, alphamax]``.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "    \n",
      "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500, method='del2')\n",
      "        Find a fixed point of the function.\n",
      "        \n",
      "        Given a function of one or more variables and a starting point, find a\n",
      "        fixed point of the function: i.e., where ``func(x0) == x0``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            Function to evaluate.\n",
      "        x0 : array_like\n",
      "            Fixed point of function.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to `func`.\n",
      "        xtol : float, optional\n",
      "            Convergence tolerance, defaults to 1e-08.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations, defaults to 500.\n",
      "        method : {\"del2\", \"iteration\"}, optional\n",
      "            Method of finding the fixed-point, defaults to \"del2\",\n",
      "            which uses Steffensen's Method with Aitken's ``Del^2``\n",
      "            convergence acceleration [1]_. The \"iteration\" method simply iterates\n",
      "            the function until convergence is detected, without attempting to\n",
      "            accelerate the convergence.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c1, c2):\n",
      "        ...    return np.sqrt(c1/(x+c2))\n",
      "        >>> c1 = np.array([10,12.])\n",
      "        >>> c2 = np.array([3, 5.])\n",
      "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
      "        array([ 1.4920333 ,  1.37228132])\n",
      "    \n",
      "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, initial_simplex=None)\n",
      "        Minimize a function using the downhill simplex algorithm.\n",
      "        \n",
      "        This algorithm only uses function values, not derivatives or second\n",
      "        derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            The objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func, i.e., ``f(x,*args)``.\n",
      "        xtol : float, optional\n",
      "            Absolute error in xopt between iterations that is acceptable for\n",
      "            convergence.\n",
      "        ftol : number, optional\n",
      "            Absolute error in func(xopt) between iterations that is acceptable for\n",
      "            convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : number, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            Set to True if fopt and warnflag outputs are desired.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages.\n",
      "        retall : bool, optional\n",
      "            Set to True to return list of solutions at each iteration.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        initial_simplex : array_like of shape (N + 1, N), optional\n",
      "            Initial simplex. If given, overrides `x0`.\n",
      "            ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "            the jth vertex of the ``N+1`` vertices in the simplex, where\n",
      "            ``N`` is the dimension.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter that minimizes function.\n",
      "        fopt : float\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        iter : int\n",
      "            Number of iterations performed.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            1 : Maximum number of function evaluations made.\n",
      "            2 : Maximum number of iterations reached.\n",
      "        allvecs : list\n",
      "            Solution at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Nelder-Mead' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
      "        one or more variables.\n",
      "        \n",
      "        This algorithm has a long history of successful use in applications.\n",
      "        But it will usually be slower than an algorithm that uses first or\n",
      "        second derivative information. In practice, it can have poor\n",
      "        performance in high-dimensional problems and is not robust to\n",
      "        minimizing complicated functions. Additionally, there currently is no\n",
      "        complete theory describing when the algorithm will successfully\n",
      "        converge to the minimum, or how fast it will if it does. Both the ftol and\n",
      "        xtol criteria must be met for convergence.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin(f, 1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 17\n",
      "                 Function evaluations: 34\n",
      "        >>> minimum[0]\n",
      "        -8.8817841970012523e-16\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
      "               minimization\", The Computer Journal, 7, pp. 308-313\n",
      "        \n",
      "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
      "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
      "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
      "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
      "               Harlow, UK, pp. 191-208.\n",
      "    \n",
      "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using the BFGS algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable f'(x,*args), optional\n",
      "            Gradient of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f and fprime.\n",
      "        gtol : float, optional\n",
      "            Gradient norm must be less than gtol before successful termination.\n",
      "        norm : float, optional\n",
      "            Order of norm (Inf is max, -Inf is min)\n",
      "        epsilon : int or ndarray, optional\n",
      "            If fprime is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function to call after each\n",
      "            iteration. Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True,return fopt, func_calls, grad_calls, and warnflag\n",
      "            in addition to xopt.\n",
      "        disp : bool, optional\n",
      "            Print convergence message if True.\n",
      "        retall : bool, optional\n",
      "            Return a list of results at each iteration if True.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., f(xopt) == fopt.\n",
      "        fopt : float\n",
      "            Minimum value.\n",
      "        gopt : ndarray\n",
      "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
      "        Bopt : ndarray\n",
      "            Value of 1/f''(xopt), i.e., the inverse Hessian matrix.\n",
      "        func_calls : int\n",
      "            Number of function_calls made.\n",
      "        grad_calls : int\n",
      "            Number of gradient calls made.\n",
      "        warnflag : integer\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Gradient and/or function calls not changing.\n",
      "            3 : NaN result encountered.\n",
      "        allvecs  :  list\n",
      "            The value of xopt at each iteration. Only returned if retall is True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'BFGS' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Optimize the function, f, whose gradient is given by fprime\n",
      "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
      "        and Shanno (BFGS)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright, and Nocedal 'Numerical Optimization', 1999, p. 198.\n",
      "    \n",
      "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable, ``f(x, *args)``\n",
      "            Objective function to be minimized. Here `x` must be a 1-D array of\n",
      "            the variables that are to be changed in the search for a minimum, and\n",
      "            `args` are the other (fixed) parameters of `f`.\n",
      "        x0 : ndarray\n",
      "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
      "            It must be a 1-D array of values.\n",
      "        fprime : callable, ``fprime(x, *args)``, optional\n",
      "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
      "            are as described above for `f`. The returned value must be a 1-D array.\n",
      "            Defaults to None, in which case the gradient is approximated\n",
      "            numerically (see `epsilon`, below).\n",
      "        args : tuple, optional\n",
      "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
      "            additional fixed parameters are needed to completely specify the\n",
      "            functions `f` and `fprime`.\n",
      "        gtol : float, optional\n",
      "            Stop when the norm of the gradient is less than `gtol`.\n",
      "        norm : float, optional\n",
      "            Order to use for the norm of the gradient\n",
      "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
      "        epsilon : float or ndarray, optional\n",
      "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
      "            scalar or a 1-D array. Defaults to ``sqrt(eps)``, with eps the\n",
      "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
      "            1.5e-8.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
      "        full_output : bool, optional\n",
      "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
      "            addition to `xopt`.  See the Returns section below for additional\n",
      "            information on optional return values.\n",
      "        disp : bool, optional\n",
      "            If True, return a convergence message, followed by `xopt`.\n",
      "        retall : bool, optional\n",
      "            If True, add to the returned values the results of each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each iteration.\n",
      "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float, optional\n",
      "            Minimum value found, f(xopt). Only returned if `full_output` is True.\n",
      "        func_calls : int, optional\n",
      "            The number of function_calls made. Only returned if `full_output`\n",
      "            is True.\n",
      "        grad_calls : int, optional\n",
      "            The number of gradient calls made. Only returned if `full_output` is\n",
      "            True.\n",
      "        warnflag : int, optional\n",
      "            Integer value with warning status, only returned if `full_output` is\n",
      "            True.\n",
      "        \n",
      "            0 : Success.\n",
      "        \n",
      "            1 : The maximum number of iterations was exceeded.\n",
      "        \n",
      "            2 : Gradient and/or function calls were not changing. May indicate\n",
      "                that precision was lost, i.e., the routine did not converge.\n",
      "        \n",
      "            3 : NaN result encountered.\n",
      "        \n",
      "        allvecs : list of ndarray, optional\n",
      "            List of arrays, containing the results at each iteration.\n",
      "            Only returned if `retall` is True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize : common interface to all `scipy.optimize` algorithms for\n",
      "                   unconstrained and constrained minimization of multivariate\n",
      "                   functions. It provides an alternative way to call\n",
      "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
      "        [1]_.\n",
      "        \n",
      "        Conjugate gradient methods tend to work better when:\n",
      "        \n",
      "        1. `f` has a unique global minimizing point, and no local minima or\n",
      "           other stationary points,\n",
      "        2. `f` is, at least locally, reasonably well approximated by a\n",
      "           quadratic function of the variables,\n",
      "        3. `f` is continuous and has a continuous gradient,\n",
      "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
      "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
      "           minimizing point, `xopt`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Example 1: seek the minimum value of the expression\n",
      "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
      "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
      "        \n",
      "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
      "        >>> def f(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
      "        >>> def gradf(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
      "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
      "        ...     return np.asarray((gu, gv))\n",
      "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
      "        >>> from scipy import optimize\n",
      "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 1.617021\n",
      "                 Iterations: 4\n",
      "                 Function evaluations: 8\n",
      "                 Gradient evaluations: 8\n",
      "        >>> res1\n",
      "        array([-1.80851064, -0.25531915])\n",
      "        \n",
      "        Example 2: solve the same problem using the `minimize` function.\n",
      "        (This `myopts` dictionary shows all of the available options,\n",
      "        although in practice only non-default values would be needed.\n",
      "        The returned value will be a dictionary.)\n",
      "        \n",
      "        >>> opts = {'maxiter' : None,    # default value.\n",
      "        ...         'disp' : True,    # non-default value.\n",
      "        ...         'gtol' : 1e-5,    # default value.\n",
      "        ...         'norm' : np.inf,  # default value.\n",
      "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
      "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
      "        ...                          method='CG', options=opts)\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 1.617021\n",
      "                Iterations: 4\n",
      "                Function evaluations: 8\n",
      "                Gradient evaluations: 8\n",
      "        >>> res2.x  # minimum found\n",
      "        array([-1.80851064, -0.25531915])\n",
      "    \n",
      "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, maxfun=1000, disp=None, catol=0.0002)\n",
      "        Minimize a function using the Constrained Optimization By Linear\n",
      "        Approximation (COBYLA) method. This method wraps a FORTRAN\n",
      "        implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Function to minimize. In the form func(x, \\*args).\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        cons : sequence\n",
      "            Constraint functions; must all be ``>=0`` (a single function\n",
      "            if only 1 constraint). Each function takes the parameters `x`\n",
      "            as its first argument, and it can return either a single number or\n",
      "            an array or list of numbers.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function.\n",
      "        consargs : tuple, optional\n",
      "            Extra arguments to pass to constraint functions (default of None means\n",
      "            use same extra arguments as those passed to func).\n",
      "            Use ``()`` for no extra arguments.\n",
      "        rhobeg : float, optional\n",
      "            Reasonable initial changes to the variables.\n",
      "        rhoend : float, optional\n",
      "            Final accuracy in the optimization (not precisely guaranteed). This\n",
      "            is a lower bound on the size of the trust region.\n",
      "        disp : {0, 1, 2, 3}, optional\n",
      "            Controls the frequency of output; 0 implies no output.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        catol : float, optional\n",
      "            Absolute tolerance for constraint violations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The argument that minimises `f`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'COBYLA' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm is based on linear approximations to the objective\n",
      "        function and each constraint. We briefly describe the algorithm.\n",
      "        \n",
      "        Suppose the function is being minimized over k variables. At the\n",
      "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
      "        an approximate solution x_j, and a radius RHO_j.\n",
      "        (i.e., linear plus a constant) approximations to the objective\n",
      "        function and constraint functions such that their function values\n",
      "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
      "        This gives a linear program to solve (where the linear approximations\n",
      "        of the constraint functions are constrained to be non-negative).\n",
      "        \n",
      "        However, the linear approximations are likely only good\n",
      "        approximations near the current simplex, so the linear program is\n",
      "        given the further requirement that the solution, which\n",
      "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
      "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
      "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
      "        like a trust region algorithm.\n",
      "        \n",
      "        Additionally, the linear program may be inconsistent, or the\n",
      "        approximation may give poor improvement. For details about\n",
      "        how these issues are resolved, as well as how the points v_i are\n",
      "        updated, refer to the source code or the references below.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
      "        the objective and constraint functions by linear interpolation.\", in\n",
      "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
      "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
      "        \n",
      "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
      "        calculations\", Acta Numerica 7, 287-336\n",
      "        \n",
      "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
      "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Minimize the objective function f(x,y) = x*y subject\n",
      "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
      "        \n",
      "            >>> def objective(x):\n",
      "            ...     return x[0]*x[1]\n",
      "            ...\n",
      "            >>> def constr1(x):\n",
      "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
      "            ...\n",
      "            >>> def constr2(x):\n",
      "            ...     return x[1]\n",
      "            ...\n",
      "            >>> from scipy.optimize import fmin_cobyla\n",
      "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
      "            array([-0.70710685,  0.70710671])\n",
      "        \n",
      "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
      "    \n",
      "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=-1, maxfun=15000, maxiter=15000, disp=None, callback=None, maxls=20)\n",
      "        Minimize a function func using the L-BFGS-B algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Function to minimize.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable fprime(x,*args), optional\n",
      "            The gradient of `func`. If None, then `func` returns the function\n",
      "            value and the gradient (``f, g = func(x, *args)``), unless\n",
      "            `approx_grad` is True in which case `func` returns only ``f``.\n",
      "        args : sequence, optional\n",
      "            Arguments to pass to `func` and `fprime`.\n",
      "        approx_grad : bool, optional\n",
      "            Whether to approximate the gradient numerically (in which case\n",
      "            `func` returns only the function value).\n",
      "        bounds : list, optional\n",
      "            ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the bounds on that parameter. Use None or +-inf for one of ``min`` or\n",
      "            ``max`` when there is no bound in that direction.\n",
      "        m : int, optional\n",
      "            The maximum number of variable metric corrections\n",
      "            used to define the limited memory matrix. (The limited memory BFGS\n",
      "            method does not store the full hessian but uses this many terms in an\n",
      "            approximation to it.)\n",
      "        factr : float, optional\n",
      "            The iteration stops when\n",
      "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
      "            where ``eps`` is the machine precision, which is automatically\n",
      "            generated by the code. Typical values for `factr` are: 1e12 for\n",
      "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
      "            high accuracy. See Notes for relationship to `ftol`, which is exposed\n",
      "            (instead of `factr`) by the `scipy.optimize.minimize` interface to\n",
      "            L-BFGS-B.\n",
      "        pgtol : float, optional\n",
      "            The iteration will stop when\n",
      "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
      "            where ``pg_i`` is the i-th component of the projected gradient.\n",
      "        epsilon : float, optional\n",
      "            Step size used when `approx_grad` is True, for numerically\n",
      "            calculating the gradient\n",
      "        iprint : int, optional\n",
      "            Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "            ``iprint = 0``    print only one line at the last iteration;\n",
      "            ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n",
      "            ``iprint = 99``   print details of every iteration except n-vectors;\n",
      "            ``iprint = 100``  print also the changes of active set and final x;\n",
      "            ``iprint > 100``  print details of every iteration including x and g.\n",
      "        disp : int, optional\n",
      "            If zero, then no output. If a positive number, then this over-rides\n",
      "            `iprint` (i.e., `iprint` gets the value of `disp`).\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        maxls : int, optional\n",
      "            Maximum number of line search steps (per iteration). Default is 20.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : array_like\n",
      "            Estimated position of the minimum.\n",
      "        f : float\n",
      "            Value of `func` at the minimum.\n",
      "        d : dict\n",
      "            Information dictionary.\n",
      "        \n",
      "            * d['warnflag'] is\n",
      "        \n",
      "              - 0 if converged,\n",
      "              - 1 if too many function evaluations or too many iterations,\n",
      "              - 2 if stopped for another reason, given in d['task']\n",
      "        \n",
      "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
      "            * d['funcalls'] is the number of function calls made.\n",
      "            * d['nit'] is the number of iterations.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'L-BFGS-B' `method` in particular. Note that the\n",
      "            `ftol` option is made available via that interface, while `factr` is\n",
      "            provided via this interface, where `factr` is the factor multiplying\n",
      "            the default machine floating-point precision to arrive at `ftol`:\n",
      "            ``ftol = factr * numpy.finfo(float).eps``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        License of L-BFGS-B (FORTRAN code):\n",
      "        \n",
      "        The version included here (in fortran code) is 3.0\n",
      "        (released April 25, 2011). It was written by Ciyou Zhu, Richard Byrd,\n",
      "        and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
      "        condition for use:\n",
      "        \n",
      "        This software is freely available, but we expect that all publications\n",
      "        describing work using this software, or all commercial products using it,\n",
      "        quote at least one of the references given below. This software is released\n",
      "        under the BSD License.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
      "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
      "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
      "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
      "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
      "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
      "          ACM Transactions on Mathematical Software, 38, 1.\n",
      "    \n",
      "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Unconstrained minimization of a function using the Newton-CG method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable ``f(x, *args)``\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable ``f'(x, *args)``\n",
      "            Gradient of f.\n",
      "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
      "            Function which computes the Hessian of f times an\n",
      "            arbitrary vector, p.\n",
      "        fhess : callable ``fhess(x, *args)``, optional\n",
      "            Function to compute the Hessian matrix of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
      "            (the same set of extra arguments is supplied to all of\n",
      "            these functions).\n",
      "        epsilon : float or ndarray, optional\n",
      "            If fhess is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function which is called after\n",
      "            each iteration. Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        avextol : float, optional\n",
      "            Convergence is assumed when the average relative error in\n",
      "            the minimizer falls below this amount.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True, return the optional outputs.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence message.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of results at each iteration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float\n",
      "            Value of the function at xopt, i.e., ``fopt = f(xopt)``.\n",
      "        fcalls : int\n",
      "            Number of function calls made.\n",
      "        gcalls : int\n",
      "            Number of gradient calls made.\n",
      "        hcalls : int\n",
      "            Number of Hessian calls made.\n",
      "        warnflag : int\n",
      "            Warnings generated by the algorithm.\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Line search failure (precision loss).\n",
      "            3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            The result at each iteration, if retall is True (see below).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Newton-CG' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
      "        is provided, then `fhess_p` will be ignored. If neither `fhess`\n",
      "        nor `fhess_p` is provided, then the hessian product will be\n",
      "        approximated using finite differences on `fprime`. `fhess_p`\n",
      "        must compute the hessian times an arbitrary vector. If it is not\n",
      "        given, finite-differences on `fprime` are used to compute\n",
      "        it.\n",
      "        \n",
      "        Newton-CG methods are also called truncated Newton methods. This\n",
      "        function differs from scipy.optimize.fmin_tnc because\n",
      "        \n",
      "        1. scipy.optimize.fmin_ncg is written purely in Python using NumPy\n",
      "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
      "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
      "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
      "            or box constrained minimization. (Box constraints give\n",
      "            lower and upper bounds for each variable separately.)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright & Nocedal, 'Numerical Optimization', 1999, p. 140.\n",
      "    \n",
      "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
      "        Minimize a function using modified Powell's method.\n",
      "        \n",
      "        This method only uses function values, not derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func.\n",
      "        xtol : float, optional\n",
      "            Line-search error tolerance.\n",
      "        ftol : float, optional\n",
      "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            If True, ``fopt``, ``xi``, ``direc``, ``iter``, ``funcalls``, and\n",
      "            ``warnflag`` are returned.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence messages.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of the solution at each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each\n",
      "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        direc : ndarray, optional\n",
      "            Initial fitting step and parameter order set as an (N, N) array, where N\n",
      "            is the number of fitting parameters in `x0`. Defaults to step size 1.0\n",
      "            fitting all parameters simultaneously (``np.ones((N, N))``). To\n",
      "            prevent initial consideration of values in a step or to change initial\n",
      "            step size, set to 0 or desired step size in the Jth position in the Mth\n",
      "            block, where J is the position in `x0` and M is the desired evaluation\n",
      "            step, with steps being evaluated in index order. Step size and ordering\n",
      "            will change freely as minimization proceeds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter which minimizes `func`.\n",
      "        fopt : number\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        direc : ndarray\n",
      "            Current direction set.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            Integer warning flag:\n",
      "                1 : Maximum number of function evaluations.\n",
      "                2 : Maximum number of iterations.\n",
      "                3 : NaN result encountered.\n",
      "                4 : The result is out of the provided bounds.\n",
      "        allvecs : list\n",
      "            List of solutions at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to unconstrained minimization algorithms for\n",
      "            multivariate functions. See the 'Powell' method in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a modification of Powell's method to find the minimum of\n",
      "        a function of N variables. Powell's method is a conjugate\n",
      "        direction method.\n",
      "        \n",
      "        The algorithm has two loops. The outer loop merely iterates over the inner\n",
      "        loop. The inner loop minimizes over each current direction in the direction\n",
      "        set. At the end of the inner loop, if certain conditions are met, the\n",
      "        direction that gave the largest decrease is dropped and replaced with the\n",
      "        difference between the current estimated x and the estimated x from the\n",
      "        beginning of the inner-loop.\n",
      "        \n",
      "        The technical conditions for replacing the direction of greatest\n",
      "        increase amount to checking that\n",
      "        \n",
      "        1. No further gain can be made along the direction of greatest increase\n",
      "           from that iteration.\n",
      "        2. The direction of greatest increase accounted for a large sufficient\n",
      "           fraction of the decrease in the function value from that iteration of\n",
      "           the inner loop.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
      "        function of several variables without calculating derivatives,\n",
      "        Computer Journal, 7 (2):155-162.\n",
      "        \n",
      "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
      "        Numerical Recipes (any edition), Cambridge University Press\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin_powell(f, -1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 2\n",
      "                 Function evaluations: 18\n",
      "        >>> minimum\n",
      "        array(0.0)\n",
      "    \n",
      "    fmin_slsqp(func, x0, eqcons=(), f_eqcons=None, ieqcons=(), f_ieqcons=None, bounds=(), fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=1.4901161193847656e-08, callback=None)\n",
      "        Minimize a function using Sequential Least Squares Programming\n",
      "        \n",
      "        Python interface function for the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.  Must return a scalar.\n",
      "        x0 : 1-D ndarray of float\n",
      "            Initial guess for the independent variable(s).\n",
      "        eqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_eqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D array in which each element must equal 0.0 in a\n",
      "            successfully optimized problem. If f_eqcons is specified,\n",
      "            eqcons is ignored.\n",
      "        ieqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_ieqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D ndarray in which each element must be greater or\n",
      "            equal to 0.0 in a successfully optimized problem. If\n",
      "            f_ieqcons is specified, ieqcons is ignored.\n",
      "        bounds : list, optional\n",
      "            A list of tuples specifying the lower and upper bound\n",
      "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
      "            Infinite values will be interpreted as large floating values.\n",
      "        fprime : callable `f(x,*args)`, optional\n",
      "            A function that evaluates the partial derivatives of func.\n",
      "        fprime_eqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of equality constraint normals. If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
      "        fprime_ieqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of inequality constraint normals. If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
      "        args : sequence, optional\n",
      "            Additional arguments passed to func and fprime.\n",
      "        iter : int, optional\n",
      "            The maximum number of iterations.\n",
      "        acc : float, optional\n",
      "            Requested accuracy.\n",
      "        iprint : int, optional\n",
      "            The verbosity of fmin_slsqp :\n",
      "        \n",
      "            * iprint <= 0 : Silent operation\n",
      "            * iprint == 1 : Print summary upon completion (default)\n",
      "            * iprint >= 2 : Print status of each iterate and summary\n",
      "        disp : int, optional\n",
      "            Overrides the iprint interface (preferred).\n",
      "        full_output : bool, optional\n",
      "            If False, return only the minimizer of func (default).\n",
      "            Otherwise, output final objective function and summary\n",
      "            information.\n",
      "        epsilon : float, optional\n",
      "            The step size for finite-difference derivative estimates.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(x)``, where ``x`` is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray of float\n",
      "            The final minimizer of func.\n",
      "        fx : ndarray of float, if full_output is true\n",
      "            The final value of the objective function.\n",
      "        its : int, if full_output is true\n",
      "            The number of iterations.\n",
      "        imode : int, if full_output is true\n",
      "            The exit mode from the optimizer (see below).\n",
      "        smode : string, if full_output is true\n",
      "            Message describing the exit mode from the optimizer.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'SLSQP' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Exit modes are defined as follows ::\n",
      "        \n",
      "            -1 : Gradient evaluation required (g & a)\n",
      "             0 : Optimization terminated successfully\n",
      "             1 : Function evaluation required (f & c)\n",
      "             2 : More equality constraints than independent variables\n",
      "             3 : More than 3*n iterations in LSQ subproblem\n",
      "             4 : Inequality constraints incompatible\n",
      "             5 : Singular matrix E in LSQ subproblem\n",
      "             6 : Singular matrix C in LSQ subproblem\n",
      "             7 : Rank-deficient equality constraint subproblem HFTI\n",
      "             8 : Positive directional derivative for linesearch\n",
      "             9 : Iteration limit reached\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
      "    \n",
      "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
      "        Minimize a function with variables subject to bounds, using\n",
      "        gradient information in a truncated Newton algorithm. This\n",
      "        method wraps a C implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x, *args)``\n",
      "            Function to minimize.  Must do one of:\n",
      "        \n",
      "            1. Return f and g, where f is the value of the function and g its\n",
      "               gradient (a list of floats).\n",
      "        \n",
      "            2. Return the function value but supply gradient function\n",
      "               separately as `fprime`.\n",
      "        \n",
      "            3. Return the function value and set ``approx_grad=True``.\n",
      "        \n",
      "            If the function returns None, the minimization\n",
      "            is aborted.\n",
      "        x0 : array_like\n",
      "            Initial estimate of minimum.\n",
      "        fprime : callable ``fprime(x, *args)``, optional\n",
      "            Gradient of `func`. If None, then either `func` must return the\n",
      "            function value and the gradient (``f,g = func(x, *args)``)\n",
      "            or `approx_grad` must be True.\n",
      "        args : tuple, optional\n",
      "            Arguments to pass to function.\n",
      "        approx_grad : bool, optional\n",
      "            If true, approximate the gradient numerically.\n",
      "        bounds : list, optional\n",
      "            (min, max) pairs for each element in x0, defining the\n",
      "            bounds on that parameter. Use None or +/-inf for one of\n",
      "            min or max when there is no bound in that direction.\n",
      "        epsilon : float, optional\n",
      "            Used if approx_grad is True. The stepsize in a finite\n",
      "            difference approximation for fprime.\n",
      "        scale : array_like, optional\n",
      "            Scaling factors to apply to each variable. If None, the\n",
      "            factors are up-low for interval bounded variables and\n",
      "            1+|x| for the others. Defaults to None.\n",
      "        offset : array_like, optional\n",
      "            Value to subtract from each variable. If None, the\n",
      "            offsets are (up+low)/2 for interval bounded variables\n",
      "            and x for the others.\n",
      "        messages : int, optional\n",
      "            Bit mask used to select messages display during\n",
      "            minimization values defined in the MSGS dict. Defaults to\n",
      "            MGS_ALL.\n",
      "        disp : int, optional\n",
      "            Integer interface to messages. 0 = no message, 5 = all messages\n",
      "        maxCGit : int, optional\n",
      "            Maximum number of hessian*vector evaluations per main\n",
      "            iteration. If maxCGit == 0, the direction chosen is\n",
      "            -gradient if maxCGit < 0, maxCGit is set to\n",
      "            max(1,min(50,n/2)). Defaults to -1.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluation. If None, maxfun is\n",
      "            set to max(100, 10*len(x0)). Defaults to None.\n",
      "        eta : float, optional\n",
      "            Severity of the line search. If < 0 or > 1, set to 0.25.\n",
      "            Defaults to -1.\n",
      "        stepmx : float, optional\n",
      "            Maximum step for the line search. May be increased during\n",
      "            call. If too small, it will be set to 10.0. Defaults to 0.\n",
      "        accuracy : float, optional\n",
      "            Relative precision for finite difference calculations. If\n",
      "            <= machine_precision, set to sqrt(machine_precision).\n",
      "            Defaults to 0.\n",
      "        fmin : float, optional\n",
      "            Minimum function value estimate. Defaults to 0.\n",
      "        ftol : float, optional\n",
      "            Precision goal for the value of f in the stopping criterion.\n",
      "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "        xtol : float, optional\n",
      "            Precision goal for the value of x in the stopping\n",
      "            criterion (after applying x scaling factors). If xtol <\n",
      "            0.0, xtol is set to sqrt(machine_precision). Defaults to\n",
      "            -1.\n",
      "        pgtol : float, optional\n",
      "            Precision goal for the value of the projected gradient in\n",
      "            the stopping criterion (after applying x scaling factors).\n",
      "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
      "            Setting it to 0.0 is not recommended. Defaults to -1.\n",
      "        rescale : float, optional\n",
      "            Scaling factor (in log10) used to trigger f value\n",
      "            rescaling. If 0, rescale at each iteration. If a large\n",
      "            value, never rescale. If < 0, rescale is set to 1.3.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution.\n",
      "        nfeval : int\n",
      "            The number of function evaluations.\n",
      "        rc : int\n",
      "            Return code, see below\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'TNC' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The underlying algorithm is truncated Newton, also called\n",
      "        Newton Conjugate-Gradient. This method differs from\n",
      "        scipy.optimize.fmin_ncg in that\n",
      "        \n",
      "        1. it wraps a C implementation of the algorithm\n",
      "        2. it allows each variable to be given an upper and lower bound.\n",
      "        \n",
      "        The algorithm incorporates the bound constraints by determining\n",
      "        the descent direction as in an unconstrained truncated Newton,\n",
      "        but never taking a step-size large enough to leave the space\n",
      "        of feasible x's. The algorithm keeps track of a set of\n",
      "        currently active constraints, and ignores them when computing\n",
      "        the minimum allowable step size. (The x's associated with the\n",
      "        active constraint are kept fixed.) If the maximum allowable\n",
      "        step size is zero then a new constraint is added. At the end\n",
      "        of each iteration one of the constraints may be deemed no\n",
      "        longer active and removed. A constraint is considered\n",
      "        no longer active is if it is currently active\n",
      "        but the gradient for that variable points inward from the\n",
      "        constraint. The specific constraint removed is the one\n",
      "        associated with the variable of largest index whose\n",
      "        constraint is no longer active.\n",
      "        \n",
      "        Return codes are defined as follows::\n",
      "        \n",
      "            -1 : Infeasible (lower bound > upper bound)\n",
      "             0 : Local minimum reached (|pg| ~= 0)\n",
      "             1 : Converged (|f_n-f_(n-1)| ~= 0)\n",
      "             2 : Converged (|x_n-x_(n-1)| ~= 0)\n",
      "             3 : Max. number of function evaluations reached\n",
      "             4 : Linear search failed\n",
      "             5 : All lower bounds are equal to the upper bounds\n",
      "             6 : Unable to progress\n",
      "             7 : User requested end of minimization\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
      "        \n",
      "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
      "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
      "    \n",
      "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
      "        Bounded minimization for scalar functions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized (must accept and return scalars).\n",
      "        x1, x2 : float or array scalar\n",
      "            The optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to function.\n",
      "        xtol : float, optional\n",
      "            The convergence tolerance.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations allowed.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        disp : int, optional\n",
      "            If non-zero, print messages.\n",
      "                0 : no message printing.\n",
      "                1 : non-convergence notification messages only.\n",
      "                2 : print a message on convergence too.\n",
      "                3 : print iteration results.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters (over given interval) which minimize the\n",
      "            objective function.\n",
      "        fval : number\n",
      "            The function value at the minimum point.\n",
      "        ierr : int\n",
      "            An error flag (0 if converged, 1 if maximum number of\n",
      "            function calls reached).\n",
      "        numfunc : int\n",
      "          The number of function calls made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Bounded' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Finds a local minimizer of the scalar function `func` in the\n",
      "        interval x1 < xopt < x2 using Brent's method. (See `brent`\n",
      "        for auto-bracketing.)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        `fminbound` finds the minimum of the function in the given range.\n",
      "        The following examples illustrate the same\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fminbound(f, -1, 2)\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.fminbound(f, 1, 2)\n",
      "        >>> minimum\n",
      "        1.0000059608609866\n",
      "    \n",
      "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
      "        Find the roots of a function.\n",
      "        \n",
      "        Return the roots of the (non-linear) equations defined by\n",
      "        ``func(x) = 0`` given a starting estimate.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            A function that takes at least one (possibly vector) argument,\n",
      "            and returns a value of the same length.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the roots of ``func(x) = 0``.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to `func`.\n",
      "        fprime : callable ``f(x, *args)``, optional\n",
      "            A function to compute the Jacobian of `func` with derivatives\n",
      "            across the rows. By default, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            Specify whether the Jacobian function computes derivatives down\n",
      "            the columns (faster, because there is no transpose operation).\n",
      "        xtol : float, optional\n",
      "            The calculation will terminate if the relative error between two\n",
      "            consecutive iterates is at most `xtol`.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If zero, then\n",
      "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
      "            in `x0`.\n",
      "        band : tuple, optional\n",
      "            If set to a two-sequence containing the number of sub- and\n",
      "            super-diagonals within the band of the Jacobi matrix, the\n",
      "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
      "        epsfcn : float, optional\n",
      "            A suitable step length for the forward-difference\n",
      "            approximation of the Jacobian (for ``fprime=None``). If\n",
      "            `epsfcn` is less than the machine precision, it is assumed\n",
      "            that the relative errors in the functions are of the order of\n",
      "            the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in the interval\n",
      "            ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the\n",
      "            variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for\n",
      "            an unsuccessful call).\n",
      "        infodict : dict\n",
      "            A dictionary of optional outputs with the keys:\n",
      "        \n",
      "            ``nfev``\n",
      "                number of function calls\n",
      "            ``njev``\n",
      "                number of Jacobian calls\n",
      "            ``fvec``\n",
      "                function evaluated at the output\n",
      "            ``fjac``\n",
      "                the orthogonal matrix, q, produced by the QR\n",
      "                factorization of the final approximate Jacobian\n",
      "                matrix, stored column wise\n",
      "            ``r``\n",
      "                upper triangular matrix produced by QR factorization\n",
      "                of the same matrix\n",
      "            ``qtf``\n",
      "                the vector ``(transpose(q) * fvec)``\n",
      "        \n",
      "        ier : int\n",
      "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
      "            to `mesg` for more information.\n",
      "        mesg : str\n",
      "            If no solution is found, `mesg` details the cause of failure.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See the ``method=='hybr'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Find a solution to the system of equations:\n",
      "        ``x0*cos(x1) = 4,  x1*x0 - x1 = 5``.\n",
      "        \n",
      "        >>> from scipy.optimize import fsolve\n",
      "        >>> def func(x):\n",
      "        ...     return [x[0] * np.cos(x[1]) - 4,\n",
      "        ...             x[1] * x[0] - x[1] - 5]\n",
      "        >>> root = fsolve(func, [1, 1])\n",
      "        >>> root\n",
      "        array([6.50409711, 0.90841421])\n",
      "        >>> np.isclose(func(root), [0.0, 0.0])  # func(root) should be almost 0.0.\n",
      "        array([ True,  True])\n",
      "    \n",
      "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0, maxiter=5000)\n",
      "        Return the minimum of a function of one variable using golden section\n",
      "        method.\n",
      "        \n",
      "        Given a function of one variable and a possible bracketing interval,\n",
      "        return the minimum of the function isolated to a fractional precision of\n",
      "        tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            Objective function to minimize.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to func.\n",
      "        brack : tuple, optional\n",
      "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
      "            func(a),func(c). If bracket consists of two numbers (a,\n",
      "            c), then they are assumed to be a starting interval for a\n",
      "            downhill bracket search (see `bracket`); it doesn't always\n",
      "            mean that obtained solution will satisfy a<=x<=c.\n",
      "        tol : float, optional\n",
      "            x tolerance stop criterion\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Golden' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses analog of bisection method to decrease the bracketed\n",
      "        interval.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3, respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range ``(xa, xb)``.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.golden(f, brack=(1, 2))\n",
      "        >>> minimum\n",
      "        1.5717277788484873e-162\n",
      "        >>> minimum = optimize.golden(f, brack=(-1, 0.5, 2))\n",
      "        >>> minimum\n",
      "        -1.5717277788484873e-162\n",
      "    \n",
      "    least_squares(fun, x0, jac='2-point', bounds=(-inf, inf), method='trf', ftol=1e-08, xtol=1e-08, gtol=1e-08, x_scale=1.0, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={})\n",
      "        Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given the residuals f(x) (an m-D real function of n real\n",
      "        variables) and the loss function rho(s) (a scalar function), `least_squares`\n",
      "        finds a local minimum of the cost function F(x)::\n",
      "        \n",
      "            minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        The purpose of the loss function rho(s) is to reduce the influence of\n",
      "        outliers on the solution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Function which computes the vector of residuals, with the signature\n",
      "            ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\n",
      "            respect to its first argument. The argument ``x`` passed to this\n",
      "            function is an ndarray of shape (n,) (never a scalar, even for n=1).\n",
      "            It must allocate and return a 1-D array_like of shape (m,) or a scalar.\n",
      "            If the argument ``x`` is complex or the function ``fun`` returns\n",
      "            complex residuals, it must be wrapped in a real function of real\n",
      "            arguments, as shown at the end of the Examples section.\n",
      "        x0 : array_like with shape (n,) or float\n",
      "            Initial guess on independent variables. If float, it will be treated\n",
      "            as a 1-D array with one element.\n",
      "        jac : {'2-point', '3-point', 'cs', callable}, optional\n",
      "            Method of computing the Jacobian matrix (an m-by-n matrix, where\n",
      "            element (i, j) is the partial derivative of f[i] with respect to\n",
      "            x[j]). The keywords select a finite difference scheme for numerical\n",
      "            estimation. The scheme '3-point' is more accurate, but requires\n",
      "            twice as many operations as '2-point' (default). The scheme 'cs'\n",
      "            uses complex steps, and while potentially the most accurate, it is\n",
      "            applicable only when `fun` correctly handles complex inputs and\n",
      "            can be analytically continued to the complex plane. Method 'lm'\n",
      "            always uses the '2-point' scheme. If callable, it is used as\n",
      "            ``jac(x, *args, **kwargs)`` and should return a good approximation\n",
      "            (or the exact value) for the Jacobian as an array_like (np.atleast_2d\n",
      "            is applied), a sparse matrix (csr_matrix preferred for performance) or\n",
      "            a `scipy.sparse.linalg.LinearOperator`.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must match the size of `x0` or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : {'trf', 'dogbox', 'lm'}, optional\n",
      "            Algorithm to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm, particularly suitable\n",
      "                  for large sparse problems with bounds. Generally robust method.\n",
      "                * 'dogbox' : dogleg algorithm with rectangular trust regions,\n",
      "                  typical use case is small problems with bounds. Not recommended\n",
      "                  for problems with rank-deficient Jacobian.\n",
      "                * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\n",
      "                  Doesn't handle bounds and sparse Jacobians. Usually the most\n",
      "                  efficient method for small unconstrained problems.\n",
      "        \n",
      "            Default is 'trf'. See Notes for more information.\n",
      "        ftol : float or None, optional\n",
      "            Tolerance for termination by the change of the cost function. Default\n",
      "            is 1e-8. The optimization process is stopped when ``dF < ftol * F``,\n",
      "            and there was an adequate agreement between a local quadratic model and\n",
      "            the true model in the last step.\n",
      "        \n",
      "            If None and 'method' is not 'lm', the termination by this condition is\n",
      "            disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "            machine epsilon.\n",
      "        xtol : float or None, optional\n",
      "            Tolerance for termination by the change of the independent variables.\n",
      "            Default is 1e-8. The exact condition depends on the `method` used:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``.\n",
      "                * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\n",
      "                  a trust-region radius and ``xs`` is the value of ``x``\n",
      "                  scaled according to `x_scale` parameter (see below).\n",
      "        \n",
      "            If None and 'method' is not 'lm', the termination by this condition is\n",
      "            disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "            machine epsilon.\n",
      "        gtol : float or None, optional\n",
      "            Tolerance for termination by the norm of the gradient. Default is 1e-8.\n",
      "            The exact condition depends on a `method` used:\n",
      "        \n",
      "                * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\n",
      "                  ``g_scaled`` is the value of the gradient scaled to account for\n",
      "                  the presence of the bounds [STIR]_.\n",
      "                * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\n",
      "                  ``g_free`` is the gradient with respect to the variables which\n",
      "                  are not in the optimal state on the boundary.\n",
      "                * For 'lm' : the maximum absolute value of the cosine of angles\n",
      "                  between columns of the Jacobian and the residual vector is less\n",
      "                  than `gtol`, or the residual vector is zero.\n",
      "        \n",
      "            If None and 'method' is not 'lm', the termination by this condition is\n",
      "            disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "            machine epsilon.\n",
      "        x_scale : array_like or 'jac', optional\n",
      "            Characteristic scale of each variable. Setting `x_scale` is equivalent\n",
      "            to reformulating the problem in scaled variables ``xs = x / x_scale``.\n",
      "            An alternative view is that the size of a trust region along jth\n",
      "            dimension is proportional to ``x_scale[j]``. Improved convergence may\n",
      "            be achieved by setting `x_scale` such that a step of a given size\n",
      "            along any of the scaled variables has a similar effect on the cost\n",
      "            function. If set to 'jac', the scale is iteratively updated using the\n",
      "            inverse norms of the columns of the Jacobian matrix (as described in\n",
      "            [JJMore]_).\n",
      "        loss : str or callable, optional\n",
      "            Determines the loss function. The following keyword values are allowed:\n",
      "        \n",
      "                * 'linear' (default) : ``rho(z) = z``. Gives a standard\n",
      "                  least-squares problem.\n",
      "                * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\n",
      "                  approximation of l1 (absolute value) loss. Usually a good\n",
      "                  choice for robust least squares.\n",
      "                * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\n",
      "                  similarly to 'soft_l1'.\n",
      "                * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\n",
      "                  influence, but may cause difficulties in optimization process.\n",
      "                * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\n",
      "                  a single residual, has properties similar to 'cauchy'.\n",
      "        \n",
      "            If callable, it must take a 1-D ndarray ``z=f**2`` and return an\n",
      "            array_like with shape (3, m) where row 0 contains function values,\n",
      "            row 1 contains first derivatives and row 2 contains second\n",
      "            derivatives. Method 'lm' supports only 'linear' loss.\n",
      "        f_scale : float, optional\n",
      "            Value of soft margin between inlier and outlier residuals, default\n",
      "            is 1.0. The loss function is evaluated as follows\n",
      "            ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\n",
      "            and ``rho`` is determined by `loss` parameter. This parameter has\n",
      "            no effect with ``loss='linear'``, but for other `loss` values it is\n",
      "            of crucial importance.\n",
      "        max_nfev : None or int, optional\n",
      "            Maximum number of function evaluations before the termination.\n",
      "            If None (default), the value is chosen automatically:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : 100 * n.\n",
      "                * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\n",
      "                  otherwise (because 'lm' counts function calls in Jacobian\n",
      "                  estimation).\n",
      "        \n",
      "        diff_step : None or array_like, optional\n",
      "            Determines the relative step size for the finite difference\n",
      "            approximation of the Jacobian. The actual step is computed as\n",
      "            ``x * diff_step``. If None (default), then `diff_step` is taken to be\n",
      "            a conventional \"optimal\" power of machine epsilon for the finite\n",
      "            difference scheme used [NR]_.\n",
      "        tr_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method for solving trust-region subproblems, relevant only for 'trf'\n",
      "            and 'dogbox' methods.\n",
      "        \n",
      "                * 'exact' is suitable for not very large problems with dense\n",
      "                  Jacobian matrices. The computational complexity per iteration is\n",
      "                  comparable to a singular value decomposition of the Jacobian\n",
      "                  matrix.\n",
      "                * 'lsmr' is suitable for problems with sparse and large Jacobian\n",
      "                  matrices. It uses the iterative procedure\n",
      "                  `scipy.sparse.linalg.lsmr` for finding a solution of a linear\n",
      "                  least-squares problem and only requires matrix-vector product\n",
      "                  evaluations.\n",
      "        \n",
      "            If None (default), the solver is chosen based on the type of Jacobian\n",
      "            returned on the first iteration.\n",
      "        tr_options : dict, optional\n",
      "            Keyword options passed to trust-region solver.\n",
      "        \n",
      "                * ``tr_solver='exact'``: `tr_options` are ignored.\n",
      "                * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\n",
      "                  Additionally,  ``method='trf'`` supports  'regularize' option\n",
      "                  (bool, default is True), which adds a regularization term to the\n",
      "                  normal equation, which improves convergence if the Jacobian is\n",
      "                  rank-deficient [Byrd]_ (eq. 3.4).\n",
      "        \n",
      "        jac_sparsity : {None, array_like, sparse matrix}, optional\n",
      "            Defines the sparsity structure of the Jacobian matrix for finite\n",
      "            difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "            only few non-zero elements in *each* row, providing the sparsity\n",
      "            structure will greatly speed up the computations [Curtis]_. A zero\n",
      "            entry means that a corresponding element in the Jacobian is identically\n",
      "            zero. If provided, forces the use of 'lsmr' trust-region solver.\n",
      "            If None (default), then dense differencing will be used. Has no effect\n",
      "            for 'lm' method.\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 (default) : work silently.\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations (not supported by 'lm'\n",
      "                  method).\n",
      "        \n",
      "        args, kwargs : tuple and dict, optional\n",
      "            Additional arguments passed to `fun` and `jac`. Both empty by default.\n",
      "            The calling signature is ``fun(x, *args, **kwargs)`` and the same for\n",
      "            `jac`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        result : OptimizeResult\n",
      "            `OptimizeResult` with the following fields defined:\n",
      "        \n",
      "                x : ndarray, shape (n,)\n",
      "                    Solution found.\n",
      "                cost : float\n",
      "                    Value of the cost function at the solution.\n",
      "                fun : ndarray, shape (m,)\n",
      "                    Vector of residuals at the solution.\n",
      "                jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\n",
      "                    Modified Jacobian matrix at the solution, in the sense that J^T J\n",
      "                    is a Gauss-Newton approximation of the Hessian of the cost function.\n",
      "                    The type is the same as the one used by the algorithm.\n",
      "                grad : ndarray, shape (m,)\n",
      "                    Gradient of the cost function at the solution.\n",
      "                optimality : float\n",
      "                    First-order optimality measure. In unconstrained problems, it is\n",
      "                    always the uniform norm of the gradient. In constrained problems,\n",
      "                    it is the quantity which was compared with `gtol` during iterations.\n",
      "                active_mask : ndarray of int, shape (n,)\n",
      "                    Each component shows whether a corresponding constraint is active\n",
      "                    (that is, whether a variable is at the bound):\n",
      "        \n",
      "                        *  0 : a constraint is not active.\n",
      "                        * -1 : a lower bound is active.\n",
      "                        *  1 : an upper bound is active.\n",
      "        \n",
      "                    Might be somewhat arbitrary for 'trf' method as it generates a\n",
      "                    sequence of strictly feasible iterates and `active_mask` is\n",
      "                    determined within a tolerance threshold.\n",
      "                nfev : int\n",
      "                    Number of function evaluations done. Methods 'trf' and 'dogbox' do\n",
      "                    not count function calls for numerical Jacobian approximation, as\n",
      "                    opposed to 'lm' method.\n",
      "                njev : int or None\n",
      "                    Number of Jacobian evaluations done. If numerical Jacobian\n",
      "                    approximation is used in 'lm' method, it is set to None.\n",
      "                status : int\n",
      "                    The reason for algorithm termination:\n",
      "        \n",
      "                        * -1 : improper input parameters status returned from MINPACK.\n",
      "                        *  0 : the maximum number of function evaluations is exceeded.\n",
      "                        *  1 : `gtol` termination condition is satisfied.\n",
      "                        *  2 : `ftol` termination condition is satisfied.\n",
      "                        *  3 : `xtol` termination condition is satisfied.\n",
      "                        *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\n",
      "        \n",
      "                message : str\n",
      "                    Verbal description of the termination reason.\n",
      "                success : bool\n",
      "                    True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        leastsq : A legacy wrapper for the MINPACK implementation of the\n",
      "                  Levenberg-Marquadt algorithm.\n",
      "        curve_fit : Least-squares minimization applied to a curve-fitting problem.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\n",
      "        algorithms implemented in MINPACK (lmder, lmdif). It runs the\n",
      "        Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\n",
      "        The implementation is based on paper [JJMore]_, it is very robust and\n",
      "        efficient with a lot of smart tricks. It should be your first choice\n",
      "        for unconstrained problems. Note that it doesn't support bounds. Also,\n",
      "        it doesn't work when m < n.\n",
      "        \n",
      "        Method 'trf' (Trust Region Reflective) is motivated by the process of\n",
      "        solving a system of equations, which constitute the first-order optimality\n",
      "        condition for a bound-constrained minimization problem as formulated in\n",
      "        [STIR]_. The algorithm iteratively solves trust-region subproblems\n",
      "        augmented by a special diagonal quadratic term and with trust-region shape\n",
      "        determined by the distance from the bounds and the direction of the\n",
      "        gradient. This enhancements help to avoid making steps directly into bounds\n",
      "        and efficiently explore the whole space of variables. To further improve\n",
      "        convergence, the algorithm considers search directions reflected from the\n",
      "        bounds. To obey theoretical requirements, the algorithm keeps iterates\n",
      "        strictly feasible. With dense Jacobians trust-region subproblems are\n",
      "        solved by an exact method very similar to the one described in [JJMore]_\n",
      "        (and implemented in MINPACK). The difference from the MINPACK\n",
      "        implementation is that a singular value decomposition of a Jacobian\n",
      "        matrix is done once per iteration, instead of a QR decomposition and series\n",
      "        of Givens rotation eliminations. For large sparse Jacobians a 2-D subspace\n",
      "        approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\n",
      "        The subspace is spanned by a scaled gradient and an approximate\n",
      "        Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\n",
      "        constraints are imposed the algorithm is very similar to MINPACK and has\n",
      "        generally comparable performance. The algorithm works quite robust in\n",
      "        unbounded and bounded problems, thus it is chosen as a default algorithm.\n",
      "        \n",
      "        Method 'dogbox' operates in a trust-region framework, but considers\n",
      "        rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\n",
      "        The intersection of a current trust region and initial bounds is again\n",
      "        rectangular, so on each iteration a quadratic minimization problem subject\n",
      "        to bound constraints is solved approximately by Powell's dogleg method\n",
      "        [NumOpt]_. The required Gauss-Newton step can be computed exactly for\n",
      "        dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\n",
      "        sparse Jacobians. The algorithm is likely to exhibit slow convergence when\n",
      "        the rank of Jacobian is less than the number of variables. The algorithm\n",
      "        often outperforms 'trf' in bounded problems with a small number of\n",
      "        variables.\n",
      "        \n",
      "        Robust loss functions are implemented as described in [BA]_. The idea\n",
      "        is to modify a residual vector and a Jacobian matrix on each iteration\n",
      "        such that computed gradient and Gauss-Newton Hessian approximation match\n",
      "        the true gradient and Hessian approximation of the cost function. Then\n",
      "        the algorithm proceeds in a normal way, i.e., robust loss functions are\n",
      "        implemented as a simple wrapper over standard least-squares algorithms.\n",
      "        \n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\n",
      "                Computing. 3rd edition\", Sec. 5.7.\n",
      "        .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\n",
      "                  solution of the trust region problem by minimization over\n",
      "                  two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\n",
      "                  1988.\n",
      "        .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\n",
      "                    sparse Jacobian matrices\", Journal of the Institute of\n",
      "                    Mathematics and its Applications, 13, pp. 117-120, 1974.\n",
      "        .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\n",
      "                    and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\n",
      "                    Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\n",
      "        .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\n",
      "                    Dogleg Approach for Unconstrained and Bound Constrained\n",
      "                    Nonlinear Optimization\", WSEAS International Conference on\n",
      "                    Applied Mathematics, Corfu, Greece, 2004.\n",
      "        .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\n",
      "                    2nd edition\", Chapter 4.\n",
      "        .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\n",
      "                Proceedings of the International Workshop on Vision Algorithms:\n",
      "                Theory and Practice, pp. 298-372, 1999.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example we find a minimum of the Rosenbrock function without bounds\n",
      "        on independent variables.\n",
      "        \n",
      "        >>> def fun_rosenbrock(x):\n",
      "        ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\n",
      "        \n",
      "        Notice that we only provide the vector of the residuals. The algorithm\n",
      "        constructs the cost function as a sum of squares of the residuals, which\n",
      "        gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> x0_rosenbrock = np.array([2, 2])\n",
      "        >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\n",
      "        >>> res_1.x\n",
      "        array([ 1.,  1.])\n",
      "        >>> res_1.cost\n",
      "        9.8669242910846867e-30\n",
      "        >>> res_1.optimality\n",
      "        8.8928864934219529e-14\n",
      "        \n",
      "        We now constrain the variables, in such a way that the previous solution\n",
      "        becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\n",
      "        ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\n",
      "        to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\n",
      "        \n",
      "        We also provide the analytic Jacobian:\n",
      "        \n",
      "        >>> def jac_rosenbrock(x):\n",
      "        ...     return np.array([\n",
      "        ...         [-20 * x[0], 10],\n",
      "        ...         [-1, 0]])\n",
      "        \n",
      "        Putting this all together, we see that the new solution lies on the bound:\n",
      "        \n",
      "        >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\n",
      "        ...                       bounds=([-np.inf, 1.5], np.inf))\n",
      "        >>> res_2.x\n",
      "        array([ 1.22437075,  1.5       ])\n",
      "        >>> res_2.cost\n",
      "        0.025213093946805685\n",
      "        >>> res_2.optimality\n",
      "        1.5885401433157753e-07\n",
      "        \n",
      "        Now we solve a system of equations (i.e., the cost function should be zero\n",
      "        at a minimum) for a Broyden tridiagonal vector-valued function of 100000\n",
      "        variables:\n",
      "        \n",
      "        >>> def fun_broyden(x):\n",
      "        ...     f = (3 - x) * x + 1\n",
      "        ...     f[1:] -= x[:-1]\n",
      "        ...     f[:-1] -= 2 * x[1:]\n",
      "        ...     return f\n",
      "        \n",
      "        The corresponding Jacobian matrix is sparse. We tell the algorithm to\n",
      "        estimate it by finite differences and provide the sparsity structure of\n",
      "        Jacobian to significantly speed up this process.\n",
      "        \n",
      "        >>> from scipy.sparse import lil_matrix\n",
      "        >>> def sparsity_broyden(n):\n",
      "        ...     sparsity = lil_matrix((n, n), dtype=int)\n",
      "        ...     i = np.arange(n)\n",
      "        ...     sparsity[i, i] = 1\n",
      "        ...     i = np.arange(1, n)\n",
      "        ...     sparsity[i, i - 1] = 1\n",
      "        ...     i = np.arange(n - 1)\n",
      "        ...     sparsity[i, i + 1] = 1\n",
      "        ...     return sparsity\n",
      "        ...\n",
      "        >>> n = 100000\n",
      "        >>> x0_broyden = -np.ones(n)\n",
      "        ...\n",
      "        >>> res_3 = least_squares(fun_broyden, x0_broyden,\n",
      "        ...                       jac_sparsity=sparsity_broyden(n))\n",
      "        >>> res_3.cost\n",
      "        4.5687069299604613e-23\n",
      "        >>> res_3.optimality\n",
      "        1.1650454296851518e-11\n",
      "        \n",
      "        Let's also solve a curve fitting problem using robust loss function to\n",
      "        take care of outliers in the data. Define the model function as\n",
      "        ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\n",
      "        observation and a, b, c are parameters to estimate.\n",
      "        \n",
      "        First, define the function which generates the data with noise and\n",
      "        outliers, define the model parameters, and generate data:\n",
      "        \n",
      "        >>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):\n",
      "        ...     y = a + b * np.exp(t * c)\n",
      "        ...\n",
      "        ...     rnd = np.random.RandomState(random_state)\n",
      "        ...     error = noise * rnd.randn(t.size)\n",
      "        ...     outliers = rnd.randint(0, t.size, n_outliers)\n",
      "        ...     error[outliers] *= 10\n",
      "        ...\n",
      "        ...     return y + error\n",
      "        ...\n",
      "        >>> a = 0.5\n",
      "        >>> b = 2.0\n",
      "        >>> c = -1\n",
      "        >>> t_min = 0\n",
      "        >>> t_max = 10\n",
      "        >>> n_points = 15\n",
      "        ...\n",
      "        >>> t_train = np.linspace(t_min, t_max, n_points)\n",
      "        >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\n",
      "        \n",
      "        Define function for computing residuals and initial estimate of\n",
      "        parameters.\n",
      "        \n",
      "        >>> def fun(x, t, y):\n",
      "        ...     return x[0] + x[1] * np.exp(x[2] * t) - y\n",
      "        ...\n",
      "        >>> x0 = np.array([1.0, 1.0, 0.0])\n",
      "        \n",
      "        Compute a standard least-squares solution:\n",
      "        \n",
      "        >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\n",
      "        \n",
      "        Now compute two solutions with two different robust loss functions. The\n",
      "        parameter `f_scale` is set to 0.1, meaning that inlier residuals should\n",
      "        not significantly exceed 0.1 (the noise level used).\n",
      "        \n",
      "        >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\n",
      "        ...                             args=(t_train, y_train))\n",
      "        >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\n",
      "        ...                         args=(t_train, y_train))\n",
      "        \n",
      "        And, finally, plot all the curves. We see that by selecting an appropriate\n",
      "        `loss`  we can get estimates close to optimal even in the presence of\n",
      "        strong outliers. But keep in mind that generally it is recommended to try\n",
      "        'soft_l1' or 'huber' losses first (if at all necessary) as the other two\n",
      "        options may cause difficulties in optimization process.\n",
      "        \n",
      "        >>> t_test = np.linspace(t_min, t_max, n_points * 10)\n",
      "        >>> y_true = gen_data(t_test, a, b, c)\n",
      "        >>> y_lsq = gen_data(t_test, *res_lsq.x)\n",
      "        >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
      "        >>> y_log = gen_data(t_test, *res_log.x)\n",
      "        ...\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(t_train, y_train, 'o')\n",
      "        >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\n",
      "        >>> plt.plot(t_test, y_lsq, label='linear loss')\n",
      "        >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\n",
      "        >>> plt.plot(t_test, y_log, label='cauchy loss')\n",
      "        >>> plt.xlabel(\"t\")\n",
      "        >>> plt.ylabel(\"y\")\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "        \n",
      "        In the next example, we show how complex-valued residual functions of\n",
      "        complex variables can be optimized with ``least_squares()``. Consider the\n",
      "        following function:\n",
      "        \n",
      "        >>> def f(z):\n",
      "        ...     return z - (0.5 + 0.5j)\n",
      "        \n",
      "        We wrap it into a function of real variables that returns real residuals\n",
      "        by simply handling the real and imaginary parts as independent variables:\n",
      "        \n",
      "        >>> def f_wrap(x):\n",
      "        ...     fx = f(x[0] + 1j*x[1])\n",
      "        ...     return np.array([fx.real, fx.imag])\n",
      "        \n",
      "        Thus, instead of the original m-D complex function of n complex\n",
      "        variables we optimize a 2m-D real function of 2n real variables:\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\n",
      "        >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\n",
      "        >>> z\n",
      "        (0.49999999999925893+0.49999999999925893j)\n",
      "    \n",
      "    leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
      "        Minimize the sum of squares of a set of equations.\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            x = arg min(sum(func(y)**2,axis=0))\n",
      "                     y\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Should take at least one (possibly length N vector) argument and\n",
      "            returns M floating point numbers. It must not return NaNs or\n",
      "            fitting might fail.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the minimization.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to func are placed in this tuple.\n",
      "        Dfun : callable, optional\n",
      "            A function or method to compute the Jacobian of func with derivatives\n",
      "            across the rows. If this is None, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            non-zero to return all optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            non-zero to specify that the Jacobian function computes derivatives\n",
      "            down the columns (faster, because there is no transpose operation).\n",
      "        ftol : float, optional\n",
      "            Relative error desired in the sum of squares.\n",
      "        xtol : float, optional\n",
      "            Relative error desired in the approximate solution.\n",
      "        gtol : float, optional\n",
      "            Orthogonality desired between the function vector and the columns of\n",
      "            the Jacobian.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If `Dfun` is provided,\n",
      "            then the default `maxfev` is 100*(N+1) where N is the number of elements\n",
      "            in x0, otherwise the default `maxfev` is 200*(N+1).\n",
      "        epsfcn : float, optional\n",
      "            A variable used in determining a suitable step length for the forward-\n",
      "            difference approximation of the Jacobian (for Dfun=None).\n",
      "            Normally the actual step length will be sqrt(epsfcn)*x\n",
      "            If epsfcn is less than the machine precision, it is assumed that the\n",
      "            relative errors are of the order of the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for an unsuccessful\n",
      "            call).\n",
      "        cov_x : ndarray\n",
      "            The inverse of the Hessian. `fjac` and `ipvt` are used to construct an\n",
      "            estimate of the Hessian. A value of None indicates a singular matrix,\n",
      "            which means the curvature in parameters `x` is numerically flat. To\n",
      "            obtain the covariance matrix of the parameters `x`, `cov_x` must be\n",
      "            multiplied by the variance of the residuals -- see curve_fit.\n",
      "        infodict : dict\n",
      "            a dictionary of optional outputs with the keys:\n",
      "        \n",
      "            ``nfev``\n",
      "                The number of function calls\n",
      "            ``fvec``\n",
      "                The function evaluated at the output\n",
      "            ``fjac``\n",
      "                A permutation of the R matrix of a QR\n",
      "                factorization of the final approximate\n",
      "                Jacobian matrix, stored column wise.\n",
      "                Together with ipvt, the covariance of the\n",
      "                estimate can be approximated.\n",
      "            ``ipvt``\n",
      "                An integer array of length N which defines\n",
      "                a permutation matrix, p, such that\n",
      "                fjac*p = q*r, where r is upper triangular\n",
      "                with diagonal elements of nonincreasing\n",
      "                magnitude. Column j of p is column ipvt(j)\n",
      "                of the identity matrix.\n",
      "            ``qtf``\n",
      "                The vector (transpose(q) * fvec).\n",
      "        \n",
      "        mesg : str\n",
      "            A string message giving information about the cause of failure.\n",
      "        ier : int\n",
      "            An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n",
      "            found. Otherwise, the solution was not found. In either case, the\n",
      "            optional output variable 'mesg' gives more information.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Newer interface to solve nonlinear least-squares problems\n",
      "            with bounds on the variables. See ``method=='lm'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
      "        \n",
      "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
      "        objective function.\n",
      "        This approximation assumes that the objective function is based on the\n",
      "        difference between some observed target data (ydata) and a (non-linear)\n",
      "        function of the parameters `f(xdata, params)` ::\n",
      "        \n",
      "               func(params) = ydata - f(xdata, params)\n",
      "        \n",
      "        so that the objective function is ::\n",
      "        \n",
      "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
      "             params\n",
      "        \n",
      "        The solution, `x`, is always a 1-D array, regardless of the shape of `x0`,\n",
      "        or whether `x0` is a scalar.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import leastsq\n",
      "        >>> def func(x):\n",
      "        ...     return 2*(x-3)**2+1\n",
      "        >>> leastsq(func, 0)\n",
      "        (array([2.99999999]), 1)\n",
      "    \n",
      "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=None, extra_condition=None, maxiter=10)\n",
      "        Find alpha that satisfies strong Wolfe conditions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function.\n",
      "        myfprime : callable f'(x,*args)\n",
      "            Objective function gradient.\n",
      "        xk : ndarray\n",
      "            Starting point.\n",
      "        pk : ndarray\n",
      "            Search direction.\n",
      "        gfk : ndarray, optional\n",
      "            Gradient value for x=xk (xk being the current parameter\n",
      "            estimate). Will be recomputed if omitted.\n",
      "        old_fval : float, optional\n",
      "            Function value for x=xk. Will be recomputed if omitted.\n",
      "        old_old_fval : float, optional\n",
      "            Function value for the point preceding x=xk.\n",
      "        args : tuple, optional\n",
      "            Additional arguments passed to objective function.\n",
      "        c1 : float, optional\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, optional\n",
      "            Parameter for curvature condition rule.\n",
      "        amax : float, optional\n",
      "            Maximum step size\n",
      "        extra_condition : callable, optional\n",
      "            A callable of the form ``extra_condition(alpha, x, f, g)``\n",
      "            returning a boolean. Arguments are the proposed step ``alpha``\n",
      "            and the corresponding ``x``, ``f`` and ``g`` values. The line search\n",
      "            accepts the value of ``alpha`` only if this\n",
      "            callable returns ``True``. If the callable returns ``False``\n",
      "            for the step length, the algorithm will continue with\n",
      "            new iterates. The callable is only called for iterates\n",
      "            satisfying the strong Wolfe conditions.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alpha : float or None\n",
      "            Alpha for which ``x_new = x0 + alpha * pk``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        fc : int\n",
      "            Number of function evaluations made.\n",
      "        gc : int\n",
      "            Number of gradient evaluations made.\n",
      "        new_fval : float or None\n",
      "            New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        old_fval : float\n",
      "            Old function value ``f(x0)``.\n",
      "        new_slope : float or None\n",
      "            The local slope along the search direction at the\n",
      "            new value ``<myfprime(x_new), pk>``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses the line search algorithm to enforce strong Wolfe\n",
      "        conditions. See Wright and Nocedal, 'Numerical Optimization',\n",
      "        1999, pp. 59-61.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import line_search\n",
      "        \n",
      "        A objective function and its gradient are defined.\n",
      "        \n",
      "        >>> def obj_func(x):\n",
      "        ...     return (x[0])**2+(x[1])**2\n",
      "        >>> def obj_grad(x):\n",
      "        ...     return [2*x[0], 2*x[1]]\n",
      "        \n",
      "        We can find alpha that satisfies strong Wolfe conditions.\n",
      "        \n",
      "        >>> start_point = np.array([1.8, 1.7])\n",
      "        >>> search_gradient = np.array([-1.0, -1.0])\n",
      "        >>> line_search(obj_func, obj_grad, start_point, search_gradient)\n",
      "        (1.0, 2, 1, 1.1300000000000001, 6.13, [1.6, 1.4])\n",
      "    \n",
      "    linear_sum_assignment(cost_matrix, maximize=False)\n",
      "        Solve the linear sum assignment problem.\n",
      "        \n",
      "        The linear sum assignment problem is also known as minimum weight matching\n",
      "        in bipartite graphs. A problem instance is described by a matrix C, where\n",
      "        each C[i,j] is the cost of matching vertex i of the first partite set\n",
      "        (a \"worker\") and vertex j of the second set (a \"job\"). The goal is to find\n",
      "        a complete assignment of workers to jobs of minimal cost.\n",
      "        \n",
      "        Formally, let X be a boolean matrix where :math:`X[i,j] = 1` iff row i is\n",
      "        assigned to column j. Then the optimal assignment has cost\n",
      "        \n",
      "        .. math::\n",
      "            \\min \\sum_i \\sum_j C_{i,j} X_{i,j}\n",
      "        \n",
      "        where, in the case where the matrix X is square, each row is assigned to\n",
      "        exactly one column, and each column to exactly one row.\n",
      "        \n",
      "        This function can also solve a generalization of the classic assignment\n",
      "        problem where the cost matrix is rectangular. If it has more rows than\n",
      "        columns, then not every row needs to be assigned to a column, and vice\n",
      "        versa.\n",
      "        \n",
      "        The problem is also solved for sparse inputs in\n",
      "        :func:`scipy.sparse.csgraph.min_weight_full_bipartite_matching` which\n",
      "        may perform better if the input is sparse, or for certain classes of\n",
      "        problems, such as uniformly distributed costs.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cost_matrix : array\n",
      "            The cost matrix of the bipartite graph.\n",
      "        \n",
      "        maximize : bool (default: False)\n",
      "            Calculates a maximum weight matching if true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        row_ind, col_ind : array\n",
      "            An array of row indices and one of corresponding column indices giving\n",
      "            the optimal assignment. The cost of the assignment can be computed\n",
      "            as ``cost_matrix[row_ind, col_ind].sum()``. The row indices will be\n",
      "            sorted; in the case of a square cost matrix they will be equal to\n",
      "            ``numpy.arange(cost_matrix.shape[0])``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        1. https://en.wikipedia.org/wiki/Assignment_problem\n",
      "        \n",
      "        2. DF Crouse. On implementing 2D rectangular assignment algorithms.\n",
      "           *IEEE Transactions on Aerospace and Electronic Systems*,\n",
      "           52(4):1679-1696, August 2016, :doi:`10.1109/TAES.2016.140952`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cost = np.array([[4, 1, 3], [2, 0, 5], [3, 2, 2]])\n",
      "        >>> from scipy.optimize import linear_sum_assignment\n",
      "        >>> row_ind, col_ind = linear_sum_assignment(cost)\n",
      "        >>> col_ind\n",
      "        array([1, 0, 2])\n",
      "        >>> cost[row_ind, col_ind].sum()\n",
      "        5\n",
      "    \n",
      "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a scalar Jacobian approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            The Jacobian approximation is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='linearmixing'`` in particular.\n",
      "    \n",
      "    linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='interior-point', callback=None, options=None, x0=None)\n",
      "        Linear programming: minimize a linear objective function subject to linear\n",
      "        equality and inequality constraints.\n",
      "        \n",
      "        Linear programming solves problems of the following form:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_x \\ & c^T x \\\\\n",
      "            \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "            & A_{eq} x = b_{eq},\\\\\n",
      "            & l \\leq x \\leq u ,\n",
      "        \n",
      "        where :math:`x` is a vector of decision variables; :math:`c`,\n",
      "        :math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      "        :math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "        \n",
      "        Alternatively, that's:\n",
      "        \n",
      "        minimize::\n",
      "        \n",
      "            c @ x\n",
      "        \n",
      "        such that::\n",
      "        \n",
      "            A_ub @ x <= b_ub\n",
      "            A_eq @ x == b_eq\n",
      "            lb <= x <= ub\n",
      "        \n",
      "        Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "        ``bounds``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        c : 1-D array\n",
      "            The coefficients of the linear objective function to be minimized.\n",
      "        A_ub : 2-D array, optional\n",
      "            The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "            coefficients of a linear inequality constraint on ``x``.\n",
      "        b_ub : 1-D array, optional\n",
      "            The inequality constraint vector. Each element represents an\n",
      "            upper bound on the corresponding value of ``A_ub @ x``.\n",
      "        A_eq : 2-D array, optional\n",
      "            The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "            coefficients of a linear equality constraint on ``x``.\n",
      "        b_eq : 1-D array, optional\n",
      "            The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "            the corresponding element of ``b_eq``.\n",
      "        bounds : sequence, optional\n",
      "            A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the minimum and maximum values of that decision variable. Use ``None``\n",
      "            to indicate that there is no bound. By default, bounds are\n",
      "            ``(0, None)`` (all decision variables are non-negative).\n",
      "            If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "            ``max`` will serve as bounds for all decision variables.\n",
      "        method : str, optional\n",
      "            The algorithm used to solve the standard form problem.\n",
      "            :ref:`'highs-ds' <optimize.linprog-highs-ds>`,\n",
      "            :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`,\n",
      "            :ref:`'highs' <optimize.linprog-highs>`,\n",
      "            :ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      "            :ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      "            :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "            are supported.\n",
      "        callback : callable, optional\n",
      "            If a callback function is provided, it will be called at least once per\n",
      "            iteration of the algorithm. The callback function must accept a single\n",
      "            `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "        \n",
      "            x : 1-D array\n",
      "                The current solution vector.\n",
      "            fun : float\n",
      "                The current value of the objective function ``c @ x``.\n",
      "            success : bool\n",
      "                ``True`` when the algorithm has completed successfully.\n",
      "            slack : 1-D array\n",
      "                The (nominally positive) values of the slack,\n",
      "                ``b_ub - A_ub @ x``.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints,\n",
      "                ``b_eq - A_eq @ x``.\n",
      "            phase : int\n",
      "                The phase of the algorithm being executed.\n",
      "            status : int\n",
      "                An integer representing the status of the algorithm.\n",
      "        \n",
      "                ``0`` : Optimization proceeding nominally.\n",
      "        \n",
      "                ``1`` : Iteration limit reached.\n",
      "        \n",
      "                ``2`` : Problem appears to be infeasible.\n",
      "        \n",
      "                ``3`` : Problem appears to be unbounded.\n",
      "        \n",
      "                ``4`` : Numerical difficulties encountered.\n",
      "        \n",
      "                nit : int\n",
      "                    The current iteration number.\n",
      "                message : str\n",
      "                    A string descriptor of the algorithm status.\n",
      "        \n",
      "            Callback functions are not currently supported by the HiGHS methods.\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            options:\n",
      "        \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "                Default: see method-specific documentation.\n",
      "            disp : bool\n",
      "                Set to ``True`` to print convergence messages.\n",
      "                Default: ``False``.\n",
      "            presolve : bool\n",
      "                Set to ``False`` to disable automatic presolve.\n",
      "                Default: ``True``.\n",
      "        \n",
      "            All methods except the HiGHS solvers also accept:\n",
      "        \n",
      "            tol : float\n",
      "                A tolerance which determines when a residual is \"close enough\" to\n",
      "                zero to be considered exactly zero.\n",
      "            autoscale : bool\n",
      "                Set to ``True`` to automatically perform equilibration.\n",
      "                Consider using this option if the numerical values in the\n",
      "                constraints are separated by several orders of magnitude.\n",
      "                Default: ``False``.\n",
      "            rr : bool\n",
      "                Set to ``False`` to disable automatic redundancy removal.\n",
      "                Default: ``True``.\n",
      "            rr_method : string\n",
      "                Method used to identify and remove redundant rows from the\n",
      "                equality constraint matrix after presolve. For problems with\n",
      "                dense input, the available methods for redundancy removal are:\n",
      "        \n",
      "                \"SVD\":\n",
      "                    Repeatedly performs singular value decomposition on\n",
      "                    the matrix, detecting redundant rows based on nonzeros\n",
      "                    in the left singular vectors that correspond with\n",
      "                    zero singular values. May be fast when the matrix is\n",
      "                    nearly full rank.\n",
      "                \"pivot\":\n",
      "                    Uses the algorithm presented in [5]_ to identify\n",
      "                    redundant rows.\n",
      "                \"ID\":\n",
      "                    Uses a randomized interpolative decomposition.\n",
      "                    Identifies columns of the matrix transpose not used in\n",
      "                    a full-rank interpolative decomposition of the matrix.\n",
      "                None:\n",
      "                    Uses \"svd\" if the matrix is nearly full rank, that is,\n",
      "                    the difference between the matrix rank and the number\n",
      "                    of rows is less than five. If not, uses \"pivot\". The\n",
      "                    behavior of this default is subject to change without\n",
      "                    prior notice.\n",
      "        \n",
      "                Default: None.\n",
      "                For problems with sparse input, this option is ignored, and the\n",
      "                pivot-based algorithm presented in [5]_ is used.\n",
      "        \n",
      "            For method-specific options, see\n",
      "            :func:`show_options('linprog') <show_options>`.\n",
      "        \n",
      "        x0 : 1-D array, optional\n",
      "            Guess values of the decision variables, which will be refined by\n",
      "            the optimization algorithm. This argument is currently used only by the\n",
      "            'revised simplex' method, and can only be used if `x0` represents a\n",
      "            basic feasible solution.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "        \n",
      "            x : 1-D array\n",
      "                The values of the decision variables that minimizes the\n",
      "                objective function while satisfying the constraints.\n",
      "            fun : float\n",
      "                The optimal value of the objective function ``c @ x``.\n",
      "            slack : 1-D array\n",
      "                The (nominally positive) values of the slack variables,\n",
      "                ``b_ub - A_ub @ x``.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints,\n",
      "                ``b_eq - A_eq @ x``.\n",
      "            success : bool\n",
      "                ``True`` when the algorithm succeeds in finding an optimal\n",
      "                solution.\n",
      "            status : int\n",
      "                An integer representing the exit status of the algorithm.\n",
      "        \n",
      "                ``0`` : Optimization terminated successfully.\n",
      "        \n",
      "                ``1`` : Iteration limit reached.\n",
      "        \n",
      "                ``2`` : Problem appears to be infeasible.\n",
      "        \n",
      "                ``3`` : Problem appears to be unbounded.\n",
      "        \n",
      "                ``4`` : Numerical difficulties encountered.\n",
      "        \n",
      "            nit : int\n",
      "                The total number of iterations performed in all phases.\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the algorithm.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "        \n",
      "        `'highs-ds'` and\n",
      "        `'highs-ipm'` are interfaces to the\n",
      "        HiGHS simplex and interior-point method solvers [13]_, respectively.\n",
      "        `'highs'` chooses between\n",
      "        the two automatically. These are the fastest linear\n",
      "        programming solvers in SciPy, especially for large, sparse problems;\n",
      "        which of these two is faster is problem-dependent.\n",
      "        `'interior-point'` is the default\n",
      "        as it was the fastest and most robust method before the recent\n",
      "        addition of the HiGHS solvers.\n",
      "        `'revised simplex'` is more\n",
      "        accurate than interior-point for the problems it solves.\n",
      "        `'simplex'` is the legacy method and is\n",
      "        included for backwards compatibility and educational purposes.\n",
      "        \n",
      "        Method *highs-ds* is a wrapper of the C++ high performance dual\n",
      "        revised simplex implementation (HSOL) [13]_, [14]_. Method *highs-ipm*\n",
      "        is a wrapper of a C++ implementation of an **i**\\ nterior-\\ **p**\\ oint\n",
      "        **m**\\ ethod [13]_; it features a crossover routine, so it is as accurate\n",
      "        as a simplex solver. Method *highs* chooses between the two automatically.\n",
      "        For new code involving `linprog`, we recommend explicitly choosing one of\n",
      "        these three method values.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Method *interior-point* uses the primal-dual path following algorithm\n",
      "        as outlined in [4]_. This algorithm supports sparse constraint matrices and\n",
      "        is typically faster than the simplex methods, especially for large, sparse\n",
      "        problems. Note, however, that the solution returned may be slightly less\n",
      "        accurate than those of the simplex methods and will not, in general,\n",
      "        correspond with a vertex of the polytope defined by the constraints.\n",
      "        \n",
      "        .. versionadded:: 1.0.0\n",
      "        \n",
      "        Method *revised simplex* uses the revised simplex method as described in\n",
      "        [9]_, except that a factorization [11]_ of the basis matrix, rather than\n",
      "        its inverse, is efficiently maintained and used to solve the linear systems\n",
      "        at each iteration of the algorithm.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Method *simplex* uses a traditional, full-tableau implementation of\n",
      "        Dantzig's simplex algorithm [1]_, [2]_ (*not* the\n",
      "        Nelder-Mead simplex). This algorithm is included for backwards\n",
      "        compatibility and educational purposes.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Before applying *interior-point*, *revised simplex*, or *simplex*,\n",
      "        a presolve procedure based on [8]_ attempts\n",
      "        to identify trivial infeasibilities, trivial unboundedness, and potential\n",
      "        problem simplifications. Specifically, it checks for:\n",
      "        \n",
      "        - rows of zeros in ``A_eq`` or ``A_ub``, representing trivial constraints;\n",
      "        - columns of zeros in ``A_eq`` `and` ``A_ub``, representing unconstrained\n",
      "          variables;\n",
      "        - column singletons in ``A_eq``, representing fixed variables; and\n",
      "        - column singletons in ``A_ub``, representing simple bounds.\n",
      "        \n",
      "        If presolve reveals that the problem is unbounded (e.g. an unconstrained\n",
      "        and unbounded variable has negative cost) or infeasible (e.g., a row of\n",
      "        zeros in ``A_eq`` corresponds with a nonzero in ``b_eq``), the solver\n",
      "        terminates with the appropriate status code. Note that presolve terminates\n",
      "        as soon as any sign of unboundedness is detected; consequently, a problem\n",
      "        may be reported as unbounded when in reality the problem is infeasible\n",
      "        (but infeasibility has not been detected yet). Therefore, if it is\n",
      "        important to know whether the problem is actually infeasible, solve the\n",
      "        problem again with option ``presolve=False``.\n",
      "        \n",
      "        If neither infeasibility nor unboundedness are detected in a single pass\n",
      "        of the presolve, bounds are tightened where possible and fixed\n",
      "        variables are removed from the problem. Then, linearly dependent rows\n",
      "        of the ``A_eq`` matrix are removed, (unless they represent an\n",
      "        infeasibility) to avoid numerical difficulties in the primary solve\n",
      "        routine. Note that rows that are nearly linearly dependent (within a\n",
      "        prescribed tolerance) may also be removed, which can change the optimal\n",
      "        solution in rare cases. If this is a concern, eliminate redundancy from\n",
      "        your problem formulation and run with option ``rr=False`` or\n",
      "        ``presolve=False``.\n",
      "        \n",
      "        Several potential improvements can be made here: additional presolve\n",
      "        checks outlined in [8]_ should be implemented, the presolve routine should\n",
      "        be run multiple times (until no further simplifications can be made), and\n",
      "        more of the efficiency improvements from [5]_ should be implemented in the\n",
      "        redundancy removal routines.\n",
      "        \n",
      "        After presolve, the problem is transformed to standard form by converting\n",
      "        the (tightened) simple bounds to upper bound constraints, introducing\n",
      "        non-negative slack variables for inequality constraints, and expressing\n",
      "        unbounded variables as the difference between two non-negative variables.\n",
      "        Optionally, the problem is automatically scaled via equilibration [12]_.\n",
      "        The selected algorithm solves the standard form problem, and a\n",
      "        postprocessing routine converts the result to a solution to the original\n",
      "        problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "               Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "               1963\n",
      "        .. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "               Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      "        .. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "               Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "        .. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "               optimizer for linear programming: an implementation of the\n",
      "               homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "               2000. 197-232.\n",
      "        .. [5] Andersen, Erling D. \"Finding all linearly dependent rows in\n",
      "               large-scale linear programming.\" Optimization Methods and Software\n",
      "               6.3 (1995): 219-227.\n",
      "        .. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "               Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "               March 2004. Available 2/25/2017 at\n",
      "               https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      "        .. [7] Fourer, Robert. \"Solving Linear Programs by Interior-Point Methods.\"\n",
      "               Unpublished Course Notes, August 26, 2005. Available 2/25/2017 at\n",
      "               http://www.4er.org/CourseNotes/Book%20B/B-III.pdf\n",
      "        .. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "               programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      "        .. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "               programming.\" Athena Scientific 1 (1997): 997.\n",
      "        .. [10] Andersen, Erling D., et al. Implementation of interior point\n",
      "                methods for large scale linear programming. HEC/Universite de\n",
      "                Geneve, 1996.\n",
      "        .. [11] Bartels, Richard H. \"A stabilization of the simplex method.\"\n",
      "                Journal in  Numerische Mathematik 16.5 (1971): 414-434.\n",
      "        .. [12] Tomlin, J. A. \"On scaling linear programming problems.\"\n",
      "                Mathematical Programming Study 4 (1975): 146-166.\n",
      "        .. [13] Huangfu, Q., Galabova, I., Feldmeier, M., and Hall, J. A. J.\n",
      "                \"HiGHS - high performance software for linear optimization.\"\n",
      "                Accessed 4/16/2020 at https://www.maths.ed.ac.uk/hall/HiGHS/#guide\n",
      "        .. [14] Huangfu, Q. and Hall, J. A. J. \"Parallelizing the dual revised\n",
      "                simplex method.\" Mathematical Programming Computation, 10 (1),\n",
      "                119-142, 2018. DOI: 10.1007/s12532-017-0130-5\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the following problem:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_{x_0, x_1} \\ -x_0 + 4x_1 & \\\\\n",
      "            \\mbox{such that} \\ -3x_0 + x_1 & \\leq 6,\\\\\n",
      "            -x_0 - 2x_1 & \\geq -4,\\\\\n",
      "            x_1 & \\geq -3.\n",
      "        \n",
      "        The problem is not presented in the form accepted by `linprog`. This is\n",
      "        easily remedied by converting the \"greater than\" inequality\n",
      "        constraint to a \"less than\" inequality constraint by\n",
      "        multiplying both sides by a factor of :math:`-1`. Note also that the last\n",
      "        constraint is really the simple bound :math:`-3 \\leq x_1 \\leq \\infty`.\n",
      "        Finally, since there are no bounds on :math:`x_0`, we must explicitly\n",
      "        specify the bounds :math:`-\\infty \\leq x_0 \\leq \\infty`, as the\n",
      "        default is for variables to be non-negative. After collecting coeffecients\n",
      "        into arrays and tuples, the input for this problem is:\n",
      "        \n",
      "        >>> c = [-1, 4]\n",
      "        >>> A = [[-3, 1], [1, 2]]\n",
      "        >>> b = [6, 4]\n",
      "        >>> x0_bounds = (None, None)\n",
      "        >>> x1_bounds = (-3, None)\n",
      "        >>> from scipy.optimize import linprog\n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds])\n",
      "        \n",
      "        Note that the default method for `linprog` is 'interior-point', which is\n",
      "        approximate by nature.\n",
      "        \n",
      "        >>> print(res)\n",
      "             con: array([], dtype=float64)\n",
      "             fun: -21.99999984082494 # may vary\n",
      "         message: 'Optimization terminated successfully.'\n",
      "             nit: 6 # may vary\n",
      "           slack: array([3.89999997e+01, 8.46872439e-08] # may vary\n",
      "          status: 0\n",
      "         success: True\n",
      "               x: array([ 9.99999989, -2.99999999]) # may vary\n",
      "        \n",
      "        If you need greater accuracy, try 'revised simplex'.\n",
      "        \n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds], method='revised simplex')\n",
      "        >>> print(res)\n",
      "             con: array([], dtype=float64)\n",
      "             fun: -22.0 # may vary\n",
      "         message: 'Optimization terminated successfully.'\n",
      "             nit: 1 # may vary\n",
      "           slack: array([39.,  0.]) # may vary\n",
      "          status: 0\n",
      "         success: True\n",
      "               x: array([10., -3.]) # may vary\n",
      "    \n",
      "    linprog_verbose_callback(res)\n",
      "        A sample callback function demonstrating the linprog callback interface.\n",
      "        This callback produces detailed output to sys.stdout before each iteration\n",
      "        and after the final iteration of the simplex algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        res : A `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "        \n",
      "            x : 1-D array\n",
      "                The independent variable vector which optimizes the linear\n",
      "                programming problem.\n",
      "            fun : float\n",
      "                Value of the objective function.\n",
      "            success : bool\n",
      "                True if the algorithm succeeded in finding an optimal solution.\n",
      "            slack : 1-D array\n",
      "                The values of the slack variables. Each slack variable corresponds\n",
      "                to an inequality constraint. If the slack is zero, then the\n",
      "                corresponding constraint is active.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints, that is,\n",
      "                ``b - A_eq @ x``\n",
      "            phase : int\n",
      "                The phase of the optimization being executed. In phase 1 a basic\n",
      "                feasible solution is sought and the T has an additional row\n",
      "                representing an alternate objective function.\n",
      "            status : int\n",
      "                An integer representing the exit status of the optimization::\n",
      "        \n",
      "                     0 : Optimization terminated successfully\n",
      "                     1 : Iteration limit reached\n",
      "                     2 : Problem appears to be infeasible\n",
      "                     3 : Problem appears to be unbounded\n",
      "                     4 : Serious numerical difficulties encountered\n",
      "        \n",
      "            nit : int\n",
      "                The number of iterations performed.\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the optimization.\n",
      "    \n",
      "    lsq_linear(A, b, bounds=(-inf, inf), method='trf', tol=1e-10, lsq_solver=None, lsmr_tol=None, max_iter=None, verbose=0)\n",
      "        Solve a linear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given a m-by-n design matrix A and a target vector b with m elements,\n",
      "        `lsq_linear` solves the following optimization problem::\n",
      "        \n",
      "            minimize 0.5 * ||A x - b||**2\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        This optimization problem is convex, hence a found minimum (if iterations\n",
      "        have converged) is guaranteed to be global.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : array_like, sparse matrix of LinearOperator, shape (m, n)\n",
      "            Design matrix. Can be `scipy.sparse.linalg.LinearOperator`.\n",
      "        b : array_like, shape (m,)\n",
      "            Target vector.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must have shape (n,) or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : 'trf' or 'bvls', optional\n",
      "            Method to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm adapted for a linear\n",
      "                  least-squares problem. This is an interior-point-like method\n",
      "                  and the required number of iterations is weakly correlated with\n",
      "                  the number of variables.\n",
      "                * 'bvls' : Bounded-variable least-squares algorithm. This is\n",
      "                  an active set method, which requires the number of iterations\n",
      "                  comparable to the number of variables. Can't be used when `A` is\n",
      "                  sparse or LinearOperator.\n",
      "        \n",
      "            Default is 'trf'.\n",
      "        tol : float, optional\n",
      "            Tolerance parameter. The algorithm terminates if a relative change\n",
      "            of the cost function is less than `tol` on the last iteration.\n",
      "            Additionally, the first-order optimality measure is considered:\n",
      "        \n",
      "                * ``method='trf'`` terminates if the uniform norm of the gradient,\n",
      "                  scaled to account for the presence of the bounds, is less than\n",
      "                  `tol`.\n",
      "                * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n",
      "                  are satisfied within `tol` tolerance.\n",
      "        \n",
      "        lsq_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method of solving unbounded least-squares problems throughout\n",
      "            iterations:\n",
      "        \n",
      "                * 'exact' : Use dense QR or SVD decomposition approach. Can't be\n",
      "                  used when `A` is sparse or LinearOperator.\n",
      "                * 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n",
      "                  which requires only matrix-vector product evaluations. Can't\n",
      "                  be used with ``method='bvls'``.\n",
      "        \n",
      "            If None (default), the solver is chosen based on type of `A`.\n",
      "        lsmr_tol : None, float or 'auto', optional\n",
      "            Tolerance parameters 'atol' and 'btol' for `scipy.sparse.linalg.lsmr`\n",
      "            If None (default), it is set to ``1e-2 * tol``. If 'auto', the\n",
      "            tolerance will be adjusted based on the optimality of the current\n",
      "            iterate, which can speed up the optimization process, but is not always\n",
      "            reliable.\n",
      "        max_iter : None or int, optional\n",
      "            Maximum number of iterations before termination. If None (default), it\n",
      "            is set to 100 for ``method='trf'`` or to the number of variables for\n",
      "            ``method='bvls'`` (not counting iterations for 'bvls' initialization).\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 : work silently (default).\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        OptimizeResult with the following fields defined:\n",
      "        x : ndarray, shape (n,)\n",
      "            Solution found.\n",
      "        cost : float\n",
      "            Value of the cost function at the solution.\n",
      "        fun : ndarray, shape (m,)\n",
      "            Vector of residuals at the solution.\n",
      "        optimality : float\n",
      "            First-order optimality measure. The exact meaning depends on `method`,\n",
      "            refer to the description of `tol` parameter.\n",
      "        active_mask : ndarray of int, shape (n,)\n",
      "            Each component shows whether a corresponding constraint is active\n",
      "            (that is, whether a variable is at the bound):\n",
      "        \n",
      "                *  0 : a constraint is not active.\n",
      "                * -1 : a lower bound is active.\n",
      "                *  1 : an upper bound is active.\n",
      "        \n",
      "            Might be somewhat arbitrary for the `trf` method as it generates a\n",
      "            sequence of strictly feasible iterates and active_mask is determined\n",
      "            within a tolerance threshold.\n",
      "        nit : int\n",
      "            Number of iterations. Zero if the unconstrained solution is optimal.\n",
      "        status : int\n",
      "            Reason for algorithm termination:\n",
      "        \n",
      "                * -1 : the algorithm was not able to make progress on the last\n",
      "                  iteration.\n",
      "                *  0 : the maximum number of iterations is exceeded.\n",
      "                *  1 : the first-order optimality measure is less than `tol`.\n",
      "                *  2 : the relative change of the cost function is less than `tol`.\n",
      "                *  3 : the unconstrained solution is optimal.\n",
      "        \n",
      "        message : str\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nnls : Linear least squares with non-negativity constraint.\n",
      "        least_squares : Nonlinear least squares with bounds on the variables.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm first computes the unconstrained least-squares solution by\n",
      "        `numpy.linalg.lstsq` or `scipy.sparse.linalg.lsmr` depending on\n",
      "        `lsq_solver`. This solution is returned as optimal if it lies within the\n",
      "        bounds.\n",
      "        \n",
      "        Method 'trf' runs the adaptation of the algorithm described in [STIR]_ for\n",
      "        a linear least-squares problem. The iterations are essentially the same as\n",
      "        in the nonlinear least-squares algorithm, but as the quadratic function\n",
      "        model is always accurate, we don't need to track or modify the radius of\n",
      "        a trust region. The line search (backtracking) is used as a safety net\n",
      "        when a selected step does not decrease the cost function. Read more\n",
      "        detailed description of the algorithm in `scipy.optimize.least_squares`.\n",
      "        \n",
      "        Method 'bvls' runs a Python implementation of the algorithm described in\n",
      "        [BVLS]_. The algorithm maintains active and free sets of variables, on\n",
      "        each iteration chooses a new variable to move from the active set to the\n",
      "        free set and then solves the unconstrained least-squares problem on free\n",
      "        variables. This algorithm is guaranteed to give an accurate solution\n",
      "        eventually, but may require up to n iterations for a problem with n\n",
      "        variables. Additionally, an ad-hoc initialization procedure is\n",
      "        implemented, that determines which variables to set free or active\n",
      "        initially. It takes some number of iterations before actual BVLS starts,\n",
      "        but can significantly reduce the number of further iterations.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [BVLS] P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:\n",
      "                  an Algorithm and Applications\", Computational Statistics, 10,\n",
      "                  129-141, 1995.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example, a problem with a large sparse matrix and bounds on the\n",
      "        variables is solved.\n",
      "        \n",
      "        >>> from scipy.sparse import rand\n",
      "        >>> from scipy.optimize import lsq_linear\n",
      "        ...\n",
      "        >>> np.random.seed(0)\n",
      "        ...\n",
      "        >>> m = 20000\n",
      "        >>> n = 10000\n",
      "        ...\n",
      "        >>> A = rand(m, n, density=1e-4)\n",
      "        >>> b = np.random.randn(m)\n",
      "        ...\n",
      "        >>> lb = np.random.randn(n)\n",
      "        >>> ub = lb + 1\n",
      "        ...\n",
      "        >>> res = lsq_linear(A, b, bounds=(lb, ub), lsmr_tol='auto', verbose=1)\n",
      "        # may vary\n",
      "        The relative change of the cost function is less than `tol`.\n",
      "        Number of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\n",
      "        first-order optimality 4.66e-08.\n",
      "    \n",
      "    minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "        Minimization of scalar function of one or more variables.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            The objective function to be minimized.\n",
      "        \n",
      "                ``fun(x, *args) -> float``\n",
      "        \n",
      "            where ``x`` is an 1-D array with shape (n,) and ``args``\n",
      "            is a tuple of the fixed parameters needed to completely\n",
      "            specify the function.\n",
      "        x0 : ndarray, shape (n,)\n",
      "            Initial guess. Array of real elements of size (n,),\n",
      "            where 'n' is the number of independent variables.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its\n",
      "            derivatives (`fun`, `jac` and `hess` functions).\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "                - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "                - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "                - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "                - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "                - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "                - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "                - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "                - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "                - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "                - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "                - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "                - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "                - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "                - custom - a callable object (added in version 0.14.0),\n",
      "                  see below for description.\n",
      "        \n",
      "            If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "            depending if the problem has constraints or bounds.\n",
      "        jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "            Method for computing the gradient vector. Only for CG, BFGS,\n",
      "            Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "            trust-exact and trust-constr.\n",
      "            If it is a callable, it should be a function that returns the gradient\n",
      "            vector:\n",
      "        \n",
      "                ``jac(x, *args) -> array_like, shape (n,)``\n",
      "        \n",
      "            where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
      "            the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
      "            assumed to return and objective and gradient as an ``(f, g)`` tuple.\n",
      "            Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
      "            'trust-krylov' require that either a callable be supplied, or that\n",
      "            `fun` return the objective and gradient.\n",
      "            If None or False, the gradient will be estimated using 2-point finite\n",
      "            difference estimation with an absolute step size.\n",
      "            Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
      "            to select a finite difference scheme for numerical estimation of the\n",
      "            gradient with a relative step size. These finite difference schemes\n",
      "            obey any specified `bounds`.\n",
      "        hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
      "            Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "            trust-ncg,  trust-krylov, trust-exact and trust-constr. If it is\n",
      "            callable, it should return the  Hessian matrix:\n",
      "        \n",
      "                ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "        \n",
      "            where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
      "            parameters. LinearOperator and sparse matrix returns are\n",
      "            allowed only for 'trust-constr' method. Alternatively, the keywords\n",
      "            {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
      "            for numerical estimation. Or, objects implementing\n",
      "            `HessianUpdateStrategy` interface can be used to approximate\n",
      "            the Hessian. Available quasi-Newton methods implementing\n",
      "            this interface are:\n",
      "        \n",
      "                - `BFGS`;\n",
      "                - `SR1`.\n",
      "        \n",
      "            Whenever the gradient is estimated via finite-differences,\n",
      "            the Hessian cannot be estimated with options\n",
      "            {'2-point', '3-point', 'cs'} and needs to be\n",
      "            estimated using one of the quasi-Newton strategies.\n",
      "            Finite-difference options {'2-point', '3-point', 'cs'} and\n",
      "            `HessianUpdateStrategy` are available only for 'trust-constr' method.\n",
      "        hessp : callable, optional\n",
      "            Hessian of objective function times an arbitrary vector p. Only for\n",
      "            Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "            Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "            provided, then `hessp` will be ignored.  `hessp` must compute the\n",
      "            Hessian times an arbitrary vector:\n",
      "        \n",
      "                ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "        \n",
      "            where x is a (n,) ndarray, p is an arbitrary vector with\n",
      "            dimension (n,) and `args` is a tuple with the fixed\n",
      "            parameters.\n",
      "        bounds : sequence or `Bounds`, optional\n",
      "            Bounds on variables for L-BFGS-B, TNC, SLSQP, Powell, and\n",
      "            trust-constr methods. There are two ways to specify the bounds:\n",
      "        \n",
      "                1. Instance of `Bounds` class.\n",
      "                2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "                   is used to specify no bound.\n",
      "        \n",
      "        constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "            Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
      "        \n",
      "            Constraints for 'trust-constr' are defined as a single object or a\n",
      "            list of objects specifying constraints to the optimization problem.\n",
      "            Available constraints are:\n",
      "        \n",
      "                - `LinearConstraint`\n",
      "                - `NonlinearConstraint`\n",
      "        \n",
      "            Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "            Each dictionary with fields:\n",
      "        \n",
      "                type : str\n",
      "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "                fun : callable\n",
      "                    The function defining the constraint.\n",
      "                jac : callable, optional\n",
      "                    The Jacobian of `fun` (only for SLSQP).\n",
      "                args : sequence, optional\n",
      "                    Extra arguments to be passed to the function and Jacobian.\n",
      "        \n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "            Note that COBYLA only supports inequality constraints.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            generic options:\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform. Depending on the\n",
      "                    method each iteration may use several function evaluations.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            For method-specific options, see :func:`show_options()`.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration. For 'trust-constr' it is a callable with\n",
      "            the signature:\n",
      "        \n",
      "                ``callback(xk, OptimizeResult state) -> bool``\n",
      "        \n",
      "            where ``xk`` is the current parameter vector. and ``state``\n",
      "            is an `OptimizeResult` object, with the same fields\n",
      "            as the ones from the return. If callback returns True\n",
      "            the algorithm execution is terminated.\n",
      "            For all the other methods, the signature is:\n",
      "        \n",
      "                ``callback(xk)``\n",
      "        \n",
      "            where ``xk`` is the current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar : Interface to minimization algorithms for scalar\n",
      "            univariate functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *BFGS*.\n",
      "        \n",
      "        **Unconstrained minimization**\n",
      "        \n",
      "        Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "        Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "        applications. However, if numerical computation of derivative can be\n",
      "        trusted, other algorithms using the first and/or second derivatives\n",
      "        information might be preferred for their better performance in\n",
      "        general.\n",
      "        \n",
      "        Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "        gradient algorithm by Polak and Ribiere, a variant of the\n",
      "        Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
      "        first derivatives are used.\n",
      "        \n",
      "        Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "        method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "        pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "        performance even for non-smooth optimizations. This method also\n",
      "        returns an approximation of the Hessian inverse, stored as\n",
      "        `hess_inv` in the OptimizeResult object.\n",
      "        \n",
      "        Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "        Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "        Newton method). It uses a CG method to the compute the search\n",
      "        direction. See also *TNC* method for a box-constrained\n",
      "        minimization with a similar algorithm. Suitable for large-scale\n",
      "        problems.\n",
      "        \n",
      "        Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "        trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "        algorithm requires the gradient and Hessian; furthermore the\n",
      "        Hessian is required to be positive definite.\n",
      "        \n",
      "        Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "        Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "        unconstrained minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "        \n",
      "        Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "        the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "        minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "        On indefinite problems it requires usually less iterations than the\n",
      "        `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "        \n",
      "        Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "        is a trust-region method for unconstrained minimization in which\n",
      "        quadratic subproblems are solved almost exactly [13]_. This\n",
      "        algorithm requires the gradient and the Hessian (which is\n",
      "        *not* required to be positive definite). It is, in many\n",
      "        situations, the Newton method to converge in fewer iteraction\n",
      "        and the most recommended for small and medium-size problems.\n",
      "        \n",
      "        **Bound-Constrained minimization**\n",
      "        \n",
      "        Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "        algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "        \n",
      "        Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "        of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "        method. It performs sequential one-dimensional minimizations along\n",
      "        each vector of the directions set (`direc` field in `options` and\n",
      "        `info`), which is updated at each iteration of the main\n",
      "        minimization loop. The function need not be differentiable, and no\n",
      "        derivatives are taken. If bounds are not provided, then an\n",
      "        unbounded line search will be used. If bounds are provided and\n",
      "        the initial guess is within the bounds, then every function\n",
      "        evaluation throughout the minimization procedure will be within\n",
      "        the bounds. If bounds are provided, the initial guess is outside\n",
      "        the bounds, and `direc` is full rank (default has full rank), then\n",
      "        some function evaluations during the first iteration may be\n",
      "        outside the bounds, but every function evaluation after the first\n",
      "        iteration will be within the bounds. If `direc` is not full rank,\n",
      "        then some parameters may not be optimized and the solution is not\n",
      "        guaranteed to be within the bounds.\n",
      "        \n",
      "        Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "        algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "        to bounds. This algorithm uses gradient information; it is also\n",
      "        called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "        method described above as it wraps a C implementation and allows\n",
      "        each variable to be given upper and lower bounds.\n",
      "        \n",
      "        **Constrained Minimization**\n",
      "        \n",
      "        Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "        Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "        [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "        approximations to the objective function and each constraint. The\n",
      "        method wraps a FORTRAN implementation of the algorithm. The\n",
      "        constraints functions 'fun' may return either a single number\n",
      "        or an array or list of numbers.\n",
      "        \n",
      "        Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "        Least SQuares Programming to minimize a function of several\n",
      "        variables with any combination of bounds, equality and inequality\n",
      "        constraints. The method wraps the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft [12]_. Note that the\n",
      "        wrapper handles infinite values in bounds by converting them into\n",
      "        large floating values.\n",
      "        \n",
      "        Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "        trust-region algorithm for constrained optimization. It swiches\n",
      "        between two implementations depending on the problem definition.\n",
      "        It is the most versatile constrained minimization algorithm\n",
      "        implemented in SciPy and the most appropriate for large-scale problems.\n",
      "        For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "        Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "        inequality constraints  are imposed as well, it swiches to the trust-region\n",
      "        interior point  method described in [16]_. This interior point algorithm,\n",
      "        in turn, solves inequality constraints by introducing slack variables\n",
      "        and solving a sequence of equality-constrained barrier problems\n",
      "        for progressively smaller values of the barrier parameter.\n",
      "        The previously described equality constrained SQP method is\n",
      "        used to solve the subproblems with increasing levels of accuracy\n",
      "        as the iterate gets closer to a solution.\n",
      "        \n",
      "        **Finite-Difference Options**\n",
      "        \n",
      "        For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "        the gradient and the Hessian may be approximated using\n",
      "        three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "        The scheme 'cs' is, potentially, the most accurate but it\n",
      "        requires the function to correctly handles complex inputs and to\n",
      "        be differentiable in the complex plane. The scheme '3-point' is more\n",
      "        accurate than '2-point' but requires twice as many operations.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "        or a different library.  You can simply pass a callable as the ``method``\n",
      "        parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "        `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "        `fun` returns just the function values and `jac` is converted to a function\n",
      "        returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "        object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method.  You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "            Minimization. The Computer Journal 7: 308-13.\n",
      "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "            191-208.\n",
      "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "           a function of several variables without calculating derivatives. The\n",
      "           Computer Journal 7: 155-162.\n",
      "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "           Numerical Recipes (any edition), Cambridge University Press.\n",
      "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "           Springer New York.\n",
      "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "           550-560.\n",
      "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "        .. [9] Powell, M J D. A direct search optimization method that models\n",
      "           the objective and constraint functions by linear interpolation.\n",
      "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "           calculations. 1998. Acta Numerica 7: 287-336.\n",
      "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "           2007/NA03\n",
      "        .. [12] Kraft, D. A software package for sequential quadratic\n",
      "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "        .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "           Trust region methods. 2000. Siam. pp. 169-200.\n",
      "        .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "           implementation of the GLTR method for iterative solution of\n",
      "           the trust region problem\", :arxiv:`1611.04718`\n",
      "        .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "           Trust-Region Subproblem using the Lanczos Method\",\n",
      "           SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "        .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "            An interior point algorithm for large-scale nonlinear  programming.\n",
      "            SIAM Journal on Optimization 9.4: 877-900.\n",
      "        .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "            implementation of an algorithm for large-scale equality constrained\n",
      "            optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function (and its respective derivatives) is implemented in `rosen`\n",
      "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "        \n",
      "        A simple application of the *Nelder-Mead* method is:\n",
      "        \n",
      "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "        >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        \n",
      "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "        options:\n",
      "        \n",
      "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "        ...                options={'gtol': 1e-6, 'disp': True})\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 26\n",
      "                 Function evaluations: 31\n",
      "                 Gradient evaluations: 31\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        >>> print(res.message)\n",
      "        Optimization terminated successfully.\n",
      "        >>> res.hess_inv\n",
      "        array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "               [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "               [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "               [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "               [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "        \n",
      "        \n",
      "        Next, consider a minimization problem with several constraints (namely\n",
      "        Example 16.4 from [5]_). The objective function is:\n",
      "        \n",
      "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "        \n",
      "        There are three constraints defined as:\n",
      "        \n",
      "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "        \n",
      "        And variables must be positive, hence the following bounds:\n",
      "        \n",
      "        >>> bnds = ((0, None), (0, None))\n",
      "        \n",
      "        The optimization problem is solved using the SLSQP method as:\n",
      "        \n",
      "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "        ...                constraints=cons)\n",
      "        \n",
      "        It should converge to the theoretical solution (1.4 ,1.7).\n",
      "    \n",
      "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)\n",
      "        Minimization of scalar function of one variable.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Objective function.\n",
      "            Scalar function, must return a scalar.\n",
      "        bracket : sequence, optional\n",
      "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
      "            interval and can either have three items ``(a, b, c)`` so that\n",
      "            ``a < b < c`` and ``fun(b) < fun(a), fun(c)`` or two items ``a`` and\n",
      "            ``c`` which are assumed to be a starting interval for a downhill\n",
      "            bracket search (see `bracket`); it doesn't always mean that the\n",
      "            obtained solution will satisfy ``a <= x <= c``.\n",
      "        bounds : sequence, optional\n",
      "            For method 'bounded', `bounds` is mandatory and must have two items\n",
      "            corresponding to the optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function.\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of:\n",
      "        \n",
      "                - 'Brent'     :ref:`(see here) <optimize.minimize_scalar-brent>`\n",
      "                - 'Bounded'   :ref:`(see here) <optimize.minimize_scalar-bounded>`\n",
      "                - 'Golden'    :ref:`(see here) <optimize.minimize_scalar-golden>`\n",
      "                - custom - a callable object (added in version 0.14.0), see below\n",
      "        \n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options.\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            See :func:`show_options()` for solver-specific options.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize : Interface to minimization algorithms for scalar multivariate\n",
      "            functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *Brent*.\n",
      "        \n",
      "        Method :ref:`Brent <optimize.minimize_scalar-brent>` uses Brent's\n",
      "        algorithm to find a local minimum.  The algorithm uses inverse\n",
      "        parabolic interpolation when possible to speed up convergence of\n",
      "        the golden section method.\n",
      "        \n",
      "        Method :ref:`Golden <optimize.minimize_scalar-golden>` uses the\n",
      "        golden section search technique. It uses analog of the bisection\n",
      "        method to decrease the bracketed interval. It is usually\n",
      "        preferable to use the *Brent* method.\n",
      "        \n",
      "        Method :ref:`Bounded <optimize.minimize_scalar-bounded>` can\n",
      "        perform bounded minimization. It uses the Brent method to find a\n",
      "        local minimum in the interval x1 < xopt < x2.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using some library frontend to minimize_scalar. You can simply\n",
      "        pass a callable as the ``method`` parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `bracket`, `tol`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  The method\n",
      "        shall return an `OptimizeResult` object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method. You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the problem of minimizing the following function.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x - 2) * x * (x + 2)**2\n",
      "        \n",
      "        Using the *Brent* method, we find the local minimum as:\n",
      "        \n",
      "        >>> from scipy.optimize import minimize_scalar\n",
      "        >>> res = minimize_scalar(f)\n",
      "        >>> res.x\n",
      "        1.28077640403\n",
      "        \n",
      "        Using the *Bounded* method, we find a local minimum with specified\n",
      "        bounds as:\n",
      "        \n",
      "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
      "        >>> res.x\n",
      "        -2.0000002026\n",
      "    \n",
      "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None, x1=None, rtol=0.0, full_output=False, disp=True)\n",
      "        Find a zero of a real or complex function using the Newton-Raphson\n",
      "        (or secant or Halley's) method.\n",
      "        \n",
      "        Find a zero of the function `func` given a nearby starting point `x0`.\n",
      "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
      "        is provided, otherwise the secant method is used. If the second order\n",
      "        derivative `fprime2` of `func` is also provided, then Halley's method is\n",
      "        used.\n",
      "        \n",
      "        If `x0` is a sequence with more than one item, then `newton` returns an\n",
      "        array, and `func` must be vectorized and return a sequence or array of the\n",
      "        same shape as its first argument. If `fprime` or `fprime2` is given, then\n",
      "        its return must also have the same shape.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The function whose zero is wanted. It must be a function of a\n",
      "            single variable of the form ``f(x,a,b,c...)``, where ``a,b,c...``\n",
      "            are extra arguments that can be passed in the `args` parameter.\n",
      "        x0 : float, sequence, or ndarray\n",
      "            An initial estimate of the zero that should be somewhere near the\n",
      "            actual zero. If not scalar, then `func` must be vectorized and return\n",
      "            a sequence or array of the same shape as its first argument.\n",
      "        fprime : callable, optional\n",
      "            The derivative of the function when available and convenient. If it\n",
      "            is None (default), then the secant method is used.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to be used in the function call.\n",
      "        tol : float, optional\n",
      "            The allowable error of the zero value. If `func` is complex-valued,\n",
      "            a larger `tol` is recommended as both the real and imaginary parts\n",
      "            of `x` contribute to ``|x - x0|``.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        fprime2 : callable, optional\n",
      "            The second order derivative of the function when available and\n",
      "            convenient. If it is None (default), then the normal Newton-Raphson\n",
      "            or the secant method is used. If it is not None, then Halley's method\n",
      "            is used.\n",
      "        x1 : float, optional\n",
      "            Another estimate of the zero that should be somewhere near the\n",
      "            actual zero. Used if `fprime` is not provided.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False (default), the root is returned.\n",
      "            If True and `x0` is scalar, the return value is ``(x, r)``, where ``x``\n",
      "            is the root and ``r`` is a `RootResults` object.\n",
      "            If True and `x0` is non-scalar, the return value is ``(x, converged,\n",
      "            zero_der)`` (see Returns section for details).\n",
      "        disp : bool, optional\n",
      "            If True, raise a RuntimeError if the algorithm didn't converge, with\n",
      "            the error message containing the number of iterations and current\n",
      "            function value. Otherwise, the convergence status is recorded in a\n",
      "            `RootResults` return object.\n",
      "            Ignored if `x0` is not scalar.\n",
      "            *Note: this has little to do with displaying, however,\n",
      "            the `disp` keyword cannot be renamed for backwards compatibility.*\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        root : float, sequence, or ndarray\n",
      "            Estimated location where function is zero.\n",
      "        r : `RootResults`, optional\n",
      "            Present if ``full_output=True`` and `x0` is scalar.\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        converged : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements converged successfully.\n",
      "        zero_der : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements had a zero derivative.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect\n",
      "        fsolve : find zeros in N dimensions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The convergence rate of the Newton-Raphson method is quadratic,\n",
      "        the Halley method is cubic, and the secant method is\n",
      "        sub-quadratic. This means that if the function is well-behaved\n",
      "        the actual error in the estimated zero after the nth iteration\n",
      "        is approximately the square (cube for Halley) of the error\n",
      "        after the (n-1)th step. However, the stopping criterion used\n",
      "        here is the step size and there is no guarantee that a zero\n",
      "        has been found. Consequently, the result should be verified.\n",
      "        Safer algorithms are brentq, brenth, ridder, and bisect,\n",
      "        but they all require that the root first be bracketed in an\n",
      "        interval where the function changes sign. The brentq algorithm\n",
      "        is recommended for general use in one dimensional problems\n",
      "        when such an interval has been found.\n",
      "        \n",
      "        When `newton` is used with arrays, it is best suited for the following\n",
      "        types of problems:\n",
      "        \n",
      "        * The initial guesses, `x0`, are all relatively the same distance from\n",
      "          the roots.\n",
      "        * Some or all of the extra arguments, `args`, are also arrays so that a\n",
      "          class of similar problems can be solved together.\n",
      "        * The size of the initial guesses, `x0`, is larger than O(100) elements.\n",
      "          Otherwise, a naive loop may perform as well or better than a vector.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        ``fprime`` is not provided, use the secant method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "        >>> root = optimize.newton(f, 1.5, fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "        \n",
      "        Only ``fprime`` is provided, use the Newton-Raphson method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        Both ``fprime2`` and ``fprime`` are provided, use Halley's method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2,\n",
      "        ...                        fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        When we want to find zeros for a set of related starting values and/or\n",
      "        function parameters, we can provide both of those as an array of inputs:\n",
      "        \n",
      "        >>> f = lambda x, a: x**3 - a\n",
      "        >>> fder = lambda x, a: 3 * x**2\n",
      "        >>> np.random.seed(4321)\n",
      "        >>> x = np.random.randn(100)\n",
      "        >>> a = np.arange(-50, 50)\n",
      "        >>> vec_res = optimize.newton(f, x, fprime=fder, args=(a, ))\n",
      "        \n",
      "        The above is the equivalent of solving for each value in ``(x, a)``\n",
      "        separately in a for-loop, just faster:\n",
      "        \n",
      "        >>> loop_res = [optimize.newton(f, x0, fprime=fder, args=(a0,))\n",
      "        ...             for x0, a0 in zip(x, a)]\n",
      "        >>> np.allclose(vec_res, loop_res)\n",
      "        True\n",
      "        \n",
      "        Plot the results found for all values of ``a``:\n",
      "        \n",
      "        >>> analytical_result = np.sign(a) * np.abs(a)**(1/3)\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(a, analytical_result, 'o')\n",
      "        >>> ax.plot(a, vec_res, '.')\n",
      "        >>> ax.set_xlabel('$a$')\n",
      "        >>> ax.set_ylabel('$x$ where $f(x, a)=0$')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
      "        \n",
      "        This method is suitable for solving large-scale problems.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        rdiff : float, optional\n",
      "            Relative step size to use in numerical differentiation.\n",
      "        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
      "            Krylov method to use to approximate the Jacobian.\n",
      "            Can be a string, or a function implementing the same interface as\n",
      "            the iterative solvers in `scipy.sparse.linalg`.\n",
      "        \n",
      "            The default is `scipy.sparse.linalg.lgmres`.\n",
      "        inner_maxiter : int, optional\n",
      "            Parameter to pass to the \"inner\" Krylov solver: maximum number of\n",
      "            iterations. Iteration will stop after maxiter steps even if the\n",
      "            specified tolerance has not been achieved.\n",
      "        inner_M : LinearOperator or InverseJacobian\n",
      "            Preconditioner for the inner Krylov iteration.\n",
      "            Note that you can use also inverse Jacobians as (adaptive)\n",
      "            preconditioners. For example,\n",
      "        \n",
      "            >>> from scipy.optimize.nonlin import BroydenFirst, KrylovJacobian\n",
      "            >>> from scipy.optimize.nonlin import InverseJacobian\n",
      "            >>> jac = BroydenFirst()\n",
      "            >>> kjac = KrylovJacobian(inner_M=InverseJacobian(jac))\n",
      "        \n",
      "            If the preconditioner has a method named 'update', it will be called\n",
      "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
      "            the current point, and ``f`` the current function value.\n",
      "        outer_k : int, optional\n",
      "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
      "            See `scipy.sparse.linalg.lgmres` for details.\n",
      "        inner_kwargs : kwargs\n",
      "            Keyword parameters for the \"inner\" Krylov solver\n",
      "            (defined with `method`). Parameter names must start with\n",
      "            the `inner_` prefix which will be stripped before passing on\n",
      "            the inner method. See, e.g., `scipy.sparse.linalg.gmres` for details.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='krylov'`` in particular.\n",
      "        scipy.sparse.linalg.gmres\n",
      "        scipy.sparse.linalg.lgmres\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements a Newton-Krylov solver. The basic idea is\n",
      "        to compute the inverse of the Jacobian with an iterative Krylov\n",
      "        method. These methods require only evaluating the Jacobian-vector\n",
      "        products, which are conveniently approximated by a finite difference:\n",
      "        \n",
      "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
      "        \n",
      "        Due to the use of iterative matrix inverses, these methods can\n",
      "        deal with large nonlinear problems.\n",
      "        \n",
      "        SciPy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
      "        solvers to choose from. The default here is `lgmres`, which is a\n",
      "        variant of restarted GMRES iteration that reuses some of the\n",
      "        information obtained in the previous Newton steps to invert\n",
      "        Jacobians in subsequent steps.\n",
      "        \n",
      "        For a review on Newton-Krylov methods, see for example [1]_,\n",
      "        and for the LGMRES sparse inverse method, see [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2004).\n",
      "               :doi:`10.1016/j.jcp.2003.08.010`\n",
      "        .. [2] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
      "               SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
      "               :doi:`10.1137/S0895479803422014`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0] + 0.5 * x[1] - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0]) ** 2]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.newton_krylov(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.66731771, 0.66536458])\n",
      "    \n",
      "    nnls(A, b, maxiter=None)\n",
      "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``. This is a wrapper\n",
      "        for a FORTRAN non-negative least squares solver.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : ndarray\n",
      "            Matrix ``A`` as shown above.\n",
      "        b : ndarray\n",
      "            Right-hand side vector.\n",
      "        maxiter: int, optional\n",
      "            Maximum number of iterations, optional.\n",
      "            Default is ``3 * A.shape[1]``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Solution vector.\n",
      "        rnorm : float\n",
      "            The residual, ``|| Ax-b ||_2``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lsq_linear : Linear least squares with bounds on the variables\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The FORTRAN code was published in the book below. The algorithm\n",
      "        is an active set method. It solves the KKT (Karush-Kuhn-Tucker)\n",
      "        conditions for the non-negative least squares problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Lawson C., Hanson R.J., (1987) Solving Least Squares Problems, SIAM\n",
      "        \n",
      "         Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import nnls\n",
      "        ...\n",
      "        >>> A = np.array([[1, 0], [1, 0], [0, 1]])\n",
      "        >>> b = np.array([2, 1, 1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([1.5, 1. ]), 0.7071067811865475)\n",
      "        \n",
      "        >>> b = np.array([-1, -1, -1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([0., 0.]), 1.7320508075688772)\n",
      "    \n",
      "    quadratic_assignment(A, B, method='faq', options=None)\n",
      "        Approximates solution to the quadratic assignment problem and\n",
      "        the graph matching problem.\n",
      "        \n",
      "        Quadratic assignment solves problems of the following form:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_P & \\ {\\ \\text{trace}(A^T P B P^T)}\\\\\n",
      "            \\mbox{s.t. } & {P \\ \\epsilon \\ \\mathcal{P}}\\\\\n",
      "        \n",
      "        where :math:`\\mathcal{P}` is the set of all permutation matrices,\n",
      "        and :math:`A` and :math:`B` are square matrices.\n",
      "        \n",
      "        Graph matching tries to *maximize* the same objective function.\n",
      "        This algorithm can be thought of as finding the alignment of the\n",
      "        nodes of two graphs that minimizes the number of induced edge\n",
      "        disagreements, or, in the case of weighted graphs, the sum of squared\n",
      "        edge weight differences.\n",
      "        \n",
      "        Note that the quadratic assignment problem is NP-hard. The results given\n",
      "        here are approximations and are not guaranteed to be optimal.\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : 2-D array, square\n",
      "            The square matrix :math:`A` in the objective function above.\n",
      "        \n",
      "        B : 2-D array, square\n",
      "            The square matrix :math:`B` in the objective function above.\n",
      "        \n",
      "        method :  str in {'faq', '2opt'} (default: 'faq')\n",
      "            The algorithm used to solve the problem.\n",
      "            :ref:`'faq' <optimize.qap-faq>` (default) and\n",
      "            :ref:`'2opt' <optimize.qap-2opt>` are available.\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All solvers support the following:\n",
      "        \n",
      "            maximize : bool (default: False)\n",
      "                Maximizes the objective function if ``True``.\n",
      "        \n",
      "            partial_match : 2-D array of integers, optional (default: None)\n",
      "                Fixes part of the matching. Also known as a \"seed\" [2]_.\n",
      "        \n",
      "                Each row of `partial_match` specifies a pair of matched nodes:\n",
      "                node ``partial_match[i, 0]`` of `A` is matched to node\n",
      "                ``partial_match[i, 1]`` of `B`. The array has shape ``(m, 2)``,\n",
      "                where ``m`` is not greater than the number of nodes, :math:`n`.\n",
      "        \n",
      "            rng : int, `RandomState`, `Generator` or None, optional (default: None)\n",
      "                Accepts an integer as a seed for the random generator or a\n",
      "                ``RandomState`` or ``Generator`` object. If None (default), uses\n",
      "                global `numpy.random` random state.\n",
      "        \n",
      "            For method-specific options, see\n",
      "            :func:`show_options('quadratic_assignment') <show_options>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            `OptimizeResult` containing the following fields.\n",
      "        \n",
      "            col_ind : 1-D array\n",
      "                Column indices corresponding to the best permutation found of the\n",
      "                nodes of `B`.\n",
      "            fun : float\n",
      "                The objective value of the solution.\n",
      "            nit : int\n",
      "                The number of iterations performed during optimization.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The default method :ref:`'faq' <optimize.qap-faq>` uses the Fast\n",
      "        Approximate QAP algorithm [1]_; it typically offers the best combination of\n",
      "        speed and accuracy.\n",
      "        Method :ref:`'2opt' <optimize.qap-2opt>` can be computationally expensive,\n",
      "        but may be a useful alternative, or it can be used to refine the solution\n",
      "        returned by another method.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J.T. Vogelstein, J.M. Conroy, V. Lyzinski, L.J. Podrazik,\n",
      "               S.G. Kratzer, E.T. Harley, D.E. Fishkind, R.J. Vogelstein, and\n",
      "               C.E. Priebe, \"Fast approximate quadratic programming for graph\n",
      "               matching,\" PLOS one, vol. 10, no. 4, p. e0121002, 2015,\n",
      "               :doi:`10.1371/journal.pone.0121002`\n",
      "        \n",
      "        .. [2] D. Fishkind, S. Adali, H. Patsolic, L. Meng, D. Singh, V. Lyzinski,\n",
      "               C. Priebe, \"Seeded graph matching\", Pattern Recognit. 87 (2019):\n",
      "               203-215, :doi:`10.1016/j.patcog.2018.09.014`\n",
      "        \n",
      "        .. [3] \"2-opt,\" Wikipedia.\n",
      "               https://en.wikipedia.org/wiki/2-opt\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.optimize import quadratic_assignment\n",
      "        >>> A = np.array([[0, 80, 150, 170], [80, 0, 130, 100],\n",
      "        ...               [150, 130, 0, 120], [170, 100, 120, 0]])\n",
      "        >>> B = np.array([[0, 5, 2, 7], [0, 0, 3, 8],\n",
      "        ...               [0, 0, 0, 3], [0, 0, 0, 0]])\n",
      "        >>> res = quadratic_assignment(A, B)\n",
      "        >>> print(res)\n",
      "         col_ind: array([0, 3, 2, 1])\n",
      "             fun: 3260\n",
      "             nit: 9\n",
      "        \n",
      "        The see the relationship between the returned ``col_ind`` and ``fun``,\n",
      "        use ``col_ind`` to form the best permutation matrix found, then evaluate\n",
      "        the objective function :math:`f(P) = trace(A^T P B P^T )`.\n",
      "        \n",
      "        >>> perm = res['col_ind']\n",
      "        >>> P = np.eye(len(A), dtype=int)[perm]\n",
      "        >>> fun = np.trace(A.T @ P @ B @ P.T)\n",
      "        >>> print(fun)\n",
      "        3260\n",
      "        \n",
      "        Alternatively, to avoid constructing the permutation matrix explicitly,\n",
      "        directly permute the rows and columns of the distance matrix.\n",
      "        \n",
      "        >>> fun = np.trace(A.T @ B[perm][:, perm])\n",
      "        >>> print(fun)\n",
      "        3260\n",
      "        \n",
      "        Although not guaranteed in general, ``quadratic_assignment`` happens to\n",
      "        have found the globally optimal solution.\n",
      "        \n",
      "        >>> from itertools import permutations\n",
      "        >>> perm_opt, fun_opt = None, np.inf\n",
      "        >>> for perm in permutations([0, 1, 2, 3]):\n",
      "        ...     perm = np.array(perm)\n",
      "        ...     fun = np.trace(A.T @ B[perm][:, perm])\n",
      "        ...     if fun < fun_opt:\n",
      "        ...         fun_opt, perm_opt = fun, perm\n",
      "        >>> print(np.array_equal(perm_opt, res['col_ind']))\n",
      "        True\n",
      "        \n",
      "        Here is an example for which the default method,\n",
      "        :ref:`'faq' <optimize.qap-faq>`, does not find the global optimum.\n",
      "        \n",
      "        >>> A = np.array([[0, 5, 8, 6], [5, 0, 5, 1],\n",
      "        ...               [8, 5, 0, 2], [6, 1, 2, 0]])\n",
      "        >>> B = np.array([[0, 1, 8, 4], [1, 0, 5, 2],\n",
      "        ...               [8, 5, 0, 5], [4, 2, 5, 0]])\n",
      "        >>> res = quadratic_assignment(A, B)\n",
      "        >>> print(res)\n",
      "         col_ind: array([1, 0, 3, 2])\n",
      "             fun: 178\n",
      "             nit: 13\n",
      "        \n",
      "        If accuracy is important, consider using  :ref:`'2opt' <optimize.qap-2opt>`\n",
      "        to refine the solution.\n",
      "        \n",
      "        >>> guess = np.array([np.arange(len(A)), res.col_ind]).T\n",
      "        >>> res = quadratic_assignment(A, B, method=\"2opt\",\n",
      "        ...                            options = {'partial_guess': guess})\n",
      "        >>> print(res)\n",
      "         col_ind: array([1, 2, 3, 0])\n",
      "             fun: 176\n",
      "             nit: 17\n",
      "    \n",
      "    ridder(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in an interval using Ridder's method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.\n",
      "            In particular, ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton : 1-D root-finding\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses [Ridders1979]_ method to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
      "        generally as fast as the Brent routines. [Ridders1979]_ provides the\n",
      "        classic description and source of the algorithm. A description can also be\n",
      "        found in any recent edition of Numerical Recipes.\n",
      "        \n",
      "        The routine used here diverges slightly from standard presentations in\n",
      "        order to be a bit more careful of tolerance.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ridders1979]\n",
      "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
      "           Single Root of a Real Continuous Function.\"\n",
      "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.ridder(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        >>> root = optimize.ridder(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "    \n",
      "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
      "        Find a root of a vector function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            A vector function to find a root of.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its Jacobian.\n",
      "        method : str, optional\n",
      "            Type of solver. Should be one of\n",
      "        \n",
      "                - 'hybr'             :ref:`(see here) <optimize.root-hybr>`\n",
      "                - 'lm'               :ref:`(see here) <optimize.root-lm>`\n",
      "                - 'broyden1'         :ref:`(see here) <optimize.root-broyden1>`\n",
      "                - 'broyden2'         :ref:`(see here) <optimize.root-broyden2>`\n",
      "                - 'anderson'         :ref:`(see here) <optimize.root-anderson>`\n",
      "                - 'linearmixing'     :ref:`(see here) <optimize.root-linearmixing>`\n",
      "                - 'diagbroyden'      :ref:`(see here) <optimize.root-diagbroyden>`\n",
      "                - 'excitingmixing'   :ref:`(see here) <optimize.root-excitingmixing>`\n",
      "                - 'krylov'           :ref:`(see here) <optimize.root-krylov>`\n",
      "                - 'df-sane'          :ref:`(see here) <optimize.root-dfsane>`\n",
      "        \n",
      "        jac : bool or callable, optional\n",
      "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "            value of Jacobian along with the objective function. If False, the\n",
      "            Jacobian will be estimated numerically.\n",
      "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
      "            this case, it must accept the same arguments as `fun`.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g., `xtol` or `maxiter`, see\n",
      "            :obj:`show_options()` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : OptimizeResult\n",
      "            The solution represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the algorithm exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *hybr*.\n",
      "        \n",
      "        Method *hybr* uses a modification of the Powell hybrid method as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *lm* solves the system of nonlinear equations in a least squares\n",
      "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *df-sane* is a derivative-free spectral method. [3]_\n",
      "        \n",
      "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
      "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
      "        with backtracking or full line searches [2]_. Each method corresponds\n",
      "        to a particular Jacobian approximations. See `nonlin` for details.\n",
      "        \n",
      "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
      "          known as Broyden's good method.\n",
      "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
      "          is known as Broyden's bad method.\n",
      "        - Method *anderson* uses (extended) Anderson mixing.\n",
      "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
      "          is suitable for large-scale problem.\n",
      "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
      "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
      "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
      "          approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            The algorithms implemented for methods *diagbroyden*,\n",
      "            *linearmixing* and *excitingmixing* may be useful for specific\n",
      "            problems, but whether they will work may depend strongly on the\n",
      "            problem.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
      "           1980. User Guide for MINPACK-1.\n",
      "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
      "           Equations. Society for Industrial and Applied Mathematics.\n",
      "           <https://archive.siam.org/books/kelley/fr16/>\n",
      "        .. [3] W. La Cruz, J.M. Martinez, M. Raydan. Math. Comp. 75, 1429 (2006).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations and its\n",
      "        jacobian.\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        >>> def jac(x):\n",
      "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
      "        ...                       -1.5 * (x[0] - x[1])**2],\n",
      "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
      "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
      "        >>> sol.x\n",
      "        array([ 0.8411639,  0.1588361])\n",
      "    \n",
      "    root_scalar(f, args=(), method=None, bracket=None, fprime=None, fprime2=None, x0=None, x1=None, xtol=None, rtol=None, maxiter=None, options=None)\n",
      "        Find a root of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            A function to find a root of.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its derivative(s).\n",
      "        method : str, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'bisect'    :ref:`(see here) <optimize.root_scalar-bisect>`\n",
      "                - 'brentq'    :ref:`(see here) <optimize.root_scalar-brentq>`\n",
      "                - 'brenth'    :ref:`(see here) <optimize.root_scalar-brenth>`\n",
      "                - 'ridder'    :ref:`(see here) <optimize.root_scalar-ridder>`\n",
      "                - 'toms748'    :ref:`(see here) <optimize.root_scalar-toms748>`\n",
      "                - 'newton'    :ref:`(see here) <optimize.root_scalar-newton>`\n",
      "                - 'secant'    :ref:`(see here) <optimize.root_scalar-secant>`\n",
      "                - 'halley'    :ref:`(see here) <optimize.root_scalar-halley>`\n",
      "        \n",
      "        bracket: A sequence of 2 floats, optional\n",
      "            An interval bracketing a root.  `f(x, *args)` must have different\n",
      "            signs at the two endpoints.\n",
      "        x0 : float, optional\n",
      "            Initial guess.\n",
      "        x1 : float, optional\n",
      "            A second guess.\n",
      "        fprime : bool or callable, optional\n",
      "            If `fprime` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the derivative.\n",
      "            `fprime` can also be a callable returning the derivative of `f`. In\n",
      "            this case, it must accept the same arguments as `f`.\n",
      "        fprime2 : bool or callable, optional\n",
      "            If `fprime2` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the\n",
      "            first and second derivatives.\n",
      "            `fprime2` can also be a callable returning the second derivative of `f`.\n",
      "            In this case, it must accept the same arguments as `f`.\n",
      "        xtol : float, optional\n",
      "            Tolerance (absolute) for termination.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g., ``k``, see\n",
      "            :obj:`show_options()` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : RootResults\n",
      "            The solution represented as a ``RootResults`` object.\n",
      "            Important attributes are: ``root`` the solution , ``converged`` a\n",
      "            boolean flag indicating if the algorithm exited successfully and\n",
      "            ``flag`` which describes the cause of the termination. See\n",
      "            `RootResults` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        root : Find a root of a vector function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "        \n",
      "        The default is to use the best method available for the situation\n",
      "        presented.\n",
      "        If a bracket is provided, it may use one of the bracketing methods.\n",
      "        If a derivative and an initial value are specified, it may\n",
      "        select one of the derivative-based methods.\n",
      "        If no method is judged applicable, it will raise an Exception.\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Find the root of a simple cubic\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        >>> def fprime(x):\n",
      "        ...     return 3*x**2\n",
      "        \n",
      "        The `brentq` method takes as input a bracket\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f, bracket=[0, 3], method='brentq')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 10, 11)\n",
      "        \n",
      "        The `newton` method takes as input a single point and uses the derivative(s)\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f, x0=0.2, fprime=fprime, method='newton')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 22)\n",
      "        \n",
      "        The function can provide the value and derivative(s) in a single call.\n",
      "        \n",
      "        >>> def f_p_pp(x):\n",
      "        ...     return (x**3 - 1), 3*x**2, 6*x\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f_p_pp, x0=0.2, fprime=True, method='newton')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 11)\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f_p_pp, x0=0.2, fprime=True, fprime2=True, method='halley')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 7, 8)\n",
      "    \n",
      "    rosen(x)\n",
      "        The Rosenbrock function.\n",
      "        \n",
      "        The function computed is::\n",
      "        \n",
      "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Rosenbrock function is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f : float\n",
      "            The value of the Rosenbrock function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen_der, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen\n",
      "        >>> X = 0.1 * np.arange(10)\n",
      "        >>> rosen(X)\n",
      "        76.56\n",
      "        \n",
      "        For higher-dimensional input ``rosen`` broadcasts.\n",
      "        In the following example, we use this to plot a 2D landscape.\n",
      "        Note that ``rosen_hess`` does not broadcast in this manner.\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from mpl_toolkits.mplot3d import Axes3D\n",
      "        >>> x = np.linspace(-1, 1, 50)\n",
      "        >>> X, Y = np.meshgrid(x, x)\n",
      "        >>> ax = plt.subplot(111, projection='3d')\n",
      "        >>> ax.plot_surface(X, Y, rosen([X, Y]))\n",
      "        >>> plt.show()\n",
      "    \n",
      "    rosen_der(x)\n",
      "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the derivative is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_der : (N,) ndarray\n",
      "            The gradient of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_der\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> rosen_der(X)\n",
      "        array([ -2. ,  10.6,  15.6,  13.4,   6.4,  -3. , -12.4, -19.4,  62. ])\n",
      "    \n",
      "    rosen_hess(x)\n",
      "        The Hessian matrix of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess\n",
      "        >>> X = 0.1 * np.arange(4)\n",
      "        >>> rosen_hess(X)\n",
      "        array([[-38.,   0.,   0.,   0.],\n",
      "               [  0., 134., -40.,   0.],\n",
      "               [  0., -40., 130., -80.],\n",
      "               [  0.,   0., -80., 200.]])\n",
      "    \n",
      "    rosen_hess_prod(x, p)\n",
      "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        p : array_like\n",
      "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess_prod : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
      "            by the vector `p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess_prod\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> p = 0.5 * np.arange(9)\n",
      "        >>> rosen_hess_prod(X, p)\n",
      "        array([  -0.,   27.,  -10.,  -95., -192., -265., -278., -195., -180.])\n",
      "    \n",
      "    shgo(func, bounds, args=(), constraints=None, n=100, iters=1, callback=None, minimizer_kwargs=None, options=None, sampling_method='simplicial')\n",
      "        Finds the global minimum of a function using SHG optimization.\n",
      "        \n",
      "        SHGO stands for \"simplicial homology global optimization\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized.  Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence\n",
      "            Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
      "            defining the lower and upper bounds for the optimizing argument of\n",
      "            `func`. It is required to have ``len(bounds) == len(x)``.\n",
      "            ``len(bounds)`` is used to determine the number of parameters in ``x``.\n",
      "            Use ``None`` for one of min or max when there is no bound in that\n",
      "            direction. By default bounds are ``(None, None)``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        constraints : dict or sequence of dict, optional\n",
      "            Constraints definition.\n",
      "            Function(s) ``R**n`` in the form::\n",
      "        \n",
      "                g(x) >= 0 applied as g : R^n -> R^m\n",
      "                h(x) == 0 applied as h : R^n -> R^p\n",
      "        \n",
      "            Each constraint is defined in a dictionary with fields:\n",
      "        \n",
      "                type : str\n",
      "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "                fun : callable\n",
      "                    The function defining the constraint.\n",
      "                jac : callable, optional\n",
      "                    The Jacobian of `fun` (only for SLSQP).\n",
      "                args : sequence, optional\n",
      "                    Extra arguments to be passed to the function and Jacobian.\n",
      "        \n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "            Note that COBYLA only supports inequality constraints.\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "               Only the COBYLA and SLSQP local minimize methods currently\n",
      "               support constraint arguments. If the ``constraints`` sequence\n",
      "               used in the local optimization problem is not defined in\n",
      "               ``minimizer_kwargs`` and a constrained method is used then the\n",
      "               global ``constraints`` will be used.\n",
      "               (Defining a ``constraints`` sequence in ``minimizer_kwargs``\n",
      "               means that ``constraints`` will not be added so if equality\n",
      "               constraints and so forth need to be added then the inequality\n",
      "               functions in ``constraints`` need to be added to\n",
      "               ``minimizer_kwargs`` too).\n",
      "        \n",
      "        n : int, optional\n",
      "            Number of sampling points used in the construction of the simplicial\n",
      "            complex. Note that this argument is only used for ``sobol`` and other\n",
      "            arbitrary `sampling_methods`.\n",
      "        iters : int, optional\n",
      "            Number of iterations used in the construction of the simplicial complex.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the minimizer\n",
      "            ``scipy.optimize.minimize`` Some important options could be:\n",
      "        \n",
      "                * method : str\n",
      "                    The minimization method (e.g. ``SLSQP``).\n",
      "                * args : tuple\n",
      "                    Extra arguments passed to the objective function (``func``) and\n",
      "                    its derivatives (Jacobian, Hessian).\n",
      "                * options : dict, optional\n",
      "                    Note that by default the tolerance is specified as\n",
      "                    ``{ftol: 1e-12}``\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. Many of the options specified for the\n",
      "            global routine are also passed to the scipy.optimize.minimize routine.\n",
      "            The options that are also passed to the local routine are marked with\n",
      "            \"(L)\".\n",
      "        \n",
      "            Stopping criteria, the algorithm will terminate if any of the specified\n",
      "            criteria are met. However, the default algorithm does not require any to\n",
      "            be specified:\n",
      "        \n",
      "            * maxfev : int (L)\n",
      "                Maximum number of function evaluations in the feasible domain.\n",
      "                (Note only methods that support this option will terminate\n",
      "                the routine at precisely exact specified value. Otherwise the\n",
      "                criterion will only terminate during a global iteration)\n",
      "            * f_min\n",
      "                Specify the minimum objective function value, if it is known.\n",
      "            * f_tol : float\n",
      "                Precision goal for the value of f in the stopping\n",
      "                criterion. Note that the global routine will also\n",
      "                terminate if a sampling point in the global routine is\n",
      "                within this tolerance.\n",
      "            * maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            * maxev : int\n",
      "                Maximum number of sampling evaluations to perform (includes\n",
      "                searching in infeasible points).\n",
      "            * maxtime : float\n",
      "                Maximum processing runtime allowed\n",
      "            * minhgrd : int\n",
      "                Minimum homology group rank differential. The homology group of the\n",
      "                objective function is calculated (approximately) during every\n",
      "                iteration. The rank of this group has a one-to-one correspondence\n",
      "                with the number of locally convex subdomains in the objective\n",
      "                function (after adequate sampling points each of these subdomains\n",
      "                contain a unique global minimum). If the difference in the hgr is 0\n",
      "                between iterations for ``maxhgrd`` specified iterations the\n",
      "                algorithm will terminate.\n",
      "        \n",
      "            Objective function knowledge:\n",
      "        \n",
      "            * symmetry : bool\n",
      "                Specify True if the objective function contains symmetric variables.\n",
      "                The search space (and therefore performance) is decreased by O(n!).\n",
      "        \n",
      "            * jac : bool or callable, optional\n",
      "                Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "                Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. If ``jac`` is a\n",
      "                boolean and is True, ``fun`` is assumed to return the gradient along\n",
      "                with the objective function. If False, the gradient will be\n",
      "                estimated numerically. ``jac`` can also be a callable returning the\n",
      "                gradient of the objective. In this case, it must accept the same\n",
      "                arguments as ``fun``. (Passed to `scipy.optimize.minmize` automatically)\n",
      "        \n",
      "            * hess, hessp : callable, optional\n",
      "                Hessian (matrix of second-order derivatives) of objective function\n",
      "                or Hessian of objective function times an arbitrary vector p.\n",
      "                Only for Newton-CG, dogleg, trust-ncg. Only one of ``hessp`` or\n",
      "                ``hess`` needs to be given. If ``hess`` is provided, then\n",
      "                ``hessp`` will be ignored. If neither ``hess`` nor ``hessp`` is\n",
      "                provided, then the Hessian product will be approximated using\n",
      "                finite differences on ``jac``. ``hessp`` must compute the Hessian\n",
      "                times an arbitrary vector. (Passed to `scipy.optimize.minmize`\n",
      "                automatically)\n",
      "        \n",
      "            Algorithm settings:\n",
      "        \n",
      "            * minimize_every_iter : bool\n",
      "                If True then promising global sampling points will be passed to a\n",
      "                local minimization routine every iteration. If False then only the\n",
      "                final minimizer pool will be run. Defaults to False.\n",
      "            * local_iter : int\n",
      "                Only evaluate a few of the best minimizer pool candidates every\n",
      "                iteration. If False all potential points are passed to the local\n",
      "                minimization routine.\n",
      "            * infty_constraints: bool\n",
      "                If True then any sampling points generated which are outside will\n",
      "                the feasible domain will be saved and given an objective function\n",
      "                value of ``inf``. If False then these points will be discarded.\n",
      "                Using this functionality could lead to higher performance with\n",
      "                respect to function evaluations before the global minimum is found,\n",
      "                specifying False will use less memory at the cost of a slight\n",
      "                decrease in performance. Defaults to True.\n",
      "        \n",
      "            Feedback:\n",
      "        \n",
      "            * disp : bool (L)\n",
      "                Set to True to print convergence messages.\n",
      "        \n",
      "        sampling_method : str or function, optional\n",
      "            Current built in sampling method options are ``sobol`` and\n",
      "            ``simplicial``. The default ``simplicial`` uses less memory and provides\n",
      "            the theoretical guarantee of convergence to the global minimum in finite\n",
      "            time. The ``sobol`` method is faster in terms of sampling point\n",
      "            generation at the cost of higher memory resources and the loss of\n",
      "            guaranteed convergence. It is more appropriate for most \"easier\"\n",
      "            problems where the convergence is relatively fast.\n",
      "            User defined sampling functions must accept two arguments of ``n``\n",
      "            sampling points of dimension ``dim`` per call and output an array of\n",
      "            sampling points with shape `n x dim`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are:\n",
      "            ``x`` the solution array corresponding to the global minimum,\n",
      "            ``fun`` the function output at the global solution,\n",
      "            ``xl`` an ordered list of local minima solutions,\n",
      "            ``funl`` the function output at the corresponding local solutions,\n",
      "            ``success`` a Boolean flag indicating if the optimizer exited\n",
      "            successfully,\n",
      "            ``message`` which describes the cause of the termination,\n",
      "            ``nfev`` the total number of objective function evaluations including\n",
      "            the sampling calls,\n",
      "            ``nlfev`` the total number of objective function evaluations\n",
      "            culminating from all local search optimizations,\n",
      "            ``nit`` number of iterations performed by the global routine.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Global optimization using simplicial homology global optimization [1]_.\n",
      "        Appropriate for solving general purpose NLP and blackbox optimization\n",
      "        problems to global optimality (low-dimensional problems).\n",
      "        \n",
      "        In general, the optimization problems are of the form::\n",
      "        \n",
      "            minimize f(x) subject to\n",
      "        \n",
      "            g_i(x) >= 0,  i = 1,...,m\n",
      "            h_j(x)  = 0,  j = 1,...,p\n",
      "        \n",
      "        where x is a vector of one or more variables. ``f(x)`` is the objective\n",
      "        function ``R^n -> R``, ``g_i(x)`` are the inequality constraints, and\n",
      "        ``h_j(x)`` are the equality constraints.\n",
      "        \n",
      "        Optionally, the lower and upper bounds for each element in x can also be\n",
      "        specified using the `bounds` argument.\n",
      "        \n",
      "        While most of the theoretical advantages of SHGO are only proven for when\n",
      "        ``f(x)`` is a Lipschitz smooth function, the algorithm is also proven to\n",
      "        converge to the global optimum for the more general case where ``f(x)`` is\n",
      "        non-continuous, non-convex and non-smooth, if the default sampling method\n",
      "        is used [1]_.\n",
      "        \n",
      "        The local search method may be specified using the ``minimizer_kwargs``\n",
      "        parameter which is passed on to ``scipy.optimize.minimize``. By default,\n",
      "        the ``SLSQP`` method is used. In general, it is recommended to use the\n",
      "        ``SLSQP`` or ``COBYLA`` local minimization if inequality constraints\n",
      "        are defined for the problem since the other methods do not use constraints.\n",
      "        \n",
      "        The ``sobol`` method points are generated using the Sobol (1967) [2]_\n",
      "        sequence. The primitive polynomials and various sets of initial direction\n",
      "        numbers for generating Sobol sequences is provided by [3]_ by Frances Kuo\n",
      "        and Stephen Joe. The original program sobol.cc (MIT) is available and\n",
      "        described at https://web.maths.unsw.edu.au/~fkuo/sobol/ translated to\n",
      "        Python 3 by Carl Sandrock 2016-03-31.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Endres, SC, Sandrock, C, Focke, WW (2018) \"A simplicial homology\n",
      "               algorithm for lipschitz optimisation\", Journal of Global Optimization.\n",
      "        .. [2] Sobol, IM (1967) \"The distribution of points in a cube and the\n",
      "               approximate evaluation of integrals\", USSR Comput. Math. Math. Phys.\n",
      "               7, 86-112.\n",
      "        .. [3] Joe, SW and Kuo, FY (2008) \"Constructing Sobol sequences with\n",
      "               better  two-dimensional projections\", SIAM J. Sci. Comput. 30,\n",
      "               2635-2654.\n",
      "        .. [4] Hoch, W and Schittkowski, K (1981) \"Test examples for nonlinear\n",
      "               programming codes\", Lecture Notes in Economics and Mathematical\n",
      "               Systems, 187. Springer-Verlag, New York.\n",
      "               http://www.ai7.uni-bayreuth.de/test_problem_coll.pdf\n",
      "        .. [5] Wales, DJ (2015) \"Perspective: Insight into reaction coordinates and\n",
      "               dynamics from the potential energy landscape\",\n",
      "               Journal of Chemical Physics, 142(13), 2015.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First consider the problem of minimizing the Rosenbrock function, `rosen`:\n",
      "        \n",
      "        >>> from scipy.optimize import rosen, shgo\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 1.,  1.,  1.,  1.,  1.]), 2.9203923741900809e-18)\n",
      "        \n",
      "        Note that bounds determine the dimensionality of the objective\n",
      "        function and is therefore a required input, however you can specify\n",
      "        empty bounds using ``None`` or objects like ``np.inf`` which will be\n",
      "        converted to large float numbers.\n",
      "        \n",
      "        >>> bounds = [(None, None), ]*4\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x\n",
      "        array([ 0.99999851,  0.99999704,  0.99999411,  0.9999882 ])\n",
      "        \n",
      "        Next, we consider the Eggholder function, a problem with several local\n",
      "        minima and one global minimum. We will demonstrate the use of arguments and\n",
      "        the capabilities of `shgo`.\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization)\n",
      "        \n",
      "        >>> def eggholder(x):\n",
      "        ...     return (-(x[1] + 47.0)\n",
      "        ...             * np.sin(np.sqrt(abs(x[0]/2.0 + (x[1] + 47.0))))\n",
      "        ...             - x[0] * np.sin(np.sqrt(abs(x[0] - (x[1] + 47.0))))\n",
      "        ...             )\n",
      "        ...\n",
      "        >>> bounds = [(-512, 512), (-512, 512)]\n",
      "        \n",
      "        `shgo` has two built-in low discrepancy sampling sequences. First, we will\n",
      "        input 30 initial sampling points of the Sobol sequence:\n",
      "        \n",
      "        >>> result = shgo(eggholder, bounds, n=30, sampling_method='sobol')\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 512.        ,  404.23180542]), -959.64066272085051)\n",
      "        \n",
      "        `shgo` also has a return for any other local minima that was found, these\n",
      "        can be called using:\n",
      "        \n",
      "        >>> result.xl\n",
      "        array([[ 512.        ,  404.23180542],\n",
      "               [ 283.07593402, -487.12566542],\n",
      "               [-294.66820039, -462.01964031],\n",
      "               [-105.87688985,  423.15324143],\n",
      "               [-242.97923629,  274.38032063],\n",
      "               [-506.25823477,    6.3131022 ],\n",
      "               [-408.71981195, -156.10117154],\n",
      "               [ 150.23210485,  301.31378508],\n",
      "               [  91.00922754, -391.28375925],\n",
      "               [ 202.8966344 , -269.38042147],\n",
      "               [ 361.66625957, -106.96490692],\n",
      "               [-219.40615102, -244.06022436],\n",
      "               [ 151.59603137, -100.61082677]])\n",
      "        \n",
      "        >>> result.funl\n",
      "        array([-959.64066272, -718.16745962, -704.80659592, -565.99778097,\n",
      "               -559.78685655, -557.36868733, -507.87385942, -493.9605115 ,\n",
      "               -426.48799655, -421.15571437, -419.31194957, -410.98477763,\n",
      "               -202.53912972])\n",
      "        \n",
      "        These results are useful in applications where there are many global minima\n",
      "        and the values of other global minima are desired or where the local minima\n",
      "        can provide insight into the system (for example morphologies\n",
      "        in physical chemistry [5]_).\n",
      "        \n",
      "        If we want to find a larger number of local minima, we can increase the\n",
      "        number of sampling points or the number of iterations. We'll increase the\n",
      "        number of sampling points to 60 and the number of iterations from the\n",
      "        default of 1 to 5. This gives us 60 x 5 = 300 initial sampling points.\n",
      "        \n",
      "        >>> result_2 = shgo(eggholder, bounds, n=60, iters=5, sampling_method='sobol')\n",
      "        >>> len(result.xl), len(result_2.xl)\n",
      "        (13, 39)\n",
      "        \n",
      "        Note the difference between, e.g., ``n=180, iters=1`` and ``n=60, iters=3``.\n",
      "        In the first case the promising points contained in the minimiser pool\n",
      "        is processed only once. In the latter case it is processed every 60 sampling\n",
      "        points for a total of 3 times.\n",
      "        \n",
      "        To demonstrate solving problems with non-linear constraints consider the\n",
      "        following example from Hock and Schittkowski problem 73 (cattle-feed) [4]_::\n",
      "        \n",
      "            minimize: f = 24.55 * x_1 + 26.75 * x_2 + 39 * x_3 + 40.50 * x_4\n",
      "        \n",
      "            subject to: 2.3 * x_1 + 5.6 * x_2 + 11.1 * x_3 + 1.3 * x_4 - 5     >= 0,\n",
      "        \n",
      "                        12 * x_1 + 11.9 * x_2 + 41.8 * x_3 + 52.1 * x_4 - 21\n",
      "                            -1.645 * sqrt(0.28 * x_1**2 + 0.19 * x_2**2 +\n",
      "                                          20.5 * x_3**2 + 0.62 * x_4**2)       >= 0,\n",
      "        \n",
      "                        x_1 + x_2 + x_3 + x_4 - 1                              == 0,\n",
      "        \n",
      "                        1 >= x_i >= 0 for all i\n",
      "        \n",
      "        The approximate answer given in [4]_ is::\n",
      "        \n",
      "            f([0.6355216, -0.12e-11, 0.3127019, 0.05177655]) = 29.894378\n",
      "        \n",
      "        >>> def f(x):  # (cattle-feed)\n",
      "        ...     return 24.55*x[0] + 26.75*x[1] + 39*x[2] + 40.50*x[3]\n",
      "        ...\n",
      "        >>> def g1(x):\n",
      "        ...     return 2.3*x[0] + 5.6*x[1] + 11.1*x[2] + 1.3*x[3] - 5  # >=0\n",
      "        ...\n",
      "        >>> def g2(x):\n",
      "        ...     return (12*x[0] + 11.9*x[1] +41.8*x[2] + 52.1*x[3] - 21\n",
      "        ...             - 1.645 * np.sqrt(0.28*x[0]**2 + 0.19*x[1]**2\n",
      "        ...                             + 20.5*x[2]**2 + 0.62*x[3]**2)\n",
      "        ...             ) # >=0\n",
      "        ...\n",
      "        >>> def h1(x):\n",
      "        ...     return x[0] + x[1] + x[2] + x[3] - 1  # == 0\n",
      "        ...\n",
      "        >>> cons = ({'type': 'ineq', 'fun': g1},\n",
      "        ...         {'type': 'ineq', 'fun': g2},\n",
      "        ...         {'type': 'eq', 'fun': h1})\n",
      "        >>> bounds = [(0, 1.0),]*4\n",
      "        >>> res = shgo(f, bounds, iters=3, constraints=cons)\n",
      "        >>> res\n",
      "             fun: 29.894378159142136\n",
      "            funl: array([29.89437816])\n",
      "         message: 'Optimization terminated successfully.'\n",
      "            nfev: 114\n",
      "             nit: 3\n",
      "           nlfev: 35\n",
      "           nlhev: 0\n",
      "           nljev: 5\n",
      "         success: True\n",
      "               x: array([6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02])\n",
      "              xl: array([[6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02]])\n",
      "        \n",
      "        >>> g1(res.x), g2(res.x), h1(res.x)\n",
      "        (-5.0626169922907138e-14, -2.9594104944408173e-12, 0.0)\n",
      "    \n",
      "    show_options(solver=None, method=None, disp=True)\n",
      "        Show documentation for additional options of optimization solvers.\n",
      "        \n",
      "        These are method-specific options that can be supplied through the\n",
      "        ``options`` dict.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        solver : str\n",
      "            Type of optimization solver. One of 'minimize', 'minimize_scalar',\n",
      "            'root', 'root_scalar', 'linprog', or 'quadratic_assignment'.\n",
      "        method : str, optional\n",
      "            If not given, shows all methods of the specified solver. Otherwise,\n",
      "            show only the options for the specified method. Valid values\n",
      "            corresponds to methods' names of respective solver (e.g., 'BFGS' for\n",
      "            'minimize').\n",
      "        disp : bool, optional\n",
      "            Whether to print the result rather than returning it.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        text\n",
      "            Either None (for disp=True) or the text string (disp=False)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The solver-specific methods are:\n",
      "        \n",
      "        `scipy.optimize.minimize`\n",
      "        \n",
      "        - :ref:`Nelder-Mead <optimize.minimize-neldermead>`\n",
      "        - :ref:`Powell      <optimize.minimize-powell>`\n",
      "        - :ref:`CG          <optimize.minimize-cg>`\n",
      "        - :ref:`BFGS        <optimize.minimize-bfgs>`\n",
      "        - :ref:`Newton-CG   <optimize.minimize-newtoncg>`\n",
      "        - :ref:`L-BFGS-B    <optimize.minimize-lbfgsb>`\n",
      "        - :ref:`TNC         <optimize.minimize-tnc>`\n",
      "        - :ref:`COBYLA      <optimize.minimize-cobyla>`\n",
      "        - :ref:`SLSQP       <optimize.minimize-slsqp>`\n",
      "        - :ref:`dogleg      <optimize.minimize-dogleg>`\n",
      "        - :ref:`trust-ncg   <optimize.minimize-trustncg>`\n",
      "        \n",
      "        `scipy.optimize.root`\n",
      "        \n",
      "        - :ref:`hybr              <optimize.root-hybr>`\n",
      "        - :ref:`lm                <optimize.root-lm>`\n",
      "        - :ref:`broyden1          <optimize.root-broyden1>`\n",
      "        - :ref:`broyden2          <optimize.root-broyden2>`\n",
      "        - :ref:`anderson          <optimize.root-anderson>`\n",
      "        - :ref:`linearmixing      <optimize.root-linearmixing>`\n",
      "        - :ref:`diagbroyden       <optimize.root-diagbroyden>`\n",
      "        - :ref:`excitingmixing    <optimize.root-excitingmixing>`\n",
      "        - :ref:`krylov            <optimize.root-krylov>`\n",
      "        - :ref:`df-sane           <optimize.root-dfsane>`\n",
      "        \n",
      "        `scipy.optimize.minimize_scalar`\n",
      "        \n",
      "        - :ref:`brent       <optimize.minimize_scalar-brent>`\n",
      "        - :ref:`golden      <optimize.minimize_scalar-golden>`\n",
      "        - :ref:`bounded     <optimize.minimize_scalar-bounded>`\n",
      "        \n",
      "        `scipy.optimize.root_scalar`\n",
      "        \n",
      "        - :ref:`bisect  <optimize.root_scalar-bisect>`\n",
      "        - :ref:`brentq  <optimize.root_scalar-brentq>`\n",
      "        - :ref:`brenth  <optimize.root_scalar-brenth>`\n",
      "        - :ref:`ridder  <optimize.root_scalar-ridder>`\n",
      "        - :ref:`toms748 <optimize.root_scalar-toms748>`\n",
      "        - :ref:`newton  <optimize.root_scalar-newton>`\n",
      "        - :ref:`secant  <optimize.root_scalar-secant>`\n",
      "        - :ref:`halley  <optimize.root_scalar-halley>`\n",
      "        \n",
      "        `scipy.optimize.linprog`\n",
      "        \n",
      "        - :ref:`simplex           <optimize.linprog-simplex>`\n",
      "        - :ref:`interior-point    <optimize.linprog-interior-point>`\n",
      "        - :ref:`revised simplex   <optimize.linprog-revised_simplex>`\n",
      "        - :ref:`highs             <optimize.linprog-highs>`\n",
      "        - :ref:`highs-ds          <optimize.linprog-highs-ds>`\n",
      "        - :ref:`highs-ipm         <optimize.linprog-highs-ipm>`\n",
      "        \n",
      "        `scipy.optimize.quadratic_assignment`\n",
      "        \n",
      "        - :ref:`faq             <optimize.qap-faq>`\n",
      "        - :ref:`2opt            <optimize.qap-2opt>`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We can print documentations of a solver in stdout:\n",
      "        \n",
      "        >>> from scipy.optimize import show_options\n",
      "        >>> show_options(solver=\"minimize\")\n",
      "        ...\n",
      "        \n",
      "        Specifying a method is possible:\n",
      "        \n",
      "        >>> show_options(solver=\"minimize\", method=\"Nelder-Mead\")\n",
      "        ...\n",
      "        \n",
      "        We can also get the documentations as a string:\n",
      "        \n",
      "        >>> show_options(solver=\"minimize\", method=\"Nelder-Mead\", disp=False)\n",
      "        Minimization of scalar function of one or more variables using the ...\n",
      "    \n",
      "    toms748(f, a, b, args=(), k=1, xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a zero using TOMS Algorithm 748 method.\n",
      "        \n",
      "        Implements the Algorithm 748 method of Alefeld, Potro and Shi to find a\n",
      "        zero of the function `f` on the interval `[a , b]`, where `f(a)` and\n",
      "        `f(b)` must have opposite signs.\n",
      "        \n",
      "        It uses a mixture of inverse cubic interpolation and\n",
      "        \"Newton-quadratic\" steps. [APS1995].\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a scalar. The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)`\n",
      "            have opposite signs.\n",
      "        a : scalar,\n",
      "            lower boundary of the search interval\n",
      "        b : scalar,\n",
      "            upper boundary of the search interval\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``f(x, *args)``.\n",
      "        k : int, optional\n",
      "            The number of Newton quadratic steps to perform each\n",
      "            iteration. ``k>=1``.\n",
      "        xtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in the `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Approximate Zero of `f`\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect, newton\n",
      "        fsolve : find zeroes in N dimensions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.\n",
      "        Algorithm 748 with ``k=2`` is asymptotically the most efficient\n",
      "        algorithm known for finding roots of a four times continuously\n",
      "        differentiable function.\n",
      "        In contrast with Brent's algorithm, which may only decrease the length of\n",
      "        the enclosing bracket on the last step, Algorithm 748 decreases it each\n",
      "        iteration with the same asymptotic efficiency as it finds the root.\n",
      "        \n",
      "        For easy statement of efficiency indices, assume that `f` has 4\n",
      "        continuouous deriviatives.\n",
      "        For ``k=1``, the convergence order is at least 2.7, and with about\n",
      "        asymptotically 2 function evaluations per iteration, the efficiency\n",
      "        index is approximately 1.65.\n",
      "        For ``k=2``, the order is about 4.6 with asymptotically 3 function\n",
      "        evaluations per iteration, and the efficiency index 1.66.\n",
      "        For higher values of `k`, the efficiency index approaches\n",
      "        the kth root of ``(3k-2)``, hence ``k=1`` or ``k=2`` are\n",
      "        usually appropriate.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [APS1995]\n",
      "           Alefeld, G. E. and Potra, F. A. and Shi, Yixun,\n",
      "           *Algorithm 748: Enclosing Zeros of Continuous Functions*,\n",
      "           ACM Trans. Math. Softw. Volume 221(1995)\n",
      "           doi = {10.1145/210089.210111}\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> root, results = optimize.toms748(f, 0, 2, full_output=True)\n",
      "        >>> root\n",
      "        1.0\n",
      "        >>> results\n",
      "              converged: True\n",
      "                   flag: 'converged'\n",
      "         function_calls: 11\n",
      "             iterations: 5\n",
      "                   root: 1.0\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BFGS', 'Bounds', 'HessianUpdateStrategy', 'LbfgsInvHessPro...\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41, 48, 51, 44, 55, 38, 47, 49, 45, 58, 41, 50, 57, 55, 53, 54, 49,\n",
       "       47, 38, 54, 44, 56, 53, 55, 56, 53, 55, 44, 54, 55, 49, 51, 56, 53,\n",
       "       54, 41, 49, 41, 52, 53, 58, 52, 53, 50, 56, 48, 41, 53, 52, 52, 49,\n",
       "       50, 59, 59, 53, 59, 56, 56, 43, 41, 57, 51, 47, 46, 50, 50, 55, 50,\n",
       "       45, 50, 51, 49, 54, 56, 46, 48, 45, 54, 57, 55, 48, 50, 42, 43, 47,\n",
       "       49, 52, 56, 55, 45, 56, 45, 52, 52, 61, 42, 48, 47, 54, 55])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coin Flip\n",
    "iterations = 100\n",
    "np.array([np.sum(npr.random(iterations) < 0.5) for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_5_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_mean (flips=int(1e3), num_experiments=100):\n",
    "    return np.mean(np.array([np.sum(npr.random(flips) < 0.5) for _ in range(num_experiments)]))\n",
    "\n",
    "means = []\n",
    "for i in range(1, int(1e3)):\n",
    "    means.append(get_mean(num_experiments=i))\n",
    "plt.plot(means);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_6_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts, bin_edges, _ = plt.hist(npr.random(100), bins=int(1e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_7_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_size = bin_edges[1] - bin_edges[0]\n",
    "new_widths = bin_size * counts / counts.max()\n",
    "plt.bar(bin_edges[:-1], counts, width=new_widths, color=['r', 'g', 'b']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_8_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log2bins = np.logspace(-8, 0, num=9, base=2)\n",
    "log2bins[0] = 0.0\n",
    "counts, bin_edges, _ = plt.hist(npr.random(100), bins=log2bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_9_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bin_size = bin_edges[1] - bin_edges[0]\n",
    "plt.bar(bin_edges[:-1], counts, width=bin_size, align='edge');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAFoCAYAAAB34a4mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABP3klEQVR4nO3de5BsZ0Hv/d+69X0ue24b2Bc3bEkUlISAgHVCkDfE+MY3dV4gQNgQCfHU0ZD3BBQBgRgQlIh1oDjyVkjMeUEuRsAickRPWRUCSMVoRIVYIiayrZ193zN7rt3Tt9VrrfePYfXu6eme6ctaz2Wt36cqlb1nuqef3d3T69vPenotIwiCAEREREQ0FFP2AIiIiIh0xIgiIiIiGgEjioiIiGgEjCgiIiKiETCiiIiIiEbAiCIiIiIawUAR9cQTT+CWW24BADz99NN44xvfiGPHjuEDH/gAfN+PdYBEREREKtozoh544AHcddddaDQaAIB77rkH73jHO/Dggw8iCAI88sgjsQ+SiIiISDV7RtThw4fxyU9+sv3373//+3jJS14CALjmmmvw2GOPxTc6IiIiIkXtGVHXX389bNtu/z0IAhiGAQAoFosol8vxjY6IiIhIUfbeF9nONC911+bmJiYnJ/e8zmte85phb0ZJU1NTqNfraDQacBwHCwsLsoc0sIMHD8oewg6HDh2SPYRdHT58WPYQRqLT85JG5/s+KpUKms2m8mtTT548KXsIuzp16pTsIexw+vRp2UMY2JkzZyL7WaZpwrIs+L4Pz/Ng2zZyuRwqlQqArckbYKs/bNtuT+ZE/Tvw0EMPDXS5oSPqec97Hh5//HG89KUvxbe//W287GUvG3pwujhw4MCOr5VKJQkjGZ2K8RQ6deqU0iHV+cKvU1AtLi7u+n1GlroWFxcxOTnZfrO6sbEB3/eRz+fheR6azSYymQyKxSKCIIDv+8oGlOrhFFIxoIBLr906xFSvbWWnYSLLtu12QAFAoVBor8kGtuIpZBgGZJ/+d+iIes973oPf+q3fwsc//nE85znPwfXXXx/HuITY64HXlcrhpKvuDYJOUdVtr8jqxugazbD3c2hjY2PH12q1WvvPzWYTzWZz5HHFRZdo0k3n67kOQdXLMJFlmiYymQyq1Sp834fjOD2f7/l8HoVCIZZZqGEYgYCMk7E7L6mB1I+u4aTyTNSgdA6quCUhwEaNoaRLSjSpOhO1F12DahQXLlxAq9UCsDX7VCwWkclksLGx0f561GLbnReXtEVPFHQNp06q79IbRJJmqaLGAEmWpIRTSNeAApIxQzWo/fv39/x6Pp8f+WdGtY5LSEQxkKKRhGhKA0YVJUXSoimpurcNSY+qKETVJcrMRNEljKVk0XWBOqVTWsJJ51movfTbhjCuoseIkoShdEkSdukNapANFEOL4pKWQKLedtvuMLBGw4iKGWOJhjXoho6xRSHGEY2Ls1ejYUSNiZFEsgyz4WRw6YVRFK8k78qLGmevdpf6iGIEqSFNu/RkELFRTlOoMXKIotl+6h5iykYU44ZILwwLIhqWqG19XLEmJKIYRDQIzkYRUdy4Ky+d4uoQc++LEBEREVE3ZXfnEUUpCAKsrq7CdV0YhoF9+/bBcZz292u1GtbX19unFNDtRNNERCQeZ6JIKXFNtddqNQRBgP3792Nqagpra2vt7wVBgLW1NSwsLGBhYQGVSqV9BnEiSg7uyqOoMaIoFRqNRvs8S9lsFq7rtr/nui5s24ZpmjAMA9lsFo1GQ9ZQiYhIE4woSoUgCGAYxo6v9fqeaZrwfV/o+IiISD9cE0XKGXfKvdcn/AzDaEdT59d6fc/3/W3rpaIcG1Gazc7Oolwuo9lswjAM7N+/H+fPn99xuZmZmfY6RiKVCYkofmxdfzrFw3XX3bjjaydOHMfJkydwzTXXYnHxPHz/H3Dddf8XAMD3PTz00BdxzTXXwbYd/MVfPIRXver/RLG4c3H5pz99b+zjJ0qqZrOJXC7X/n+v3eZzc3NoNBool8sSRhgdbvfSwQi6357H4MEHH4z7JkgAnULqttvetu3vQRDgscf+GqurywgC4OUv/z+wvLwE13XxEz/xfJw8eQLf+953EATAc5/7E3je8356x89kQBGNxzAMzMzMwLIsBEGA5eVl+L6PUqmEVqsFwzAwOzu7La7W19fRbDYljnp4DCj9HTt2bKDLcXceDSx8YdAppkKGYeA//aef2/a16el97T8fPnwEhw8fETsoopQJw6lbpVJp/1nn04AwntKHC8tpaHyhICK65NChQ3xdTClGFI0kbS8a3JVHRL2k6XWQdmJE0Vj4AkJEaZS2N5LUG9dE0dh0Xis1iDTNQum8HkUXPCG7/hhPFBISUYcPHxZxM32dPHlS6u2nxaFDhxIbUjpgAOkhiseJISYH40kc2d0wqFTMREXxYDDEBpP0WSkZGEfUbdDnBGMrOgyo4egSQeNKRURFYZgnBIMrObNSce3KYxiRCMM8zxhcvTGeLklLGA0jMRFlmiYmJiZgGAZarda2446I1u+Jlra44qwUY4n00eu5mvawSmtAMZYGl5iIKhQKaDabqNfrKBQKyGQyO45ym8lkkM/n4XkeGo0GXNcVOsbdnphJDixdY2qUWShGEyVJWsMqDfHEUIqGlhFVLBZh2zY8z2ufKsAwDDQaDQRBgFarhWw2uy2iHMdBoVDAxsYGHMdBJpMRHlG7ScPsVVJ28YUYTOKcOXNm6OscOHAghpFQ0sMqaQHFWIqXkHPnPfXUU1hcXIzs55mmCdu2YVkWLMuCbdtoNpuoVqsAgHw+D9u22yewNAwDuVwOjuPA8zwEQQDXdZWKqGHpHlciYqr7/HnD6jUTxXDqbZTI0R0jbSedY0r3eGIsRevqq68e6HLCZqIWFhZGvm53gPm+v22WKZfLwTQvHTfUsiy0Wq323w3DQDabRavVau/u8zxv5PGo4PDhw9qHlMoYUNulMZL2std9ksbICn9HdI4p3TCe+hunOwalxe68Qe+IUqkEAKjX69vOAg5shVej0YDnefA8D7Zt77iMbhhSYqQlnhhK0drt/kx6YDGmxEhbQImIomFpEVHDyuVyyOVy277mui7CPZemaWo/ExViSEUvnIVKWjwxktSRllms06dPM6RoJCoGUy+JjKheHMfBzMzMtq/VajVJo4kWQyp6ugcUg0lvvR4/XcOKs1LxSNIslOxgMk0Tvu+PdN3URFQvvR64KBfAi8SQisaHPvQ+2UMYGcMp2TofXx2DijEVHZ0DSnYw9ZLNZmFZ1kjHl0x1RPWic1gxpEanazwxnNJJ56BiTI1Hp4BSMZh6qdVqKBaLmJiYaH+qf1CMqAHoFFYMqeHpFlAMJ+qka1BxvdTwVA4oXYKpn83NTeTzeUxOTg61ZpoRNaLuJ4xKUcWQGpwuAcVwokHoFlQMqcGpFlC6R1MvtVoNtm0PtT6KERWR8AmlSkwxpPamekAxnGgcugQVQ2pvKgVUEuMp5DhO+3BIg2JERUylmGJI6YfhNJjjx4/v+NrRo0cljEQPugQV7aRKQCU5njr5vs+ZKBWoElMMqd5UmoVKUzj1ih+VfnYaQkzFoOJsVG8qBFRa4gnYOruJbdvbzoCy53VEnDsPAJaWLq14N81Lp2xZXl4WcfPSyY4p1UJK1ImIe50/T3ZAJTGa4owjVSUtuGQHleyIUu3cebIDKi3xtG/fM7u+4sEwfMzNzfS8fDcpM1G+n2mH1Ozs7J6XT0JoyZ6Z4oyUGpIQUGkMpl6StksxfG7KiinORl0iM6CSFk+zs7Pw/cwQ17AQBIPPREnbndcZUnvZK7R0iqyFhQWGlESyZqF0jScG03CSEFYyY4ohJS+gkhJPvXrBNJtDhtTghEWUaRrI5234foAgABqNVmQ/u19kqRpXMmel0hxS4TnxRNMloBhM8dA1rM6cOSN9F1/ayAgoneNpcnIS9Xodvu8jl8shk+kfSruFlGka8P3OlU3GwGMQElFBECCbtdBoeGi1fOTzNjIZC81mBoYx2GzUKDrjSsWgkhVTaQ0pGefDUzWgGExy6RJWMkIqrbNRogNKt3gqFArI5/NwXRdBEMBxHBiGAcdxBv4Z4d6v7phyHBOWZaBaHX5yR9hMVBAECIKto4DW6y1kszYcx0SzOfhuvXGoHFQyYiptISVjN54qAcVg0oOqYcWQip/IgNIhnmZmZmAYBlqtFprNJkzTbM8yDRNN/XTPSjUaHnI5C4WCg1rNxTAftxMSUZubm3DdFgwDME0fvm+j2fRQKNhotfyh1kdFoXv3nypRJTqm0hZSIskOKIZTMnQ+jjKDirv24pP2gNptzbNt27DteDKlO6TqdQ+ZDFAoONjcdAf+OUIiyrIsBEEA27YAuAgCD75vwPMCbO17DISHVCfVZqlExlQaQkrkLBTjieISPrayYkr0gvM0zEaJCiiV4mmQT+SL0h1SzaaHVmvw8+YBgo4T5fs+lpcrsCwThuHDtk0AFizL2FF8skKqFxWCChATU6JDStRxokSug5IZUIyn9JE5MyUqpERGlOjjRIkIKFXiSaVw6qXXgvP5+YmBriskooIgwMWLFYQHsTJNG6ZpwXV3HlpdpYjqJjuq4o4pkSGVtIiSFVCMJ5IVU0kLKZERFXdAyY4n1aOpl86QMk0Ds7Olga4ncCZqA4bhwTAc+H748cFwd952KodUSGZQxRlTokJKREQlOaCSHE8nTpyI7WcfOXIktp8tm4yYSlJIiYqoOANKZjzpGE7dfD8DwwCyWQsTE4WBriNkTVStVoNhBLCs7I9WvYczUFvroS79eYvM9VGDkrmOKs4DdqZhjVSURAeUbvG0tLQEx3GwsbEx1Ek94zJKoOkSXjLWTHHB+XBkn8olakkIp94GP06UkJkoz/OwslL90d86Z598GIYHwPjRYda3H2pd9ZDqJjqmdJ6RinsmKu5ZKMbTdhcuXECtVgOwFfmbm5vY3NxEqVSCbdtYX1+HoNN0CqN6XImemYo7puKejYp7JipJa6CSEE97HcFcqTVRrutibW0TW5EUzj4FMAwPQWBtDcTwEAQ2OgtwaiqLcrnc4yeqiyE1mDgjKkkBpWo8ra2toVKpoNVqYXZ2Fo1GA5VKBQBQLBaRz+cBbH0yd2Njox1YSRZlVHmeB9d14bouJiYGezHvR2RM6RxScUZUkj6Fp1NAhb876+uNoa+rVERtLSzfCG/yR7NPwY++t3XgLMNo/SioDExNZQf6uSoHlsiY0jGkdI0oUQGlWjx17gYzDANTU1Ptg+E5joNKpYJsNgvXdVGv15HJZOA4DhqNBlqt6E7xpJNRo8p1XWxsbLRn7jzPw/79+8cej6iYijOkdIyopBwHSuV4GvSNxjAxpVREAcDSUhg8PgzDRxBYHTEVADARBPbAAbUbVeKKIdVfXBGle0CpEk97rR2yLAsTExMwDAO1Wg31el3MwDQ2SFS5rov19XXk83lks1nYto2lpSVMTU3tel6wYYiIKR1DKo6IYkBFa9xZWWDwkBo0ooSd9uWSSwvJg8BsBxVgRhJQwPY7WmZQhU882YdGSAudA0qFeBpm0bXneVhbW4ttLEkU3r/9YioIAmxubiKXyyGXy7UPUhz1gnwRC9C54DwZVIinKMKp09RUdqTde/2MFFGu6+I3f/M3cebMGZimiQ9/+MND/EJuHbXcMFoAgvaC8qmpLIIggGEMvip+EOEDIDum4g6pOD+xl2ZxxpPscIrzUALU34kTJ3qGlO/7aDabKBaL7YA6f/48HMeJbBaqU9wxFdcRznU5krnOn8STHU9Rh1O3KEPK3PsiO/31X/81Wq0WvvjFL+KOO+7AJz7xiaGuHwQ2gsDYMQMVdUB1mpiYaP8nw+zsbOxPzLimc3V4MYhjFiqugDp+/LiUgDpx4sS2/0iefvd/EARoNptotVrtgJqbm2t/33VdNJvRfmo57udjHL9HIs9EkDYyA0rkNjqqPV8jzUQ9+9nPhud58H0flUplhBMEGtiakYruHzIMmbv74t7FxxmpaMQZUCIxlvRhWRb27duHlZUVNBoN5HI57Nu3r/39er2OxcVF5HI5+L6PZzzjGZHefpwzU9y9pz5Z8SRrYgOIZkZqpIXl586dw9ve9jZUq1Wsrq7ivvvuw1VXXbXrdS4tLN8iI572Ijqo4tzFF0dIRbnAPMqF5VG/K01CQDGe9NBrt14QBO1PNDqO0/57uVxGNptFsVjExYsXkc/nUSwWYxlXXLv4og6pKHfrRbmwXMbs/Th7IkQHlMxw6qc7pmJdWP5Hf/RHuPrqq/HOd74T586dw1ve8hZ87WtfQzbbP4w6BxT1dLSuRKyVIiJ19VofZRgGTNPE5uYmLMtCpVJBuVyGYRjwfR+FQgHNZrN9LC6icche/6SKqansSGsPR1oTNTk52S7JqakptFoteJ6363WWlspYWiorG1AyFp7rFFBpOhVMEnY7qH40bdpd9yEk5ubm8KxnPQvNZhMXLlxoz0jFIY2zUGkmYzukymGIujWbzXarDGqkiLr11lvx/e9/H8eOHcNb3vIW/Nqv/RoKhb1P1qfqLrykBVTa1kTp8mIq+jQcR44cYUxpoN+uV8Mw2gfcDA9zkM1mMTU1xdkDxYl+0znuh4pkhZSKMTVspwg72KaKM1CyHsC4n7Cqr4cCoj/Ypm6fzhPt6NGjeOSRR4TfLg1mt9htNBpYWVlpr4+anJxENhvPIWF0mYUCon/zFPXBNnU80GYaF5f3M+iuvZFmooalWkDJLGAdAyqt4tqtJ3pGCtgKt2uvvRbXXnut8Numve32QYBsNou5uTkUi8VtRy6PMqCOHj2qVUBRPGQtMVF1VmoQQiJKFbIfKJ3WQOlGl116oTg3Wv2EM2CMKfXstdvVcRzk83lkMpltu/miEOfzMK6A0u33XScyt1Oyt9GjSE1EyX5gRDwx45qFStOi8m5xv4uWFVIAY0oFo65bi2oWSsasaFro/Lop+w2/7O31MBIfUSqUrewnJI0nySEFMKZEC8NJ9qL/uJ933I2nN9nbLRW23YNIdESp8ACIeiJyLZTeU/yyQwq4FFOyN+5JpUI4hXQOKJ1/z3UjO6QANbbju0lkRKlSsCo8ASkaIt5VqxBS4Tg4OxUd1eKJu/BoGCpsx1TZpvci5BAHIj6dp9odLPKJF+csVFz79aM+xEEo7hOTxnXYg24iD4MwyEaVh0cYnirhFBIVT3G/4YhrJirqQxwAYg5zENeJ5zupeFyyuA+LoNQhDuIQlqmKhZqUgCJ5RM4WDBJsnJkanEozTyEGlBw6Ly7vpMJsVDdVGkCbiFLlDiO1xf0iK3KxrGohBTCmdqNiPAH8BB5FQ8WQ6iSrEZSOKB2jKUmzUEl5FxW1pIbUMFQNBhlUvi9EPn/4abzekvQ6qnpIdRLVD3asP31IOsVSLzo9wUgfR48eFbJG6vjx40NvdMN42O2I20mkajR1YkClw+LiopB1UaHl5WUl10jtprstolxPJXVhue7R1El0QIlYCxXnO6i4FpaH4l5gDohbZN5JREwNuvHdbSxJjCodwgmQM3spIqLi3lUfx8LyUFIWmHfSLaR20yuqBl1YLnwmKknhFEriDFSSpqDjcuDAAeEhJWJWapQZqW57BYeKkaVLJO0mqQGlu5MnTwo9IbEIOs5I9dPZJcPOUgmJqCSGEyAvnviJvHTTJaR2s1uwxBlYSQilfhhQ6SZ6tx5waRuYlJgCLvXKoP8mpdZE6SKJM09Jc/DgQSG79GTMRgFqhFRcYxhnFivJkbQbVT+AEBXdDm2QNkmMqUEJWROVlOhQ4d+h+1qoUNxrogAx66JCMkIKkL9GSuRBQak3WQElchZKRETFuSYqlMS1Ud2SElKD/juUPsSBStISUDQaWbs1RJzGg6GkJpmncOFuPHXJ3k4sLy8rsb0UhRG1B1WeEKJ+MS6//Aoht0PRYkilS9J333XirrzhyQ4pQJ1tZ9wYUX2o9AQQGVAPP/w1IbclgugXX9nvzmWEVJo25qqQfZ/Lfp7rTOSnnlUIKUCtbWkcGFFdVHvAOQOlF9kbGNkbWIqX7MdX9vObhqNKSAHqbVujwoj6ERUfYNG/AN/4xv8WensUjzg3tNytJ4/sgCI9qRRSgJrb2nGkPqJUfUBFPvEvv/wKLC2dw+TkJBzHEXa7IshYT6HCu/U4Fx13hxQ37vGSuYC8k4zndRLXQ8k4kLFqIQWou+0dVmojSuUHUHRAAYBlWXAcB/l8Ho7jwDRT+9SIhAohBcQXOJyREkOFeALUeT5T8qi8LR5E6g62qfqDJTqggiCAYRj413/9Z+Tzefi+j8nJSRiGgeXlZQg4jBjFrHNDPEz8GIax7fG3LAue5237Waps5JOE9ynFQcYRzYeh6wE7UzPdoHvtRi2cgTIMAwDQarVQq9VQr9eRyWQQBMHAJ2BUnaxdAiq+ew93DQ2yoZ6ZmcG+ffswNTWFQqGAmZmZHbt7OSMVjWEeF9FkPY+TuCsvJOvcpCru1uum27Y68TNROj0YMj+J53kepqenYRgGNjY2YBgGcrkcms0mZ6PGIOu0MIPYbYbKMAyYpolsNotyuYx8Po9CoQDf9xEEAVZWVrZdV8RpaJJGxWDqpuIbARqP6jNSIV1OcJzoiGJA7dQroMJjQ7VaLTQaDVSrVRiGsWN3DiVXuEEPQygIAly8eLH9IlapVJDJZFCpVGBZ1o7rM6AGp0M8UfxOnjwp5DQwvegUUoDau/gSF1E6hVNIZEBVKhuoVsuwLBu5XAHF4kT7++vr6/B9H8DWRpQBFQ2VZ6O6dc9Ora6uolQqYXp6GvV6HY1GQ+Lo9KVjOMmchUryrjxV6BJSwPbtumpBpdUJiHUMpH5E75u+/PIrUKtVsbR0FjMz8/B9H8vLF3D+/DlUKhUAgGmasCwLruvGPh4RJyDuJvKExL3oElK9VCoVlMtlVKtV2UPRho7hFJK9G09GRIk4AXEvsmajuukSVIOIIrQG/RlSIipJMTQomQv6wl145fI6qtUy9u8/iIcf/hocx8Hs7CwqlQoqlQqKxSJc10Wz2Yx9TDIiChAfUt0bg9OnT2sdU924G0/vWOomO54AebNQsiIqpEpMAckKqmF0hpNSEfXUU0/FfRNKUuGTEJ1roL71rb9CqVRCuVxGq9UCAGQyGRSLRayurgodl6yIAqIPqShe9B9//PEIRqI3kUGWpPAZlQrB1E3mbjzZEdVJpaAC0hlVl1122UCXY0RFSIVo6hQGVOdJhaenpwFs7Z5ptVowDANzc3NYXl5ur4cSRWZIAYPHlIwX9qTNWJFcKgZTJ9lroFQKqE6qxVQoDVHFiIqRarHUT69jkczPzwMAfN+H67pwHAee52FtbU3w6ORHFNA/pGS/qPfCsKJBqB5M3VT4XVM1okKqxlS3JMUVI2oMukRSP/0O5DY7Owvf97G6uopsNtteSB4uLBdNhYjqpPLMVC/heBlW6aVbMAHq/P50Uj2iQrrEVD86RRYjahe6R1Ivex0Bd2Zmpn0qF1WoFlHdhl07JXvj0DlehlXy6BhMIdm/G7vRJaA66R5T/agUWamNqCQG0m4GOX2AZVkoFovY2NgQMKLhqB5SoXEWo8vcgOw2boaWOnQOpF5UjqZuOkZUp6QGVS8iI0ubiEpb9ERF1rmXoqZLRHWL6hN+ojc2w447CIL2Bw7Onz8fx5ASKWlRNAidwqmT7hEVSlNMRalfmCkVUY8++mjcN5F4SYmmbrpGVKc4jj0laoMk6wCkKs6CpTF8xqFrNHVLSkR1Y1SN5+qrrx7ocowoxSQ1lnaThJDqFGeYiNhwyT6yO6krKeEUSmpA7YZxNZhBIypx587TQRpDKU06NzRRB4mIQzL0Oso6pVfSwintdtv+MLCGx4iKEWOJRAXJbj933I1gr+szrJKL0ZRe/bZZjKv+GFFjYijRMAbZQOkwezU1NQXDMFAsFmEYBqNKEwykS9K4K29UnL3qL/URxQiS79ChQ4lbFzWOQTd044bLOHHl+z4cx2n/3bZtzM/Pw7KsSMZGw2EckSzjbkN1jzClI4qBQ9TfMBvOYaKm+7K9bscwDHR+JqX7zwcPHsTm5iaq1SpKpRLy+TzDakgMI0qDuLfzcUeakIhiDBHJFfUGOZPJoFqtolgsotVqwbIsmKYJYCuwKpUKXNfFvn37YNt2zzHoHFUMHCI9xN0fSs9EUXpwl55ecrkcarUaFhcXEQQBpqenUavVEAQBMpkM1tbW4DgOyuUygiBAoVBALpfb9jMYIiQL10NRVBhRRDSS6elpAFu770zTbB/Z3DAMLCwswDCM9tfCtVJEREnCiCKikRiGse3/4e48YGt3HxFR0pl7X4RIDE6xE1Hc+DpDUWJEEREREY2AEUVEREQ0AiFrovipq2TgNDgR0d64zUsPLiyngYUvDHHGFEONiHTFeEofRhQNTURMERHpgvGUXowoGhljiojSjPFEjCgaW+cLCYOKiJKM4USdGFEUKc5OEVESMZ6oF0YUxYIxRURJwHii3YwcUffffz++8Y1vwHVdvPGNb8TrXve6vpcVcbZ2nsxUTYwpItIR40ltIrpiECNF1OOPP47vfve7+JM/+RPUajV8+tOfjnpcQ4vqDmWMxYMxlQyHDx+WPQSlnTx5UvYQaEyMp3ipEj9RGSmiHn30UVx22WW44447UKlU8O53vzvqcUkzzgPMANsbY0ocBo94cd3njLN4MZyGk7QQGsdIEbW6uoqzZ8/ivvvuw+nTp3H77bfjr/7qr9pnc0+rXk8shlVvp06dYkgNiVGUXsM+9owuigqDaXcjRdT09DSe85znIJPJ4DnPeQ6y2SxWVlYwOzsb9fi0x7CiYTCUKAq7PY8YWNQPg2l4I0XUi170Inzuc5/DW9/6ViwuLqJWq2F6ejrioSVX9xOVUZU+jCWSpd9zj3GVPoym8Y0UUa985Svxne98BzfddBOCIMDdd98Ny7KiHltqcLYqHRhOpLLO5yeDKnkYTPEY+RAHSVpMriLOViUDw4l0FD5vGVP6YjSJwYNtaoJRpRfGEyUBZ6f0wWiSQ0hEnTlzZqTrHThwIOKRJMfp06cZUophOFGSMajUxYDqb9T+GJTSM1Hj/OPTEGAMKTUwnihtuLtPHWkJqLhjaFRKR9Q4+t3hSYur8BeIMSUWw4mIs1MyJTWeVI2lfhIbUf0kNa44KyUG44moN85OiZOEgNItlvpJXUT10+sB1S2sGFLxYDgRDY6zU/HSLaCSEkv9MKJ2oeOsFUMqOownovFwdipaqgdU0oOpF0bUCFSftWJIjY7hRElnmiZ839/xdcuyYNs2Go1G5LfJ2anxqRZQaQymXhhREel8QqkQVFxwPhzGE6VFLpdDNptFvV5HrVYDAOTzedi2DcMwkM/nUalU0Gq1Yrl9zk4NR6V4YjjtZMoeQBKdOXNGmSebSr+AqmJAUZpUq1WUy2U4joN9+/ZhYmICjuOgVqthY2MDjUYDphn/poG/d3tT5fVbpW2aahhRMVLliafKL6KKVH4hX1hYkD0ESqhWq4WNjQ2sr6/Dsiw0Go1tM0+O4wgZh8q/f7Kp8LqtyjZMpOPHj+P48eMDX94IgiCIcTxtS0tlvOpVVw98+aNHj8Y4Gnlk7upTbdfeoUOHpN6+ai/gvaJpcXFRwkgoLQzDwMTEBDY3N+F5HgBgZmYG5XIZruu2L2fbNjKZDGzbRrVajXxXn+xde6dOnZJ6+91kBlRSo2mYMAKAJ554YqDLCV0T9fWvPzpwSA36D9YttsInqIyY4oLzS1QIqL1mmhhQJILv+wjfS2ezWfi+vyOgisUi6vU6ms0misUiqtXqtstQdGQFlK7xNGwcRU3ITNSFCxdgmoX234eZkRqHDoEla2ZKhZiSORMlM6IG2U3HgCJRJiYmYBgGWq0WbNtuxxKwNVNVLBZhWRZ830ez2YRpmmg2m+2Zq6jInI1SYSaK8dSbrEgadCZKSES94Q1vwOWXPx//7b/9GoIggGEYwkKqk+pRJTqoZIeUrIiSFVDDrHFiRJFI2WwWtm2jVqttO/yBZVmYmJhAvV5HvV7HxMQEarVabJ/ckxVSsiNKdEAxnHoLggC1Wg31en3g54SQ3XmFQgH/83/eh/n5Bdx885sQBMFQu/ai0v3AqBZVonf1pXH3HgOKaKdGo9E+PpRt23AcB/V6HaZpwjAM1Ot1AFvHmIrzk3uHDx+Wvj5KNJEBpWo8yd4lB2wF1MbGBnzfRyaTGfh6whaWv/71N8O2Hfz+738cAJDN5gCI27U3CNWiSlRMyQop0TNRMgJq2E/YMaBIBeEBOW3bRqFQwMbGBmzbRj6fR71ej309lOiQkjUTJSqgVIsnFaKp2+bmJprNJgqFArLZrFq78z7/+c/jQx/6EI4efS4OHDiIK654IV7zmpuUDKlOKkVV3EElI6SSHlEMqGQplUqYnJzE2bNnZQ9FqFKpBMdx4Ps+Go1Ge1YqTmmIqLgDSqVwUjGaOvm+j7W1NeRyOeTzeRiGodan8z7wgQ/gWc86iM9//kv4X//rIfzrv36/HVAqU2n3X9y7+pJ+hHORAcXjOyWLaZo4cuQIisUiKpVKz8vMzMxgZmYGtVoN58+fj3zRtUyVSgWGYcA0TWH/riTv1ktDPKkeTd3CT6Tu27cPhmGgWq0OfF0hEfXGN74RnmfA81rwfR//+I9/j7/927/BC15wJYrFopT1UaNQIapExFTSQkqHgOIslHyGYeDAgQMoFAqoVqtoNpvt4yWtrq5ic3MTtr3zJXNychILCws4deoUpqamsLCwgHPnzkn4F8QnCALhYZjEkIozoGTGk27R1C1c67eysoJcLoeVlZWBrytkd96jjz6Kd7zj1/HsZz8b//zPT+Cnfuqncc01r8Q///N38Xu/9/H25XQIqX5kBFWcu/hEhJSI3XmiAmqc2ScGlDocx0GxWEQ2m0WxWESj0cDZs2dhGAb2798P3/dx4cKF9uVt28b8/DyCIMD58+cxMTGBZz7zmXjqqack/iuSRURIididl7SA0j2cugVBgNXVVWQyGZimiX//938f6HpCTvty9dVX47d/+yM4d+4sXvjCF+LKK1+IY8duwZEjz8af/Mnn25f7+tcfFTGcWAx7qPgoxPmLo8IpB3TB3XfJ4bou1tbWcOHCBfzHf/wHzpw5gyAIYJomHMfZdvwkYCuiLMtqn8g3PAddrxkrSq8kBZSMbZ0IhmHgn/7pX/EXf/Ew3vOeuwa+npDf9D/+4z/GZz7zR3jta1+PN7/5Vnzzm1/Hxz/+UTz/+T+N5zznx7dd9sSJE3v+vCNHjsQz0AiETy5RM1Nnzpzhrr0+4p6FiiKeOAulB9M02+eYA9A+wncYWOFxkxzHQavV2nasJRqP7rv1khJQOoTTIP3wwx/u/njMzc3jWc8afJsq7DhRV1zxQrz5zbcCAGq1GkqlCVxzzSu3HXPkx398sA32bneUKoElMqbiDilAvwXnDCiKkmVZcByn/dH+fD6PVquFRqOBYrHYfiynpqawtLTEiIqYjiGVlAXkqsWT67rtf/vk5CQAYGNjY+Dr//iPH9wzpH7mZ1468M8TsiaqWq3illvegptuegM8z8OpUyfxEz/xk7j66le0LzNoQA1LlagSEVO6rZGKa00UA4ri0BlR+/btg+/7WF9fx9zcHGZmZtqzUU8//XSiPp2nkrhCKo41UbrPQKkQTydOnMCBAwe2feL1Wc96FtbW1lCtVjE3N4fNzc327vRh7BVS8/MTA/0cYQfb/O53v48vfOGzKJUm8NznXoaf+ZmXYmZmNrZ46kd2VMUdU3GFFCMq2rVPjKhkyefzyGazqFQqsZ0ShfSJqLgCKg3xtLS0BNu2sbGxgSAIMD8/j3K5jGazCd/3kc1mUSqVkM1mUavVsLGxMfKblt1CSrmIWloqo16vI5fLtc+fJzqguskMKh1jKuqQiiOiGFBEyRZHSOkQUXEHlMx46lyiY5ompqam2n/2PA+VSgW5XA6NRgOu68IwDESZLr1iatCIEvLpvFA2m23/WXZAAVsP3CAL0eIgu/aTKI6AWlhYYEARKUTW+S9lijugZBwiIdz+dm+Dfd9vn8Ou2Wy2Z3crlUp7d3rUcz/j9IjQiAo/Fvzc54o93QcREZGu4j7tV71eb2+fRdltT5DneVhfX0e5XBa2e3zUkBIaUYAaM1CdZO3SU+m8fCQGZ6GISEVHjx7FxMRgu6+iJHuNcrdR+kRoRDGgKK0YUESksvAYaKKpth0etlOERRQD6hIdF5WnFeOHiFQQ9+u6zGMB6hxSQiIqXGmvCtUeMEo2hhhRdHQ76CYNRtftsvA1UbLJfqC4Fio+cb24jhNBDCgi0oXs7ZPs7fMoUhVRsh8gnY9arttpX1TAgCJKp7heL9OwVEP2dnpYqYko3R4YIiIi0WTPRgF6ba9TEVEqPCAqPDFpdMPOKnEWioh0pcL2SoXt9iASH1G6PBBRSMNUrw4YUEQUlzS9zuuw/U50RKnyAKhQ9UREuuMn88RRZbulyna8n8RGlOp3PMUjzhfZQWaYOAtFRJQeiYwolQJKVM2naYpXVQwoIhJB1Ou9SrNRKm3XO9myBxAl1e5kVZ6A4+LhDYiISLZwG3/ixAmp4+ik/UxUWKiqBRQlU7/ZJs5CEVGnpLz5VHEyQKVtvrYzUarcgf2IfOJxV55cDCgiSrKjR4/i+PHjsoexQ2cHyJqd0m4mSqUCpXTqjCYGFJEY/GTednzzvJ2sNtAmonSKJxWnP9OEL7ZERNHSZbsmuhWU3p2nSzTJFPe7kaTs148DZ6GIiNQkahG6kjNROs06ddOl1mk8DCgi2kvcb0JF79LTcfsW94fPlJqJ0jWcQjo+wYiIiNIgjtkpJWaidJ55kokLC4mISKQkTBZE2RzSZqKSFk1JeGIREamIHxbp78CBAzhz5ozQ21T1kAfDimJmSvhMFGedxnf//Z/lLNQe+KJLRESDGGfdlJCZqKRHk8hZqPvv/yw+9KH3CbktfjKPiGg8Bw8exOnTp2UPI3JJmY3qNmyvKLEmSmeiA4qIiKiTrD0TXMbCiBqLyCfQ7be/HSsrywCAIAiE3S4RqcEwDExNTSGXy8keCikql8thenoaMzMzsG0xS57THlJKHeJAJyKfOO9//2/j85//DK688ip85zt/C9M0EQQBDMMQNgYikiebzWL//v1wXRelUgn1eh3Ly8uyh0UKOXDgADY2NlCv1wEA09PTWF5eFvKmO6m79gbBmagRiN6FNz29D1NT02i1WqhWqwiCgLNRRCmSy+XgeR7OnTuH5eVlTExMyB4SKahWq6Fer6Ner8N1XaG3ndYZqbEianl5Ga94xStSVaAy1kB5nodsNou///vH4LouLl682P4FiSumkrConJ/QI92USiWUSqUdX9/c3ITjOCiVSpibm0O1WoXjOBJGKB5/jwfXarUAbD2PHMcR/mY7jSE1ckS5rou77747VfvnZQRUEATIZLL4sR87Asuy4LouDMOA7/sAwF16RAlgmiYOHjyIyclJFAoFLCwsbPvdbrVaWF9fx/z8POr1OlqtFvbv35+akNKdqDelBw4cQD6fR7FYxOrqavvrhmHANMXseEpbSI18r370ox/FzTffjIWFhSjHoyyRT4zf/M27cfbspYOnFQoFfO1rf4aVlRVMTEygWCyi0WjA8zxhYyKi+Ni2Ddd1cfbsWSwuLsK2bUxPT++4XL1ex8WLF7GyssJd+tTT5OQkVldXt20f5ubmMD09jdnZWSFjSFNIjbSw/KGHHsLMzAxe/vKX4w//8A+jHpNyRD4hfvInfxJnz57AN77xDbz97W/HJz7xCfi+j3w+j2w2C8dx4HkeMpkMLMsSNi5d9dsVcPjwYcEjIeovnFEyTRO+72N9fR0LCwvbZhMajQZKpRJmZmaQz+fh+37i30hxV97wbNtGs9nc9rXV1VW0Wi2USiVMT09jbW0t9nGkZbG5EYzwduZNb3oTDMOAYRj4wQ9+gCNHjuBTn/oU5ufne17+iiuuGHugIsmo6P/+3z+Jz372/8Mtt7wV/+N//D42NzdRqVQwNzcHy7Lan8aL+1N5ItdCHTp0KNaff911N+Lhh7828vUZWiTK9PQ0SqUSLly40F7v+GM/9mM4d+4cms0mbNuG7/vIZDLtT+dVKhXJo46XyIA6deqUkNsRfdDN7tPBFAoF2LaNjY0NoeMI6RRVTzzxxECXGymiOt1yyy344Ac/uGt4qBpRqkw5HjhwAEEQYGNjA5lMBrlcDoZhYGNjA41GA/Pz86jVarBtO9Y1EKIXk4uIKABjhdReGFo0qkwmg+npaRSLRbiuiyAIsLm52Z4lWFhYwNraGoIgwMTEBDY2NtoLh5MuiQHVSdYRzKvVavsTfCpRMa4GjajUHCdKlWDqFh5p1jAMOI4D13XbsRTu2240Gu3vx0HWJ/FOnToVW0iFARX+Oa6QGuTFnqFF3TKZDBYWFtBoNHD69Gm4rtuOqvn5edi2DdM027tlVlZWJI9YjLTsvgtfc0XHVKFQQKFQALBzlkqmXttnFcOql7FnogYhYiZK1UjaTfeh+oMgQLlchmEYyGQysG0by8vLsR19VoXDGMQRUZ0B1S3OWalxMLQI2FoTNTMzg2aziXK5nKrF47ICSsZMVDfZ59ZTKagGFXdkCdudN4hhIkrHGBrGXuc4Cqf0m81me0F5sViMdAwqxFNIdEQB6obUXhhalFQyZ6BUiKiQ7JgC9AyqQQ0TXkpF1Gte85q4b0JZo5wYMnxIPM+LdAZKpXgKRR1RewVUSNeQ2gtDi3QjexeeShEVUiGmgGQH1V4eeuihgS6nfURZlgXTNNFqtbZNfZumicnJyfYhAUQuypR1Ru1+VIynTlGF1KABFUpqSO2FoUUqkB1PgJoBFVIlpDqlKaoGjSitF5YXi8X2EdMbjQaq1Wr7SN6Tk5PwPA9ra2solUrI5/PY3Nxsfz9qqoVTSPWAkknEp/dUxMXwJIsK4aQLWYvPd9O5nUtTUO1G24gyTROO42BjYwOu62J6ehq5XA7VahXA1kc5wwPR+b4P27bHXqSpaij1krZ4GnYWqt910xZU/TC0KAqMpvGpGFPA7tvDNAWWthEVnj8unFnyPG/bIQDCjwabpolsNotGo7FnROkUSf2kLZ6A8QJqr5/FqOqPoUW9MJzioWpM9bLXtjRJkaVtRHUfubvXImzTNDE9PQ3XdVGtVhMRSf3oHE9xHi9qXJylGg9DKx10DCeV10PtRqeY6idJkaV1RAFbC8uf8YxntL8eHkisUz6fx+TkpLCxiaRzPEUhylmoYW6LQRUdhpZ+dIympElCTPUz6ISHCrElJaKinBEqlUqR/SydpD2eALEBtddtM6rixdCSj+GkpiTH1F6i3rs0SpQJiagk70YThdGktuuuuxHz8xNYWiozqCRhaEWP4aSP7m1EGqNqXKO0ira785IqrbE07LoombNQe+FuP3UxtHaXlmjSdT3UMPptSxhX0WJESZLWWIqCygHVjbv99JPk0EpLJFF/jKtoMaJixliiToMEIENLfaqFFuOIxsW4Gg0jKgIMJXF0moUaFUMrGaIILcYRybbb9o2BxYhqYwjJt9e6qDQE1KDSfqLlpGAkiZeG9VCijLPdTEqAaRlRDB6iwcQZngy0S+K6n3kfU1LFsR2XEWZCIorRQ+PiLJR6VHhMwsNKJBV37RINTkZraDkTRemiwsaaSFUMLSJ5GFFERAnH0NrC9VAUNUYUKaV7cTlnoYjEYGgRDY8RRcpiQBGppfP0Rv0wtChNGFFERBSZ8M3PmTMn8fTT/7Hte9lsFpdf/lP427/9loSREUWPEUXKCdctfPrT90oeCRFFaXZ2Fj/4wffRbDZhGAb279+P8+fP77jczMwMgiDA6uqqhFESDU5IRA1zYllSDxdjElEUms0mcrlc+/+NRmPHZebm5tBoNFAu633oCm730oEzUbSnXi8GDCsiGlalUsHMzAwWFhYQBAGWl5cBAKVSCa1WC4ZhIJfLtf8PAOvr62g2mzKHPRBGUzoxomgk3S8YjCoi2ktnOHWqVCrtP+tyOhBGEwGMKIoIo4qIkozRRL0woigWjCoi0hmjiQbBiCIhGFVEpDJGE43ClD0AIiIiIh1xJoqIxqLLQuCoyThjPBGpRUhEHT58eKzrnzx5MqKREBGQ3vCJUtT3IaOMKFrjtscgtJiJiuOOYJhRkjCK9DfsY8jooiQRETxx0CKi4jDKA8bwIlkYSdRtr+cEI4tk0TWIRpHaiBrFIE8MhhaNgpFEUWNkURzSFEiDYERFrNcTjGFFAEOJ1LLb85GBRQCDaRCMKAEYVunFcCIdhc9bxlR6MJhGw4iShGGVbIwnSoLO5zGDKhkYS9FiRCmk35ObcaUHhpNYZ86caf/5wIEDEkeSDgwq/TCY4seI0kD3LwKjSh0Mpy2dQZOW209zuDGo1MRoEs8IgiCI+0aCIMDFi5We31tdPbfrdRcXF+MYkvZ0Dymdz52XpnCSHUdJkKbY0jmodD53HuOpt4WFhZGve9lllw10OSERBQBLS+W+39srpEaVhgDTNaZ0i6ikhRPjSD1Jiy3dgkrXiEpDQI0TQ6NSKqLK5TLW1jZhmhYymVzPy8QVUr0kLa50DCkdIkrncGIkJYvugaVDUOkYUUkLKBmx1I9SEbW0tIR6vQXXbSCXK8Jx5IdUP7oGlm4hpWpE6RpOjKb00TGsVI4pnSJK53hSKZR2M2hECVlYPjMzg5WVKgzDRKvl9o0oFfR7gFWPq/CXSreYUoVu8cRoou7ngA5RxQXp49MhoHQJpSgIiSjLsgAAvu/BNK2+l9u375lKzEb1stuTQqXAOnz4MENqCLrEE6OJ9qJbVPGAnsNTLaDSFEv9CFtYfvLkGbhuE6XSvl1DClBjt96oVAoqlWNKhd15KgcUo4mipnJUqRBSKu/OUyme0hJOSq2JunDhAiqVKkqlGZimiSAIYBjGrtfROaQAdWJK1ZCSHVGqBRSjCTh+/Liw2zp69Kiw21KValElO6RUjShVAiot8RRSKqJWVlbQatmwLANBAIS36PtVAFu7+3pF1fLyctxDE0J2UKkYUjIjSoWASlo0iQwgVSQtxFSIKpkhpWJEyQ6otIXT7Oxszz/vRtjBNsvlGizLBBCgXm+gVquh0ajBcWx4nofJycme101KSAGMqU6yIkpmQOkeTmkMpVHpHlgyg0pWSKkUUTLjKWnhNGgMjXo9YceJajRcBIEDz3PRbFbgeT5M00ahMIVabRWmaSKfz499W7pEl6ygUiWkZESUrIDSLZ4YS/HRLa5kxFTaI0pWQOkST6NGUVy3IySifN/H2to6gsBEubwOwzAxPT2FarWGRqMBy3Jg2wEymUwst696WIkOKhVCKi0RpWpAMZTUo2pgpSWkVIgo0QGlejiJCqZxblvYTJTreggCHxsb6ygW98G2bZimh7W1MvL5EgzDhGW5cQ8FgLpRpWJM5XI51Ov1yG9bdESJDihV4omxNJogCLC+vo5Wq4VMJtN3uYEIqsSV6JgSHVIyI4rxtGXQcAkPmxQEAXzflzoWIRFVr9exuenCsgysrCwhny8AsOF5dQAWstli+7Km2Yx7OD2pFlaigmqvkJqfn8fGxgYajUaktysyokQGlArxxHAaTxAEqFQqaDQaKBQKqNVqKBQK7eUGQRBgc3MT5XK5HViO4wgZmwpBJTKmRIaUrIgSFVCqhdOos0ymabb/7Ps+LMuC53lRDatNqYjyfR/Ly5uwLAOeV0e1WgUQIJPJwzAcWNalY37KiqhuKkVVnEF1+eVX4OGHv9b3+6VSCdlsNvL7Q1REiQoo2fHEcIqO7/tYXV1th9Pm5iY2NzfbG6FarYbV1VXMz8+jWq2i1WphdnZ2oEO3RElmUCUxpERHlIh4Ui2cgPF30YWHSQp/38KIijplBh2nkCOWhy8snteCYZgoFCZhGCaCYOcLju9nlAip7jtQZlSFvwhxxNSTTz6B6667EQB6xlSlUkGpVIJhGJE/SeOW9IBKezidOHGi/ecjR45E9nM9z4NhGLDtrZfHVqu1LY5830cmk4HjOHAc50dvCsXrfPxFB1X4nBcRU6dPn5Z+DKmoxR1QSYynUOe2KNydJ3P7JGQmqtVqYW2tDMMIADjY2oUZlqTZ8zoqhNReZIVV1DF1+eVXtP/cK6Smp6dhWRZarRZarRZ830etVhvrNuOeiRIRUDLiKQnh1Bk/quiMsEZj6xAspVIJtm1jeXkZlmVhenoawNbyhNXVVXieB9u2USqVUCqVhM9E9SNjhkpETMUdUqJmouIMqKTFU684Co8r2Wq1tn0t6l16se7Oc10X73vf+3DmzBk0m03cfvvtuPbaa/tevlqtolrd+hSe5wUdd4oBoPPP2+kQUiEZQRVlTPULqWw2i4mJCXieh2azCcMwUCgUsLa2hmZz9McnzoiKO6BEx5MO4aRiGEVpYWEBhUIBvu9jaWkJuVwO+Xwe5XIZuVwOxWJx7x8igeigijum4gypuCOK8TQcwzBgmmbPOLIsC77vt9+4GIYx8gLziYmJnl8f9GgBI0XUV77yFfzbv/0b3v/+92N1dRWvfvWr8a1vfavv5cM1UT2+A8NoATAQBCaA7efUm5rKolwuDzs8qUTHVFQh1RlRoYcf/hqmpqbQarVQq9Xa+6FLpRIcx8Hq6urItxdXRCUloFQNp6TH0qgOHTrU/sSQqkQFla4hFWdEpSmgotptl8lkYFkWGo1Gz0DqXGAOYM+I6hdLu93+IEaKqM3NzfbGdHV1FTfddBMeeeSRvpf3PA8rK2VszTZtHbUcCGAYHoLABmDAMNz2n4GtgBqUqqElKqiiCKleEVUur+GHP/wBLl68uO3rk5OTaDQaY31iL46IijOgRMSTauHEYIpOlGu2oiAiqOKMqThCKq6IYkCNxra3zmaSzWbHOszOsPEUGjSiRlpYHk5dVyoV3HnnnXjHO96x6+W33qFtj6fwz5d2510yTEAB/e8k2XEVPqHijqkoFp4/+eQTO0KqUChhZmYO6+vr8H0fxWIRhUIBjUZjrF15uklTQDGc4hHXIvhRHT9+PPaQOnPmjBLn45MproBKcjyFwjVPnufBcRy47tZxJE3ThGEYA62BGjWghjHyp/POnTuHO+64A8eOHcONN9645+Xn57cOVue6LhqNBkqlElzXxebmJgwDcJxc+9MwUZmYmJAeUsDWE0ylQyb0Mz+/8wnneVVcuHAO2Wz2RzOKK6kKKCIi6i2OeOo8hAGwFVOO48A0Tfi+P/DaJxEBBYwYURcvXsRtt92Gu+++Gz/7sz870HWWlsKY8WEYHmq1MrZmpQIEgYl8XsjRFmgXjz766I7ZqExmEuvr61oe4oCGd+TIEc5GxUiFWSiiccV1OpZwnVP3YQxc1x3qk6+iAgoYMaLuu+8+bGxs4N5778W9994LAHjggQeQy+UGuLb5oxX1W1NzQWBicnK43Xe60WEWaje2bSMIgliOCktERHqI81x2YUD1mmnqnJnai8iAAkaMqLvuugt33XXXGDdrtQ+0OTGRje3YKirsytNNr7VRhmEgk8mgWq0im82i1WrFes4ikouzUfHgLBTpSsSJgMfdnoiOp5DEfWjm0AvISQ7XdduzUOHxcbbWsm0d8Mx13faiPxrN0aNHlVlcTsmnwjn4SH0i4ikKsgIK2PrInBQMKHU9+eQTO74WvksIzxVWLpdhmiYmJiawb9++HcfsIKJLOAtFFA+ZAQVIiigRAZW2XXlxnqS4k+d5sCwLs7Oz8DwP1WoVGxsbXHSeMNzo07jSfniDOIl6vVed7IACJERU2magdF1U3ms2KpvNYnJyEr7vw7Is1Ot1VCoV1Ov1REcUNwZERGpRIaAAgRE1NZVNXUAljeu6aDabWFtbQ7lchuu6iY6ntONsVDRUux+5HooGpeIkwMTEhDIBBQiKKNEHZ0zbrjxRfN9HtVqF67qo1Wqyh5M43LgREfWnUjyFuBqY+uq1S4/SRbVZFN3w/iNKtsRFFGehiIiISITERZRKVNyfPCzORhFnU0bD+40oWipOkiQqolS8g4mGwXVRFKckPL8OHjwoewipotpkgGrb+cRElGp3bFKcPHlS9hBIAZxVGU7a7y8eFoTipNL2PjERlWY88NqWON+hcqNARETdEhFRKlUpUVKlfXZlULyfKBTnTH7a3zyrst3XPqJUuSO7qbYfmYjSLQnroYg6qbD91zqiVLgDiaLGjZ2+OAtFSaTypIDsDtA6oohILEYCEdEl2kaU7PokIurEwCSSQ2YPaBlRDCgieRgLRKQaWV2gXUQxoGg3STkQH9dFUZT4fCKKh3YRpQOVF+ERRYGzUdvx/riEx1RLJh22azImWbSKKM5C7ZT2Y4X0cvfdH4nl53LjQESkNtGdoE1EMaBoEEEQYHV1RfYwUiHNsy+meemlM833A8nFN9G9iewFbSKKaDdBEKDRaGBtbQ3f+c7f4ad+6grZQ6IEsm0b8/Pz2LdvHxzHkT2cgXA9FFF8tIgozkLRXqrVKur1OnK5HK688kV49atfh2uv/QXZwxqLDhu/NM3CGIaBmZkZlMtlVCoVmKaZqn8/kU5EdYPyEaVbQOmw+C5pms0marUacrkcstksFhb2wzBMTE9Pyx4aJYhlWQiCAL7vY3p6GpOTk7KHRCSUbts3Ef2gdETpFlAkh2EYCIIA2WwWpmni4sVF/NmffRmPPfZt/Oqv3il7eImXltmYVqsFAJiamsLFixdRLBYlj4iI9hJ3R9ix/nQiARzHgWEYWFtbQ6vVwoMPfhbPf/4LcMMN/xnZbFb28EhDhmFgamoKjUYDruu2A2pzcxNzc3MwDEPyCIlIBcrORHEWioYxMzODfD6PQqGAO+98F6699vp2QMV1yAMRdFgXBSRrNsq2bSwsLLT/PD8/3/5erVZDuVzW5qCuop8/PAwIqSjOnpA2E5W0SBK9rzjuj7aePHky1p8fNdM04XkestksPv3pe/GWt/xXmKaFer2Gb37z4chu58CBAzhz5kxkP4/UEz6X1tbWAADZbBZzc3O4ePHijw6hsYqpqSm5g0wpXeK1U/haevjw4Vh+/uLiYjv6RQi3dbOzs8JuMwp7NcfExMRIP9cIgiAY6ZpD0G0x2l5k/3uSEFCnTp2K/Ge6rosgCJDJZAAAb33r7fC8Fv7u7x7DuXNn8IMf/EtktyUqpMJ39uHtHT9+XMjtinTixAlpt52kGbSQrNlLEbNQcUbUoUOHYvvZobhCKiQypnrRLax2M+i/hRE1ABXGL+KgaiJnn+KIKAB48Yt/FidPnsDp00/Dtm38l//y/wAAzp8/iz/8w/838tuLM6Z6bZQ6by+JQUWjkbnbV/d4ComIqFDcMQXIDypA76hiRI1IpbGKPBqt6N13cUXU8553Bb75zYfxohe9BN/73j+gUCjg1a9+A374w6ewuVnBP/zD47HcbpQxNchGqdftMarSQ/ZaOZFrn5IWUCERIRVSIahCuoQVI2oAKo5L9GH8Za59ijqkbrvtbQCAL3/5C7jhhv+Mz33uAdRqNXieh6uuegl++qevxP33/0Gkt9ltnJgaZcPU7/YYVMkjO5wA8QvHRa2BkhFRobTGVCcVw4oRpeAYdiPjHEiyF4/HFVGPPvotVKtV/PzP34BPf/peAIDnebAsC2996+348IffH+nt9jNMUI27cRrkthhW+lAhmDrJ+NSdyEXkMiMKEBtSIVWDKiQ7rFIdUaoHU0jWySNlx1MorohyXRf/9E/fwYtf/FKYponPfOZT2y53+vTpSG93L7sFTtQbp1FnwhhYYqkWSf3IOmSB6E/hyY6okIyYAtQPqpDIsEpcROkSRnuRedZtVeIpFGVEhQEVct0mHGfrU3rhbFQn0SEF7AycuDZQcSx2Z2QNT5dQ6kXm8Z5kHMZAlYgC5IVUSJeg2su4waV0RCUliAYlM5xCqgUUEG9EdesMKRkBJUq1WkWr1YJt21hdXRVym2kOLJ1DqRfZB8tMe0B1kh1TQHKCalCd4aRURD311FNx34RyVAgnQM146hRFSO0VUKGkh1Sj0UClUkE2m0Wz2UShUEAul9Pi4KCiQyxp8TMK2cHUTdaBNFWNqJAKMQWkL6guu+yygS7Hc+dFSJVwCqkeUKLddtvb2iF18ODBRIVUEASo1WrtU9+4rou1tTXkcrkdB+xUEaMmfqpFUycdj0QuysmTJ5UIqc7tm6igCoIAruvCNM32f6phRI1BtWgKpSmeBp2F6iVJIRUEAYIggOM4AADf9xFOMgdBAMMwsLCwgHK5jFqtJnOoJIDKwdSNAbW3uE8dM6zubV9UURW+ZhmGgSAIUK/X0Wg0AGydx7JUKu24vGEY8H2/vYxBdGgxovagaij1k6aAGkXnbBRw6QVc95jyfR+macIwDABbn1C07a1fb8MwUK/X4bouisVi+8TM4bnhSH86RVOI8TQ8VWaluu22nRwmsMLXLwBotVpotVqYmJiAZVmoVCqo1+vI5XI7YqtaraLZbGJiYgKmabbjSgRGFPQLpV7SGE+jzkJ1hxSw/QVdx6AK342FXNdtz0o1m01UKhUAWyfXbTQayOfzPJmypnQMJkDNaFJ9PVQvqs1K7WWv7evCwgI8z0OlUmkfz880Tfi+D8dx2jNLQRDA87z29Tpnq1qtFhzHERZOnVKzsDwJodSP7gE16uLycXblAb0PfdCLLlG1vr7enpEKggCFQgGZTAZBEKDVasF1XdTrdTSbTeRyufY7PEDt9VJpp2s0AWqGUycdI6qTLiEVlYmJCeTz+fZMU7Vabb+GeZ6HbDbb/vu4UrGwPMlhNAjd42kc4wbUMHSZpSqVSmg2m3BdF6VSCa7rolKpoFgswnEcOI4Dy7JgWRZKpdK2FxsdFp8nic5htBfVwylJdJuVGle5XEa5XO77/XDGXeQnCYVHVNrDJwppjqco9dqttxeVg8qyLOTzeeTz+fbfQ50TzuF0eS+DbtwZWzslOYz2wnCSS9W1UrKM0xnDBpiQ3XmPPvpo3DeReEkOp2F350U9CzVsSPWiWlDpLM5AS3PoRC0J4aT77rzdMKrGc/XVVw90Oa135yVRkmNJVaPMSHVTeYZKNwwddSUhnEJJDiig/7aEcRUtRpQkjKVLDh06NPBslMi1UKNiUFGSJCmciHEVNUZUzBhLeohiNqoXBhXpiOGUPoyr0TCiIsRgilfcs1BxhVRorw0TI4tEYSTRoHpt1xhWlzCidsEoSp+4Q2o3g2zYGFq0FwYSxW3YbWOSoyvxEcUQSgYd1kKJwNBKNwZSNJK+qFw1o26HdYgvIRHFkKG9DLO4PG4yZ6OiEMWGliEWPQYQ0XB0aIfEz0SR/mTMQukeUuNSeYO/W+CpPG4iSh5GFBFphaFERKrofe4HIkXIXAvFdVhERLQbzkSRMuJcFxUEAR577K+xsrIMy7Jw9dWvxOTkVPv7J0+ewPe+9x0YhonLLvtJXH758wBwt55qDMPAzMwMLMuC7/tYWVmB7/vbLlMqlVAoFAAA9XodGxsbMoZKiuKicorSSDNRvu/j7rvvxhve8AbccsstePrpp6MeF1GkM0FPP/0f8DwPN974Wrz4xS/D3//937S/5/seHn/8UVx//Y244Yb/G08++X1Uq9VYxkHjKRaLcF0Xi4uL2NzcxOTk5LbvW5aFYrGIxcVFLC4uIpfLwXEcSaMloqQbKaK+/vWvo9ls4ktf+hLe+c534vd+7/eiHhdRpC5cOI+DB7c+Lruw8AxcvLjU/t7a2iomJ6eQzeZgWRb2738mLlw4K2uotItsNot6vQ5ga5Ypl8tt+77neVhaWtr2NQHnWCeilBopov7xH/8RL3/5ywEAV155Jf7lX/4l0kERRT3747pNOE6m/XfDMNq7gZpNF5nMpe85TgbNZjPW8dDeisUinvGMZ2z7zzTN9uMWBAEMw9hxvfD7U1NTcF0XrVZL6LiJKD1GiqhKpYJSqdT+u2VZfKEipTlOBq7rtv8eBAFMc+vpn8k4277nuk1kMtkdP4MhJdbm5ibOnz+/7T/f99uPm2EYfWeZZmZmYJomVldXRQ6ZiFLGCEaY677nnntwxRVX4IYbbgAAXHPNNfj2t78d+eCIiIiIVDXSTNRVV13Vjqbvfe97uOyyyyIdFBEREZHqRpqJ8n0fH/zgB/HUU08hCAJ85CMfwdGjR+MYHxEREZGSRoooIiIiorTjEcuJiIiIRsCIIiIiIhoBI4qIiIhoBEIiqlqt4vbbb8exY8fwy7/8y1hZWRFxs6lWLpfxq7/6q3jzm9+MN7zhDfjud78re0ip8fDDD+Od73yn7GEkGk89Jc8TTzyBW265RfYwUsF1XbzrXe/CsWPHcNNNN+GRRx6RPaTE8zwP733ve3HzzTfjTW96E06ePLnr5YVE1Je//GU8//nPx4MPPohf/MVfxL338oSucfvMZz6Dl73sZfjCF76Ae+65Bx/60IdkDykVfud3fgcf+9jHdpwUl6LFU0/J8cADD+Cuu+5Co9GQPZRU+PM//3NMT0/jwQcfxAMPPIAPf/jDsoeUeN/85jcBAF/84hdx55134p577tn18raIQd16663wPA8AcPbsWczNzYm42VS79dZb26cy8TwP2ezOI3BT9K666iq86lWvwpe+9CXZQ0k0nnpKjsOHD+OTn/wk3v3ud8seSir8wi/8Aq6//vr23y3LkjiadHjVq16Fn/u5nwMwWK9EHlF/+qd/is9+9rPbvvaRj3wEL3jBC/BLv/RLeOqpp/CZz3wm6ptNtd3u86WlJbzrXe/C+973PkmjS6Z+9/kNN9yAxx9/XNKo0qPfqadsW8j7wtS6/vrrcfr0adnDSI1isQhg6/l+55134h3veIfcAaWEbdt4z3veg4cffhh/8Ad/sPtlo77x173udXjd617X83uf+9zncPz4cfzKr/wKvv71r0d906nV7z5/8skn8eu//ut497vfjZe85CUSRpZcuz3PKX6lUgmbm5vtv/u+z4CiRDp37hzuuOMOHDt2DDfeeKPs4aTGRz/6UfzGb/wGXv/61+Mv//IvUSgUel5OyJqo+++/H1/96lcBAIVCgVOSAvzwhz/E29/+dnzsYx/DK17xCtnDIYoUTz1FaXDx4kXcdttteNe73oWbbrpJ9nBS4atf/Sruv/9+AEA+n4dhGLs2i5C3bq997Wvxnve8B1/5ylfgeR4+8pGPiLjZVPvYxz6GZrOJ3/3d3wWw9c79U5/6lORREUXjuuuuw9/8zd/g5ptvbp96iihp7rvvPmxsbODee+9tfyDrgQceQC6Xkzyy5Pr5n/95vPe978Wb3vQmtFotvO9979t1TTFP+0JEREQ0Ah5sk4iIiGgEjCgiIiKiETCiiIiIiEbAiCIiIiIaASOKiIiIaASMKCIiIqIRMKKIiIiIRsCIIiIiIhrB/w9/t0DuTOQO2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_10_0.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contour Plot\n",
    "x_vals = np.linspace(-3, 3, 21)\n",
    "y_vals = np.linspace(0, 10, 11)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "R = np.sqrt(X**2 + Y**2)\n",
    "Z = np.cos(X) * np.sin(Y)\n",
    "cs = plt.contourf(X, Y, Z, 10, cmap=None)\n",
    "plt.clabel(cs, fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4cc38bf2948f>:2: MatplotlibDeprecationWarning: Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6.  This is consistent with other Axes classes.\n",
      "  ax = Axes3D(plt.figure())\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_11_1.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Surface Plot\n",
    "ax = Axes3D(plt.figure())\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_roots (f, points=0):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    plt.plot(x, f(x));\n",
    "    roots, infodict, ier, mesg = fsolve(f, points, full_output=True)\n",
    "    plt.scatter(roots, np.zeros(len(roots)));\n",
    "    if ier == 1:\n",
    "        print('solution found')\n",
    "    else:\n",
    "        print('solution not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution found\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_13_1.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f (x): return x**2 - 1\n",
    "find_roots(f, [-0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution found\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_14_1.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f (x): return np.sin(x)**10\n",
    "find_roots(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution found\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_15_1.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f (x): return x * (1 + x**3) - 1\n",
    "find_roots(f, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.22074408+0.j        ,  0.24812606+1.03398206j,\n",
       "        0.24812606-1.03398206j,  0.72449196+0.j        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.roots([1,0,0,1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-9df5c294a716>:1: RuntimeWarning: invalid value encountered in power\n",
      "  def f (x): return x**3.4 + x - 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/davefriedman/Documents/repo/myGitHub/resources_public/docs/_build/jupyter_execute/nbs/z-repo_Solidity/Untitled_17_2.png"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f (x): return x**3.4 + x - 1\n",
    "find_roots(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}